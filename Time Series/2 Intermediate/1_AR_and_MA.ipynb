{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxFCzp7hfpCr"
   },
   "source": [
    "# **White Noise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8s9XBf9fun-"
   },
   "source": [
    "\n",
    "\n",
    "> * Like a stationary series, the white noise is also not a function of time. So, its mean and variance does not change over time. But the difference is that, the white noise is completely random with a mean of 0. In white noise there is no pattern.\n",
    "\n",
    "> * Mathematically, a sequence of completely random numbers with mean zero is a white noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNsKAuIviLuD"
   },
   "source": [
    "White noise has...\n",
    "\n",
    "> * Constant mean\n",
    "> * Constant variance\n",
    "> * Zero auto-correlation at all lags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!python -m pip install pip --upgrade --user -q --no-warn-script-location\n",
    "!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels plotly sklearn tensorflow keras torch torchvision \\\n",
    "    tqdm scikit-image --user -q --no-warn-script-location\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNIh0_PBfqNo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Plotting white noise\n",
    "plt.rcParams['figure.figsize'] = 16, 6\n",
    "white_noise = np.random.normal(loc=0, scale=1, size=1000)\n",
    "# loc is mean, scale is variance\n",
    "plt.plot(white_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gT8klfzWi7nZ"
   },
   "outputs": [],
   "source": [
    "# Plotting autocorrelation of white noise\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "plot_acf(white_noise,lags=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8yMaCCXjJz-"
   },
   "source": [
    "\n",
    "\n",
    "See how all lags are statistically insigficant as they lie inside the confidence interval(shaded portion).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWddy-LEgZO5"
   },
   "source": [
    "# **Random Walk**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5GYnd0ijNft"
   },
   "source": [
    "A random walk is a mathematical object, known as a stochastic or random process, that describes a path that consists of a succession of random steps on some mathematical space such as the integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkgHCfNdjPws"
   },
   "source": [
    "In general if we talk about stocks, Today's Price = Yesterday's Price + Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rXXDGqNjZi9"
   },
   "source": [
    "Pt = Pt-1 + εt\n",
    "\n",
    "Random walks can't be forecasted because well, noise is random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vqBBHw4kAEA"
   },
   "source": [
    "## **Generating a random walk**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZP6Xu5-jfxR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import normal, seed\n",
    "seed(42)\n",
    "plt.rcParams['figure.figsize'] = 16, 6\n",
    "random_walk = normal(loc=0, scale=0.01, size=1000)\n",
    "plt.plot(random_walk)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlmFtU_skJwn"
   },
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "fig = ff.create_distplot([random_walk],['Random Walk'],bin_size=0.001)\n",
    "iplot(fig, filename='Basic Distplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WdKfCR1gnTw"
   },
   "source": [
    "#**Autoregression(AR) and Moving Average(MA) and ARIMA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WRHJv9D-z8K"
   },
   "source": [
    "## Import The Library <a id =\"30\"></a>\n",
    "* pandas: Used for data manipulation and analysis\n",
    "* numpy : Numpy is the core library for scientific computing in Python. It is used for working with arrays and matrices.\n",
    "* matplotlib : It’s plotting library, and we are going to use it for data visualization\n",
    "* seaborn: It’s a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics\n",
    "* statsmodels: Using statsmodels module classes and functions for time series analysis and forecasting \n",
    "   * adfuller: Augmented Dickey-Fuller\n",
    "   * ACF: Auto Correlation Function\n",
    "   * PACF: Partial Auto Correlation Function\n",
    "   * ARIMA: Autoregressive Integrated Moving Average ARIMA(p,d,q) Model\n",
    "   * sm.tsa.seasonal.seasonal_decompose: For decomposition of time series\n",
    "* rcParams: To change the matplotlib properties like figure size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "9jThC9jV-z8M"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller,acf, pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import statsmodels.api as sm\n",
    "from pylab import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Nq9PwUB-z8M"
   },
   "outputs": [],
   "source": [
    "# Set plot size \n",
    "rcParams['figure.figsize'] = 10, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1kX7a-h-z8N"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/satishgunjal/datasets/master/Time_Series_AirPassengers.csv')\n",
    "print('Shape of the data= ', df.shape)\n",
    "print('Column datatypes= \\n',df.dtypes)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJr7cyqs-z8N"
   },
   "source": [
    "## Understanding The Data <a id =\"31\"></a>\n",
    "* Dataframe 'df' contains the time series data. There are two columns 'Month' and 'Passengers'. Month column contains the value of month in that year and passenger column contains the number of air passenger for that particular month.\n",
    "* As you may have noticed 'Month' column datatype is 'Object', so we are going to convert it to 'datetime'\n",
    "* To make plotting easier, we set the index of pandas dataframe 'df' to the 'Month' column so that it will act as x-axis & Passenger column as y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3yvWS1F-z8N"
   },
   "outputs": [],
   "source": [
    "df['Month'] = pd.to_datetime(df.Month)\n",
    "df = df.set_index(df.Month)\n",
    "df.drop('Month', axis = 1, inplace = True)\n",
    "print('Column datatypes= \\n',df.dtypes)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GURCHbk-z8P"
   },
   "source": [
    "## Time Series Characteristics <a id =\"32\"></a>\n",
    "\n",
    "### Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuNaYk4E-z8P"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (10,6))\n",
    "plt.plot(df)\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('No of Air Passengers')\n",
    "plt.title('Trend of the Time Series')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDTURmW0-z8Q"
   },
   "source": [
    "As you can see from above plot there is upward trend of number of passenger for every year. \n",
    "\n",
    "### Variance\n",
    "In above graph you can clearly see that the variation is also increasing with the level of the series. You will see in the later part of this exercise how we handle the variance to increase the stationarity of the series.\n",
    "\n",
    "### Seasonality\n",
    "We can also see the graph going up and down at regular interval, that is the sign of seasonality. Let's plot the graph for few months to visualize for seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STiQ_BUH-z8Q"
   },
   "outputs": [],
   "source": [
    "# To plot the seasonality we are going to create a temp dataframe and add columns for Month and Year values\n",
    "df_temp = df.copy()\n",
    "df_temp['Year'] = pd.DatetimeIndex(df_temp.index).year\n",
    "df_temp['Month'] = pd.DatetimeIndex(df_temp.index).month\n",
    "# Stacked line plot\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Seasonality of the Time Series')\n",
    "sns.pointplot(x='Month',y='Passengers',hue='Year',data=df_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cS6YlVcz-z8Q"
   },
   "source": [
    "From above graph we can say that every year in month of July we observe maximum number of passengers and similarly minimum number of passenger in the month of November.\n",
    "\n",
    "### Decomposition of Time Series\n",
    "Let's now use the decomposition technique to deconstruct the time series data into several component like trend and seasonality for visualization of time series characteristics.\n",
    "\n",
    "Here we are going to use 'additive' model because it is quick to develop, fast to train, and provide interpretable patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJlL6TpX-z8Q"
   },
   "outputs": [],
   "source": [
    "decomposition = sm.tsa.seasonal_decompose(df, model='additive') \n",
    "fig = decomposition.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDakmy40-z8Q"
   },
   "source": [
    "## Time Series Analysis <a id =\"33\"></a>\n",
    "So our time series has variance, trend and seasonality characteristics. During our analysis we are going to try multiple techniques to make time series stationary and record the stationarity scores for each method. Finally, we will select the method, which is easy for inverse transformation easy and give best stationarity score.\n",
    "\n",
    "### Check for Stationarity\n",
    "We are going to use rolling statistics and Dickey-Fuller test to check the stationarity of the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMXuNy8O-z8Q"
   },
   "outputs": [],
   "source": [
    "def stationarity_test(timeseries):\n",
    "    # Get rolling statistics for window = 12 i.e. yearly statistics\n",
    "    rolling_mean = timeseries.rolling(window = 12).mean()\n",
    "    rolling_std = timeseries.rolling(window = 12).std()\n",
    "    \n",
    "    # Plot rolling statistic\n",
    "    plt.figure(figsize= (10,6))\n",
    "    plt.xlabel('Years')\n",
    "    plt.ylabel('No of Air Passengers')    \n",
    "    plt.title('Stationary Test: Rolling Mean and Standard Deviation')\n",
    "    plt.plot(timeseries, color= 'blue', label= 'Original')\n",
    "    plt.plot(rolling_mean, color= 'green', label= 'Rolling Mean')\n",
    "    plt.plot(rolling_std, color= 'red', label= 'Rolling Std')   \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Dickey-Fuller test\n",
    "    print('Results of Dickey-Fuller Test')\n",
    "    df_test = adfuller(timeseries['Passengers'])\n",
    "    df_output = pd.Series(df_test[0:4], index = ['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "    for key, value in df_test[4].items():\n",
    "        df_output['Critical Value (%s)' %key] = value\n",
    "    print(df_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jygTFIYt-z8S"
   },
   "outputs": [],
   "source": [
    "# Lets test the stationarity score with original series data\n",
    "stationarity_test(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SY0i9oRT-z8S"
   },
   "source": [
    "Though it's clear from visual observation that it's not a stationary series, but still lets have look at the rolling statistics and Duckey Fuller test results\n",
    "\n",
    "* Rolling statistics: Standard deviation has very less variation but mean is increasing continuously.\n",
    "* Duckey Fuller Test: Test statistic is way more than the critical values.\n",
    "\n",
    "### Convert Non-Stationary Data to Stationary Data\n",
    "Let's first use the differencing technique to obtain the stationarity.\n",
    "\n",
    "#### Differencing\n",
    "To transform the series using 'Differencing' we will use the diff() method of pandas. A benefit of using the Pandas function, in addition to requiring less code, is that it maintains the date-time information for the differenced series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZAGTu6kw-z8S"
   },
   "outputs": [],
   "source": [
    "df_diff = df.diff(periods = 1) # First order differencing\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('No of Air Passengers')    \n",
    "plt.title('Convert Non Stationary Data to Stationary Data using Differencing ')\n",
    "plt.plot(df_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDJm7lP1-z8T"
   },
   "source": [
    "So from above graph its clear that differencing technique removed the trend from the time series, but variance is still there Now lets run the stationarity_test() to check the effectiveness of the 'Differencing' technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGq_1PHl-z8T"
   },
   "outputs": [],
   "source": [
    "df_diff.dropna(inplace = True)# Data transformation may add na values\n",
    "stationarity_test(df_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7x7cLqZ-z8T"
   },
   "source": [
    "The rolling values appear to be varying slightly, and we can see there is slight upward trend in standard deviation. Also, the test statistic is smaller than the 10% critical but since p-value is greater than 0.05 it is not a stationary series.\n",
    "\n",
    "Note that variance in the series is also affecting above results, which can be removed using transformation technique.\n",
    "\n",
    "Let's also check with transformation technique\n",
    "\n",
    "\n",
    "#### Transformation\n",
    "\n",
    "\n",
    "Since variance is proportional to the levels, we are going to use the log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-6dpvv1-z8T"
   },
   "outputs": [],
   "source": [
    "df_log = np.log(df)\n",
    "plt.subplot(211)\n",
    "plt.plot(df, label= 'Time Series with Variance')\n",
    "plt.legend()\n",
    "plt.subplot(212)\n",
    "plt.plot(df_log, label='Time Series without Variance (Log Transformation)')\n",
    "plt.legend()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeVuwX7t-z8T"
   },
   "source": [
    "Since log transformation has removed the variance from series, lets use this transformed data hence forward. \n",
    "Note that, Since we are using log transformation, we can use the exponential of the series to get the original scale back.\n",
    "```\n",
    "    df = exp(df_log)\n",
    "```\n",
    "\n",
    "Let cross-check the differencing method scores with this log transformed data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYU70n_v-z8U"
   },
   "outputs": [],
   "source": [
    "df_log_diff = df_log.diff(periods = 1) # First order differencing\n",
    "\n",
    "df_log_diff.dropna(inplace = True)# Data transformation may add na values\n",
    "stationarity_test(df_log_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vG_B4RH-z8U"
   },
   "source": [
    "The rolling mean and standard deviation values are okay now. The test statistic is smaller than the 10% critical values but since p-value is greater than 0.05 it is not a stationary series.\n",
    "\n",
    "Let's also check with Moving Average technique…\n",
    "\n",
    "#### Moving Average\n",
    "\n",
    "Since we have time series data from 1 Jan 1949 to 1 Dec 1960, we will define a yearly window for moving average. Window size = 12. Note that we are going to use Log transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9Qq2-iY-z8U"
   },
   "outputs": [],
   "source": [
    "df_log_moving_avg = df_log.rolling(window = 12).mean()\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('No of Air Passengers')    \n",
    "plt.title('Convert Non Stationary Data to Stationary Data using Moving Average')\n",
    "plt.plot(df_log, color= 'blue', label='Orignal')\n",
    "plt.plot(df_log_moving_avg, color= 'red', label='Moving Average')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1CPT9qC-z8U"
   },
   "source": [
    "As you can see from above graph that data is more smooth without any variance. If we use the differencing technique with log transformed data and mean average data then we should get better stationarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jqyGY-7-z8V"
   },
   "outputs": [],
   "source": [
    "df_log_moving_avg_diff = df_log - df_log_moving_avg\n",
    "df_log_moving_avg_diff.dropna(inplace = True)\n",
    "stationarity_test(df_log_moving_avg_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XSojqq7-z8V"
   },
   "source": [
    "As expected now we are able to see some real improvements. p-value is less than 0.05 that means our series is stationary, but we can only say this with 95% of confidence, as test statistics is less than 5% critical value.\n",
    "\n",
    "In order to increase the stationarity of the series lets try to use 'Weighted Moving Average' technique\n",
    "\n",
    "#### Weighted Moving Average (WMA)\n",
    "\n",
    "\n",
    "Weighted moving averages assign a heavier weighting to more current data points since they are more relevant than data points in the distant past.\n",
    "\n",
    "Here we are going to use exponentially weighted moving average with parameter ‘halflife = 12’. This parameter defines the amount of exponential decay. This is just an assumption here and would depend largely on the business domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1JR35Jb-z8V"
   },
   "outputs": [],
   "source": [
    "df_log_weighted_avg = df_log.ewm(halflife = 12).mean()\n",
    "plt.plot(df_log)\n",
    "plt.plot(df_log_weighted_avg, color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJlxYRFO-z8V"
   },
   "source": [
    "Notice that WMA follow's no of passenger values more closely than a corresponding Simple Moving Average which also results in more accurate trend direction. Now lets check, the effect of this on stationarity scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoMqXnri-z8W"
   },
   "outputs": [],
   "source": [
    "df_log_weighted_avg_diff = df_log - df_log_weighted_avg\n",
    "stationarity_test(df_log_weighted_avg_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zv5BFpHG-z8W"
   },
   "source": [
    "Test statistic is smaller than the 1% critical value, which is better than the previous case. Note that in this case there will be no missing values as all values from starting are given weights. So it’ll work even with no previous values.\n",
    "\n",
    "There is one more way to obtain better stationarity is by using the residual data from time series decomposition.\n",
    "\n",
    "#### Decomposition of Time Series\n",
    "\n",
    "Let's now use the decomposition technique to deconstruct the log transformed time series data, so that we can check the stationarity using residual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24mjh3X6-z8W"
   },
   "outputs": [],
   "source": [
    "decomposition = sm.tsa.seasonal_decompose(df_log)\n",
    "fig = decomposition.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUClTuUk-z8W"
   },
   "source": [
    "Here we can see that the trend and seasonality are separated out from log transformed data, and we can now check the stationarity of the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACAYECu_-z8W"
   },
   "outputs": [],
   "source": [
    "df_log_residual = pd.DataFrame(decomposition.resid)\n",
    "df_log_residual = df_log_residual.rename(columns = {\"resid\":\"Passengers\"})\n",
    "df_log_residual.dropna(inplace = True)\n",
    "stationarity_test(df_log_residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_D5HAK9-z8W"
   },
   "source": [
    "The Dickey-Fuller test statistic is significantly lower than the 1% critical value and p-value is almost 0. So this time series is very close to stationary. This concludes our time series analysis and data transformation to get the stationary series. Now we can start modeling it for forecast.\n",
    "\n",
    "## Forecasting <a id =\"34\"></a>\n",
    "* Though using residual values gives us very good results, but it's relatively difficult to add noise and seasonality back into predicted residuals in this case. \n",
    "* So we are going to make model on the time series(df_log_diff), where we have used log transformation and differencing technique. This is one of the most popular and beginner-friendly technique. As per our time series analysis 'df_log_diff' is not a perfectly stationary series, that's why we are going to use statistical models like ARIMA to forecast the data.\n",
    "* Remember that ARIMA model uses three parameters, 'p' for the order of Auto-Regressive (AR) part, 'q' for the order of Moving Average (MA) part and 'd' for the order of integrated part. We are going to use d =1 but to find the value for p and q lets plot ACF and PACF.\n",
    "* Note that since we are using d=1, first order of differencing will be performed on given series. Since first value of time series don't have any value to subtract from resulting series will have one less value from original series\n",
    "\n",
    "### ACF and PACF Plots\n",
    "* To figure out the order of AR model(p) we will use PACF function. p = the lag value where the PACF chart crosses the upper confidence interval for the first time\n",
    "* To figure out the order of MA model(q) we will use ACF function. q = the lag value where the ACF chart crosses the upper confidence interval for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02O8hybz-z8W"
   },
   "outputs": [],
   "source": [
    "lag_acf = acf(df_log_diff, nlags=20)\n",
    "lag_pacf = pacf(df_log_diff, nlags=20, method='ols')\n",
    "\n",
    "# Plot ACF: \n",
    "plt.subplot(121) \n",
    "plt.plot(lag_acf)\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "# Draw 95% confidence interval line\n",
    "plt.axhline(y=-1.96/np.sqrt(len(df_log_diff)),linestyle='--',color='red')\n",
    "plt.axhline(y=1.96/np.sqrt(len(df_log_diff)),linestyle='--',color='red')\n",
    "plt.xlabel('Lags')\n",
    "plt.title('Autocorrelation Function')\n",
    "\n",
    "#Plot PACF:\n",
    "plt.subplot(122)\n",
    "plt.plot(lag_pacf)\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "# Draw 95% confidence interval line\n",
    "plt.axhline(y=-1.96/np.sqrt(len(df_log_diff)),linestyle='--',color='red')\n",
    "plt.axhline(y=1.96/np.sqrt(len(df_log_diff)),linestyle='--',color='red')\n",
    "plt.xlabel('Lags')\n",
    "plt.title('Partial Autocorrelation Function')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLEP1c7G-z8X"
   },
   "source": [
    "From above graph its clear that p=2 and q=2. Now we have the ARIMA parameters values, lets make 3 different ARIMA models considering individual as well as combined effects. We will also print the RSS(Residual Sum of Square) metric for each. Please note that here RSS is for the values of residuals and not actual series.\n",
    "\n",
    "### AR Model\n",
    "Since 'q' is MA model parameter we will keep its value as '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2rYZHPw-z8X"
   },
   "outputs": [],
   "source": [
    "# freq = 'MS' > The frequency of the time-series MS = calendar month begin\n",
    "# The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters to use\n",
    "model = ARIMA(df_log, order=(2, 1, 0), freq = 'MS')  \n",
    "results_AR = model.fit()# If disp < 0 convergence information will not be printed\n",
    "plt.plot(df_log_diff)\n",
    "plt.plot(results_AR.fittedvalues, color='red')\n",
    "plt.title('AR Model, RSS: %.4f'% sum((results_AR.fittedvalues - df_log_diff['Passengers'])**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynUoRtHM-z8X"
   },
   "source": [
    "### MA Model\n",
    "Since 'p' is AR model parameter we will keep its value as '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpLdtY99-z8Y"
   },
   "outputs": [],
   "source": [
    "model = ARIMA(df_log, order=(0, 1, 2), freq = 'MS')  \n",
    "results_MA = model.fit(disp=-1)  \n",
    "plt.plot(df_log_diff)\n",
    "plt.plot(results_MA.fittedvalues, color='red')\n",
    "plt.title('MA Model, RSS: %.4f'% sum((results_MA.fittedvalues-df_log_diff['Passengers'])**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnAFf5Sa-z8Y"
   },
   "source": [
    "### Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_KR8rt7-z8Y"
   },
   "outputs": [],
   "source": [
    "model = ARIMA(df_log, order=(2, 1, 2), freq = 'MS')  \n",
    "results_ARIMA = model.fit(disp=-1)  \n",
    "plt.plot(df_log_diff)\n",
    "plt.plot(results_ARIMA.fittedvalues, color='red')\n",
    "plt.title('Combined Model, RSS: %.4f'% sum((results_ARIMA.fittedvalues-df_log_diff['Passengers'])**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkpEyisb-z8Y"
   },
   "source": [
    "Here we can see that the AR and MA models have almost the same RSS score but combined is significantly better. So we will go ahead with combined ARIMA model and use it for predictions.\n",
    "\n",
    "### Prediction and Reverse Transformation\n",
    "* We will create a separate series of predicted values using ARIMA model\n",
    "* Reverse transform the predicted values to get the original scale back\n",
    "* Compare the predicted values with original values and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jz1fj-oV-z8Z"
   },
   "outputs": [],
   "source": [
    "# Create a separate series of predicted values\n",
    "predictions_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\n",
    "\n",
    "print('Total no of predictions: ', len(predictions_diff))\n",
    "predictions_diff.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQ-twFO2-z8Z"
   },
   "source": [
    "Since we are using first order of differencing(d =1), there is no prediction available for first value (1949-02-01) of original series. In order to remove 'differencing transformation' from the prediction values we are going to add these differences consecutively to the base number. An easy way to do it is to first determine the cumulative sum at index and then add it to the base number. We are going to use pandas cumsum() function for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wz_l8MiNIemb"
   },
   "source": [
    "A cumulative sum is a sequence of partial sums of a given sequence. For example, the cumulative sums of the sequence {a,b,c,...}, are a, a+b, a+b+c, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItvrdQev-z8Z"
   },
   "outputs": [],
   "source": [
    "predictions_diff_cumsum = predictions_diff.cumsum()\n",
    "predictions_diff_cumsum.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjNyOf9e-z8a"
   },
   "source": [
    "Above values once added to the base number will completely remove the differencing transformation. For this, lets create a series with all values as base number and add the 'predictions_diff_cumsum' to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGBTFxZn-z8a"
   },
   "outputs": [],
   "source": [
    "predictions_log = pd.Series(df_log['Passengers'].iloc[0], index=df_log.index) # Series of base number\n",
    "predictions_log = predictions_log.add(predictions_diff_cumsum,fill_value=0)\n",
    "predictions_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rnnj7WA-z8a"
   },
   "source": [
    "So as of now we have removed the differencing transformation, now lets remove the log transformation to get the original scale back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykYSEaX5-z8a"
   },
   "outputs": [],
   "source": [
    "predictions = np.exp(predictions_log)\n",
    "plt.plot(df)\n",
    "plt.plot(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5RTjM8K-z8b"
   },
   "outputs": [],
   "source": [
    "df_predictions =pd.DataFrame(predictions, columns=['Predicted Values'])\n",
    "pd.concat([df,df_predictions],axis =1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hg_Sc7bF-z8c"
   },
   "source": [
    "### Future Forecasting\n",
    "* We have data from 1 Jan 1949 to 1 Dec 1960. 12 years of data with passenger number observation for each month i.e. 144 total observations.\n",
    "* If we want to forecast for next 5 years or 60 months then, ‘end’ count will be >  144 + 60 = 204.\n",
    "* We are going to use statsmodels plot_predict() method for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5v5ZAyfJ-z8c"
   },
   "outputs": [],
   "source": [
    "results_ARIMA.plot_predict(start = 1, end= 204) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ml1JL4sf-z8c"
   },
   "outputs": [],
   "source": [
    "# Forecasted values in original scale will be\n",
    "forecast_values_log_scale = results_ARIMA.forecast(steps = 60)\n",
    "forecast_values_original_scale = np.exp(forecast_values_log_scale[0])\n",
    "\n",
    "forecast_date_range= pd.date_range(\"1961-01-01\", \"1965-12-01\", freq=\"MS\")\n",
    "\n",
    "df_forecast =pd.DataFrame(forecast_values_original_scale, columns=['Forecast'])\n",
    "df_forecast['Month'] = forecast_date_range\n",
    "\n",
    "df_forecast[['Month', 'Forecast']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqArsKEsd2S-"
   },
   "source": [
    "#**Related Articles:**\n",
    "\n",
    "> * [Date Time Parsing with Pandas](https://analyticsindiamag.com/datetime-parsing-with-pandas/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOPhhPYwVPd4J8uW+SfmgiI",
   "collapsed_sections": [],
   "name": "1_AR_and_MA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
