{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSs-aJ-trfnM"
   },
   "source": [
    "# **HuggingFace DistilBERT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuviqbLkrpvx"
   },
   "source": [
    " Due to the large size of BERT, it is difficult for it to put it into production. Suppose we want to use these models on mobile phones, so we require a less weight yet efficient model, that’s when Distil-BERT comes into the picture. Distil-BERT has 97% of BERT’s performance while being trained on half of the parameters of BERT. BERT-base has 110 parameters and BERT-large has 340 parameters, which are hard to deal with. For this problem’s solution, distillation technique is used to reduce the size of these large models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zmRSyn9ruR_"
   },
   "source": [
    "## **Installation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jMlLhTBrxWb"
   },
   "source": [
    "Install HuggingFace Transformers framework via PyPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q --no-warn-script-location\n",
    "!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn tensorflow keras nltk gensim transformers --user -q --no-warn-script-location\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ll1Ed0Vry_u"
   },
   "source": [
    "## **Demo of HuggingFace DistilBERT**\n",
    "\n",
    "You can import the DistilBERT model from transformers as shown below : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0OMn32AsPY-"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4lf3xldsRZ1"
   },
   "source": [
    "### **A. Checking the configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "maX3UO8vgDyW"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertConfig\n",
    "# Initializing a DistilBERT configuration\n",
    "configuration = DistilBertConfig()\n",
    "# Initializing a model from the configuration\n",
    "model = DistilBertModel(configuration)\n",
    "# Accessing the model configuration\n",
    "configuration = model.config "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CeYoG5SsgU9"
   },
   "source": [
    "### **B. DistilBERT Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OZgX8mhsjiX"
   },
   "outputs": [],
   "source": [
    "#Similar to BERT Tokenizer, gives end-to-end tokenization for punctuation and word piece\n",
    "from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "inputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WWF6NQOsnDX"
   },
   "source": [
    "The output of the tokenizer will be :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyV2cwyUsptX"
   },
   "source": [
    "`{'input_ids': tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMdgjpcassYl"
   },
   "source": [
    "where, input_ids are numerical representation for the sequence that the DistilBERT model will use.\n",
    "\n",
    "attention_mask represents which tokens to attend to or not.\n",
    "\n",
    "You can also check [DistilBERTTokenizerFast](https://huggingface.co/transformers/model_doc/distilbert.html#distilberttokenizerfast)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLXAqQgMsxoq"
   },
   "source": [
    "### **C. DistilBERT Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZXvmpbJwdlq"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKrCC6JDwkyS"
   },
   "source": [
    "The sequence of hidden-states at the output of the last layer of the model.\n",
    "\n",
    "You can also check [DistilBERTMaskedLM](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertformaskedlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PK2bkzvFwqGI"
   },
   "source": [
    "### **D. DistilBERT Masked Language Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "We3ZmE1lwtsf"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "import torch\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bt6WGQZ2wxMB"
   },
   "source": [
    "\n",
    "where, loss is the masked language modeling loss.\n",
    "\n",
    "logits is Prediction scores of the language modeling head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3IGMeZ1w0Gj"
   },
   "source": [
    "### **E. DistilBERT for Sequence Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnySb6crw2bv"
   },
   "source": [
    "This model contains a pooler layer on the top of  pooled output that can be used for regression or classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQ6-_X1rwwhB"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IA2AK3L_w6dx"
   },
   "source": [
    "where, loss is the classification loss.\n",
    "\n",
    "logits are the classification score before Softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbPGf1xAw8Ny"
   },
   "source": [
    "### **F. DistilBERT for Multiple Choice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewpdr0REw-f9"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMultipleChoice\n",
    "import torch\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "model = DistilBertForMultipleChoice.from_pretrained('distilbert-base-cased')\n",
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "choice0 = \"It is eaten with a fork and a knife.\"\n",
    "choice1 = \"It is eaten while held in the hand.\"\n",
    "labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n",
    "encoding = tokenizer([[prompt, choice0], [prompt, choice1]], return_tensors='pt', padding=True)\n",
    "outputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=labels) # batch size is 1\n",
    "# the linear classifier still needs to be trained\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmPGwB5AxAme"
   },
   "source": [
    "### **G. DistilBERT for Token Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYtpDUUbxEQa"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForTokenClassification\n",
    "import torch\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForTokenClassification.from_pretrained('distilbert-base-uncased')\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1] * inputs[\"input_ids\"].size(1)).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOdFL3V3xGLg"
   },
   "source": [
    "### **H. DistilBERT For Question Answering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77PDs84yxJD7"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "import torch\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "inputs = tokenizer(question, text, return_tensors='pt')\n",
    "start_positions = torch.tensor([1])\n",
    "end_positions = torch.tensor([3])\n",
    "outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
    "loss = outputs.loss\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prpNOPgzWgiU"
   },
   "source": [
    "# **Related Articles:**\n",
    "\n",
    "> * [Guide to DistilBERT](https://analyticsindiamag.com/python-guide-to-huggingface-distilbert-smaller-faster-cheaper-distilled-bert/)\n",
    "\n",
    "> * [Introduction to Simple Transformers](https://analyticsindiamag.com/text-classification-using-simple-transformers/)\n",
    "\n",
    "> * [Google Sentence Embedder with Tensorflow](https://analyticsindiamag.com/guide-to-universal-sentence-encoder-with-tensorflow/)\n",
    "\n",
    "> * [Sequence-to-Sequence Modeling using LSTM for Language Translation](https://analyticsindiamag.com/sequence-to-sequence-modeling-using-lstm-for-language-translation/)\n",
    "\n",
    "> * [Text Generation using RNN](https://analyticsindiamag.com/recurrent-neural-network-in-pytorch-for-text-generation/)\n",
    "\n",
    "> * [SVD in Recommender System](https://analyticsindiamag.com/singular-value-decomposition-svd-application-recommender-system/)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP22k2yKCgBx+G8Fy0yMlss",
   "collapsed_sections": [],
   "name": "DistilBert _Huggingface.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
