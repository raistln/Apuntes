{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YGZhHPYWZKa"
   },
   "source": [
    "## Text Generation with RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-GpdD26WcdI"
   },
   "source": [
    "Text Generation is a task in Natural Language Processing (NLP) in which text is generated with some constraints such as initial characters or initial words. We come across this task in our day-to-day applications such as character/word/sentence predictions while typing texts in Gmail, Google Docs, Smartphone keyboard, and chatbot.  Understanding of text generation forms the base to advanced NLP tasks such as Neural Machine Translation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "147w0mFZWecR"
   },
   "source": [
    "This session discusses the text generation task to predict the next character given its previous characters. It employs a recurrent neural network with LSTM layers to achieve the task. The deep learning process will be carried out using TensorFlow’s Keras, a high-level API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1Ej6qoaWhJY"
   },
   "source": [
    "# Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLEGjztxVvKw"
   },
   "source": [
    "## Create the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4C6hGQzKWmms"
   },
   "source": [
    "Import the necessary frameworks, libraries and modules to create the required Python environment. Since we work with text data, an Embedding layer will be required. Since we build LSTM recurrent neural networks, an LSTM layer will be required. In addition, a Dense layer will be of use to develop a classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q --no-warn-script-location\n",
    "!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn torch tensorflow keras nltk gensim --user -q --no-warn-script-location\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:16.236964Z",
     "iopub.status.busy": "2021-05-27T13:53:16.236358Z",
     "iopub.status.idle": "2021-05-27T13:53:22.987563Z",
     "shell.execute_reply": "2021-05-27T13:53:22.986158Z",
     "shell.execute_reply.started": "2021-05-27T13:53:16.236869Z"
    },
    "executionInfo": {
     "elapsed": 2063,
     "status": "ok",
     "timestamp": 1624274177175,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "C2RRgZxFVvK0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wks-K6xTVvK2"
   },
   "source": [
    "## Download Text Data\n",
    "\n",
    "\n",
    "We need text data to train our model. TensorFlow’s data collection has a text file with contents extracted from various Shakespearean plays. Download the data file from Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:22.991878Z",
     "iopub.status.busy": "2021-05-27T13:53:22.991495Z",
     "iopub.status.idle": "2021-05-27T13:53:23.264433Z",
     "shell.execute_reply": "2021-05-27T13:53:23.263434Z",
     "shell.execute_reply.started": "2021-05-27T13:53:22.991848Z"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1624274177176,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "VHzm7TXZVvK3",
    "outputId": "e75ed622-8090-486f-bbfd-54b0dcd2ba1f"
   },
   "outputs": [],
   "source": [
    "file_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
    "file_name= \"shakespeare.txt\"\n",
    "# get the file path\n",
    "path = keras.utils.get_file(file_name, file_URL)\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdpFgMv7WuLV"
   },
   "source": [
    "Open the downloaded file and read its content. Print some sample portions from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:23.266524Z",
     "iopub.status.busy": "2021-05-27T13:53:23.266182Z",
     "iopub.status.idle": "2021-05-27T13:53:23.276416Z",
     "shell.execute_reply": "2021-05-27T13:53:23.275278Z",
     "shell.execute_reply.started": "2021-05-27T13:53:23.266492Z"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1624274177178,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "A9cB6RuDVvK4",
    "outputId": "9c05f9e2-2ec4-4413-f4cb-7b223cd9ec93"
   },
   "outputs": [],
   "source": [
    "raw = open(path, 'rb').read()\n",
    "print(raw[250:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wShRlQ1YWwe3"
   },
   "source": [
    "Rather than reading ‘\\n’ as a newline character and printing the consequent characters in the next line, Python reads it as part of text characters. It is because the original downloaded file is UTF-8 encoded. We need to decode the text into Python readable string format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:23.279197Z",
     "iopub.status.busy": "2021-05-27T13:53:23.278652Z",
     "iopub.status.idle": "2021-05-27T13:53:23.295190Z",
     "shell.execute_reply": "2021-05-27T13:53:23.294329Z",
     "shell.execute_reply.started": "2021-05-27T13:53:23.279151Z"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1624274177179,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "rU1NG6c9VvK5",
    "outputId": "191beb95-8538-45e0-b7ff-bf8965589b12"
   },
   "outputs": [],
   "source": [
    "text = raw.decode(encoding='utf-8')\n",
    "print(text[250:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:23.297041Z",
     "iopub.status.busy": "2021-05-27T13:53:23.296456Z",
     "iopub.status.idle": "2021-05-27T13:53:23.315207Z",
     "shell.execute_reply": "2021-05-27T13:53:23.314275Z",
     "shell.execute_reply.started": "2021-05-27T13:53:23.297007Z"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1624274177180,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "4l8C-7xGVvK6",
    "outputId": "8302482c-eaa0-4a25-ad53-1cd0e6ef5637"
   },
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTM5aCFvVvK7"
   },
   "source": [
    "## Vectorize Word Characters into Integers\n",
    "\n",
    "A deep learning model can not accept text characters as inputs. It should be encoded into integers that a model can understand and process with. Though there are more than a million characters in the given text, there will be a countable number of unique characters. The collection of unique characters is called vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:23.317090Z",
     "iopub.status.busy": "2021-05-27T13:53:23.316523Z",
     "iopub.status.idle": "2021-05-27T13:53:23.343718Z",
     "shell.execute_reply": "2021-05-27T13:53:23.342446Z",
     "shell.execute_reply.started": "2021-05-27T13:53:23.317055Z"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1624274177180,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "Skz-vB5_VvK8",
    "outputId": "2d126e8b-d8a3-4f66-9d28-ba147850a75e"
   },
   "outputs": [],
   "source": [
    "# unique characters\n",
    "vocabulary = np.array(sorted(set(text)))\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t997C9Y7W327"
   },
   "source": [
    "Define a tokenizer that can convert a text character into a corresponding integer. There will be 65 integers starting from 0 and ending at 64. We can assign integers on our own as per the order of characters in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:23.345660Z",
     "iopub.status.busy": "2021-05-27T13:53:23.345337Z",
     "iopub.status.idle": "2021-05-27T13:53:23.358488Z",
     "shell.execute_reply": "2021-05-27T13:53:23.357030Z",
     "shell.execute_reply.started": "2021-05-27T13:53:23.345622Z"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1624274177181,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "jLJrZ0urVvK9"
   },
   "outputs": [],
   "source": [
    "# assign an integer to each character\n",
    "tokenizer = {char:i for i,char in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ojRj6uPW6t_"
   },
   "source": [
    "What integers are assigned to what characters? Sample the first 20 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:23.362442Z",
     "iopub.status.busy": "2021-05-27T13:53:23.361947Z",
     "iopub.status.idle": "2021-05-27T13:53:23.377254Z",
     "shell.execute_reply": "2021-05-27T13:53:23.375801Z",
     "shell.execute_reply.started": "2021-05-27T13:53:23.362394Z"
    },
    "executionInfo": {
     "elapsed": 650,
     "status": "ok",
     "timestamp": 1624274177816,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "1P8nlXwoVvK9",
    "outputId": "c8f42131-25f8-455c-d9df-c99fc1b8636e"
   },
   "outputs": [],
   "source": [
    "# check characters and its corresponding integer\n",
    "\n",
    "for i in range(20):\n",
    "    char = vocabulary[i]\n",
    "    token = tokenizer[char]\n",
    "    print('%4s : %4d'%(repr(char),token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wsKWvhuW9kx"
   },
   "source": [
    "Vectorize the entire text and check whether the built tokenizer can encode and decode – texts and integers properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:23.379929Z",
     "iopub.status.busy": "2021-05-27T13:53:23.379523Z",
     "iopub.status.idle": "2021-05-27T13:53:23.705783Z",
     "shell.execute_reply": "2021-05-27T13:53:23.704742Z",
     "shell.execute_reply.started": "2021-05-27T13:53:23.379897Z"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1624274177817,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "-Ta-rh5iVvK-",
    "outputId": "acba1b3c-5384-4a4e-8e28-cf2f1b143f15"
   },
   "outputs": [],
   "source": [
    "vector = np.array([tokenizer[char] for char in text])\n",
    "\n",
    "print('\\nSample Text \\n')\n",
    "print('-'*70)\n",
    "print(text[:100])\n",
    "print('-'*70)\n",
    "print('\\n\\nCorresponding Integer Vector \\n')\n",
    "print('-'*70)\n",
    "print(vector[:100])\n",
    "print('-'*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsgzmT82XA_D"
   },
   "source": [
    "Text with one million encoded characters can not be fed into a model as such. Since we predict characters, the text must be broken down into sequences of some predefined length and then fed into the model. Use TensorFlow’s batch method to create sequences of 100 characters each. Prior to that, convert the NumPy arrays into tensors to make further processes using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:23.707471Z",
     "iopub.status.busy": "2021-05-27T13:53:23.707136Z",
     "iopub.status.idle": "2021-05-27T13:53:23.748223Z",
     "shell.execute_reply": "2021-05-27T13:53:23.747343Z",
     "shell.execute_reply.started": "2021-05-27T13:53:23.707440Z"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1624274177818,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "ZO1xROoAVvK-"
   },
   "outputs": [],
   "source": [
    "# convert into tensors\n",
    "vector = tf.data.Dataset.from_tensor_slices(vector)\n",
    "\n",
    "# make sequences each of length 100 characters\n",
    "sequences = vector.batch(100, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gawANyWfXEp_"
   },
   "source": [
    "Recurrent neural networks predict the subsequent characters based on the past characters. RNNs require a sequence of input characters and the corresponding target sequence with the subsequent characters for training. Prepare input sequences with the first 99 characters and corresponding target sequences with the last 99 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:23.749804Z",
     "iopub.status.busy": "2021-05-27T13:53:23.749381Z",
     "iopub.status.idle": "2021-05-27T13:53:23.839938Z",
     "shell.execute_reply": "2021-05-27T13:53:23.839015Z",
     "shell.execute_reply.started": "2021-05-27T13:53:23.749773Z"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1624274177818,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "w0RxIFYnVvLA"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(seq):\n",
    "    input_vector = seq[:-1]\n",
    "    target_vector = seq[1:]\n",
    "    return input_vector, target_vector\n",
    "\n",
    "dataset = sequences.map(prepare_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v12T5dQPXXw8"
   },
   "source": [
    "Let’s sample the first sequence pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:23.841739Z",
     "iopub.status.busy": "2021-05-27T13:53:23.841236Z",
     "iopub.status.idle": "2021-05-27T13:53:23.955160Z",
     "shell.execute_reply": "2021-05-27T13:53:23.953678Z",
     "shell.execute_reply.started": "2021-05-27T13:53:23.841696Z"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1624274177818,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "uAIdkkNxVvLA",
    "outputId": "6053c34c-b904-421b-ab02-72dbc3768b23"
   },
   "outputs": [],
   "source": [
    "# check how it looks\n",
    "for inp, tar in dataset.take(1):\n",
    "    print(inp.numpy())\n",
    "    print(tar.numpy())\n",
    "    inp_text = ''.join(vocabulary[inp])\n",
    "    tar_text = ''.join(vocabulary[tar])\n",
    "    print(repr(inp_text))\n",
    "    print(repr(tar_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Glyhd7eQVvLA"
   },
   "source": [
    "## Batch and Prefetch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gi7pkOtBXa2N"
   },
   "source": [
    "Model will be trained with Stochastic Gradient Descent (SGD) based optimizer Adam. It requires the input data to be batched. Further, TensorFlow’s prefetch method helps training with optimized memory. It fetches data batches just before the training requires them. We prefer not to shuffle the data to retain the contextual order of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:23.957086Z",
     "iopub.status.busy": "2021-05-27T13:53:23.956581Z",
     "iopub.status.idle": "2021-05-27T13:53:23.963852Z",
     "shell.execute_reply": "2021-05-27T13:53:23.962734Z",
     "shell.execute_reply.started": "2021-05-27T13:53:23.957039Z"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1624274177818,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "H4LJz8C9VvLB",
    "outputId": "0595adaf-8de8-4695-ca2c-cdaf99067dca"
   },
   "outputs": [],
   "source": [
    "# number of batched sequences\n",
    "len(sequences)//64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:23.966680Z",
     "iopub.status.busy": "2021-05-27T13:53:23.965907Z",
     "iopub.status.idle": "2021-05-27T13:53:23.987474Z",
     "shell.execute_reply": "2021-05-27T13:53:23.986278Z",
     "shell.execute_reply.started": "2021-05-27T13:53:23.966626Z"
    },
    "executionInfo": {
     "elapsed": 731,
     "status": "ok",
     "timestamp": 1624274178542,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "J5k8bH1eVvLB"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "# buffer size 10000\n",
    "# batch size 64\n",
    "data = dataset.batch(64, drop_remainder=True).repeat()\n",
    "data = data.prefetch(AUTOTUNE)\n",
    "# steps per epoch is number of batches available\n",
    "STEPS_PER_EPOCH = len(sequences)//64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:23.989089Z",
     "iopub.status.busy": "2021-05-27T13:53:23.988761Z",
     "iopub.status.idle": "2021-05-27T13:53:24.042036Z",
     "shell.execute_reply": "2021-05-27T13:53:24.041259Z",
     "shell.execute_reply.started": "2021-05-27T13:53:23.989057Z"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1624274178543,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "Ugv7m568VvLB",
    "outputId": "7970dfea-51bb-4071-fafd-1d3af9a0b0bd"
   },
   "outputs": [],
   "source": [
    "for inp, tar in data.take(1):\n",
    "    print(inp.numpy().shape)\n",
    "    print(tar.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRXjBMb8VvLC"
   },
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwPBKGIkXgy7"
   },
   "source": [
    "Recurrent neural networks are good at modeling time-dependent data because of its ability to retain time-steps based information in memory. Since texts have contextual information that are determined purely by order of the words, natural language processing heavily relies on sequence modeling architectures such as RNN. Here, an LSTM (Long Short-Term Memory) layers-based recurrent neural network is developed to model the task. While implementing LSTM layers, we enable stateful argument as True to keep the time-step memory of previous states while learning with consequent batches in an epoch. It helps capture the context among consecutive sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:24.043639Z",
     "iopub.status.busy": "2021-05-27T13:53:24.043295Z",
     "iopub.status.idle": "2021-05-27T13:53:25.087672Z",
     "shell.execute_reply": "2021-05-27T13:53:25.086790Z",
     "shell.execute_reply.started": "2021-05-27T13:53:24.043609Z"
    },
    "executionInfo": {
     "elapsed": 469,
     "status": "ok",
     "timestamp": 1624274178996,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "3sAJR-cBVvLC"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    # Embed len(vocabulary) into 64 dimensions\n",
    "    Embedding(len(vocabulary), 64, batch_input_shape=[64,None]),\n",
    "    # LSTM RNN layers\n",
    "    LSTM(512, return_sequences=True, stateful=True),\n",
    "    LSTM(512, return_sequences=True, stateful=True),\n",
    "    # Classification head\n",
    "    Dense(len(vocabulary))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:25.089173Z",
     "iopub.status.busy": "2021-05-27T13:53:25.088773Z",
     "iopub.status.idle": "2021-05-27T13:53:25.097934Z",
     "shell.execute_reply": "2021-05-27T13:53:25.097072Z",
     "shell.execute_reply.started": "2021-05-27T13:53:25.089144Z"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1624274178999,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "INWrmwXWVvLC",
    "outputId": "16d19c75-ea17-4af8-aacf-3ad6189c5187"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:25.100114Z",
     "iopub.status.busy": "2021-05-27T13:53:25.099454Z",
     "iopub.status.idle": "2021-05-27T13:53:25.578971Z",
     "shell.execute_reply": "2021-05-27T13:53:25.577812Z",
     "shell.execute_reply.started": "2021-05-27T13:53:25.100068Z"
    },
    "executionInfo": {
     "elapsed": 601,
     "status": "ok",
     "timestamp": 1624274179592,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "EWfo3AhiVvLC",
    "outputId": "a6cb2019-b7a8-4558-c4fe-2d55df964fa6"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2HHST9yVvLD"
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdJfRl3UXl_I"
   },
   "source": [
    "We can check whether the model can accept the processed data without any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:25.580968Z",
     "iopub.status.busy": "2021-05-27T13:53:25.580652Z",
     "iopub.status.idle": "2021-05-27T13:53:26.876155Z",
     "shell.execute_reply": "2021-05-27T13:53:26.874609Z",
     "shell.execute_reply.started": "2021-05-27T13:53:25.580935Z"
    },
    "executionInfo": {
     "elapsed": 2641,
     "status": "ok",
     "timestamp": 1624274182218,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "wdRX7KpDVvLD",
    "outputId": "e342c694-e2a0-469e-b054-2dac47b2812c"
   },
   "outputs": [],
   "source": [
    "# test whether model performs good\n",
    "\n",
    "for example_inp, example_tar in data.take(1):\n",
    "    example_pred = model(example_inp)\n",
    "    print(example_tar.numpy().shape)\n",
    "    print(example_pred.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIer4XKdXppk"
   },
   "source": [
    "The target shape is (64, 99), which refers to the batch size and the number of characters in that sequence. The last shape, 65, in the prediction refers to the size of the vocabulary. The model predicts the probability of occurrence of each character in the vocabulary. The character with a higher probability has more possibility to be the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:26.878251Z",
     "iopub.status.busy": "2021-05-27T13:53:26.877931Z",
     "iopub.status.idle": "2021-05-27T13:53:26.889712Z",
     "shell.execute_reply": "2021-05-27T13:53:26.888433Z",
     "shell.execute_reply.started": "2021-05-27T13:53:26.878220Z"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1624274182220,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "Nuc3aOAaVvLD",
    "outputId": "78e8dff8-0c14-41b0-8698-891ab3af61bf"
   },
   "outputs": [],
   "source": [
    "ids = tf.random.categorical(example_pred[0], num_samples=1)\n",
    "ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:26.891863Z",
     "iopub.status.busy": "2021-05-27T13:53:26.891215Z",
     "iopub.status.idle": "2021-05-27T13:53:26.906621Z",
     "shell.execute_reply": "2021-05-27T13:53:26.905458Z",
     "shell.execute_reply.started": "2021-05-27T13:53:26.891819Z"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1624274182221,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "_hruYU4LVvLE",
    "outputId": "f375e4f1-bdfc-4263-fa10-8c1af6003347"
   },
   "outputs": [],
   "source": [
    "ids[0][-1].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XY9KitgCXseG"
   },
   "source": [
    "Compile the model with Adam optimizer and Sparse Categorical Cross-entropy loss function. Since we have not employed softmax as the output layer’s activation function. The outputs will be independent but not mutually exclusive. Hence, we should enable the argument ‘from_logits’ to be True while declaring the loss function. Train the model for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:26.908612Z",
     "iopub.status.busy": "2021-05-27T13:53:26.908178Z",
     "iopub.status.idle": "2021-05-27T13:53:26.915951Z",
     "shell.execute_reply": "2021-05-27T13:53:26.914872Z",
     "shell.execute_reply.started": "2021-05-27T13:53:26.908574Z"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1624274182221,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "Hx6mDor4VvLE"
   },
   "outputs": [],
   "source": [
    "# callback to save checkpoints\n",
    "checkpoint_path = os.path.join(\"./checkpoints\", \"ckpt_{epoch}\")\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-27T13:53:26.920918Z",
     "iopub.status.busy": "2021-05-27T13:53:26.919791Z",
     "iopub.status.idle": "2021-05-27T14:52:49.026604Z",
     "shell.execute_reply": "2021-05-27T14:52:49.025185Z",
     "shell.execute_reply.started": "2021-05-27T13:53:26.920866Z"
    },
    "id": "Hmw2-h31VvLE",
    "outputId": "9add3820-d6da-41f4-c1a6-0f4ad95e5caf"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "history = model.fit(data, \n",
    "                    epochs=10, \n",
    "                    steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtwYrZfbVvLE"
   },
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35hxJZG5XwGg"
   },
   "source": [
    "Visualizing the losses over epochs may help get better insight on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T14:52:49.029070Z",
     "iopub.status.busy": "2021-05-27T14:52:49.028732Z",
     "iopub.status.idle": "2021-05-27T14:52:49.236134Z",
     "shell.execute_reply": "2021-05-27T14:52:49.235228Z",
     "shell.execute_reply.started": "2021-05-27T14:52:49.029039Z"
    },
    "id": "_AH1e8tMVvLF"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], '+-r')\n",
    "plt.title('Performance Analysis', size=16, color='green')\n",
    "plt.xlabel('Epochs', size=14, color='blue')\n",
    "plt.ylabel('Loss', size=14, color='blue')\n",
    "plt.xticks(range(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StVGDRFFVvLF"
   },
   "source": [
    "## Inference - Next Character Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6xavoOUXzIk"
   },
   "source": [
    "The most awaited part of this task is predicting the next character with the trained model. We can input the model some characters (probably a word) such that it will iteratively predict the next 1000 characters.\n",
    "\n",
    "Before starting prediction with the model, we should reset the model states that were stored in the memory during the last epoch training. However, resetting state memories will not affect the model’s weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T14:52:49.252724Z",
     "iopub.status.busy": "2021-05-27T14:52:49.252417Z",
     "iopub.status.idle": "2021-05-27T14:52:49.258655Z",
     "shell.execute_reply": "2021-05-27T14:52:49.257777Z",
     "shell.execute_reply.started": "2021-05-27T14:52:49.252687Z"
    },
    "id": "b6hZhcuTVvLG"
   },
   "outputs": [],
   "source": [
    "# reset previous states of model\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGDu1NwsX12m"
   },
   "source": [
    "Make predictions by providing the model ‘ANTHONIO:’ as input characters. Nevertheless, the model expects data in three dimensions: the first dimension being the batch size, 64. Vectorize the input characters, expand the dimensions, broadcast the same vector 64 times to obtain a batch of size 64 sequences. Predictions are made based on the logits output by the model. This can be sensitively adjusted by tuning a hyper-parameter called temperature, which refers to the level of randomness in choosing the probable outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T15:05:50.180989Z",
     "iopub.status.busy": "2021-05-27T15:05:50.180597Z",
     "iopub.status.idle": "2021-05-27T15:06:21.247443Z",
     "shell.execute_reply": "2021-05-27T15:06:21.246388Z",
     "shell.execute_reply.started": "2021-05-27T15:05:50.180960Z"
    },
    "id": "EN3--JdrVvLG"
   },
   "outputs": [],
   "source": [
    "sample = 'ANTHONIO:'\n",
    "# vectorize the string\n",
    "sample_vector = [tokenizer[s] for s in sample]\n",
    "predicted = sample_vector\n",
    "# convert into tensor of required dimensions\n",
    "sample_tensor = tf.expand_dims(sample_vector, 0) \n",
    "# broadcast to first dimension to 64 \n",
    "sample_tensor = tf.repeat(sample_tensor, 64, axis=0)\n",
    "\n",
    "# predict next 1000 characters\n",
    "# temperature is a sensitive variable to adjust prediction\n",
    "temperature = 0.6\n",
    "for i in range(1000):\n",
    "    pred = model(sample_tensor)\n",
    "    # reduce unnecessary dimensions\n",
    "    pred = pred[0].numpy()/temperature\n",
    "    pred = tf.random.categorical(pred, num_samples=1)[-1,0].numpy()\n",
    "    predicted.append(pred)\n",
    "    sample_tensor = predicted[-99:]\n",
    "    sample_tensor = tf.expand_dims([pred],0)\n",
    "    # broadcast to first dimension to 64 \n",
    "    sample_tensor = tf.repeat(sample_tensor, 64, axis=0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T15:06:21.249611Z",
     "iopub.status.busy": "2021-05-27T15:06:21.249144Z",
     "iopub.status.idle": "2021-05-27T15:06:21.256780Z",
     "shell.execute_reply": "2021-05-27T15:06:21.255619Z",
     "shell.execute_reply.started": "2021-05-27T15:06:21.249566Z"
    },
    "id": "AK-LdoDCVvLH"
   },
   "outputs": [],
   "source": [
    "pred_char = [vocabulary[i] for i in predicted]\n",
    "generated = ''.join(pred_char)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGU-kR0UVvLH"
   },
   "source": [
    "### vary temperature to see yet different prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0GgPDCBX4Cq"
   },
   "source": [
    "By adjusting the temperature value, we can vary randomness and obtain different predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpmIqlMBVvLH"
   },
   "outputs": [],
   "source": [
    "sample = 'ANTHONIO:'\n",
    "# vectorize the string\n",
    "sample_vector = [tokenizer[s] for s in sample]\n",
    "predicted = sample_vector\n",
    "# convert into tensor of required dimensions\n",
    "sample_tensor = tf.expand_dims(sample_vector, 0) \n",
    "# broadcast to first dimension to 64 \n",
    "sample_tensor = tf.repeat(sample_tensor, 64, axis=0)\n",
    "\n",
    "# predict next 1000 characters\n",
    "# vary temperature to change randomness\n",
    "temperature = 0.8\n",
    "for i in range(1000):\n",
    "    pred = model(sample_tensor)\n",
    "    # reduce unnecessary dimensions\n",
    "    pred = pred[0].numpy()/temperature\n",
    "    pred = tf.random.categorical(pred, num_samples=1)[-1,0].numpy()\n",
    "    predicted.append(pred)\n",
    "    sample_tensor = predicted[-99:]\n",
    "    sample_tensor = tf.expand_dims([pred],0)\n",
    "    # broadcast to first dimension to 64 \n",
    "    sample_tensor = tf.repeat(sample_tensor, 64, axis=0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-27T15:02:55.052046Z",
     "iopub.status.busy": "2021-05-27T15:02:55.051760Z",
     "iopub.status.idle": "2021-05-27T15:02:55.059127Z",
     "shell.execute_reply": "2021-05-27T15:02:55.058078Z",
     "shell.execute_reply.started": "2021-05-27T15:02:55.052018Z"
    },
    "id": "SQVy589mVvLH"
   },
   "outputs": [],
   "source": [
    "pred_char = [vocabulary[i] for i in predicted]\n",
    "generated = ''.join(pred_char)\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOikgLEu9q84upZ6udxGF3O",
   "collapsed_sections": [],
   "name": "2_Text_Generation_with_RNN_using_IMDB.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "f60a20abaabf5a658075b37fac599269792a9493ddacd7c14d8505185d5625aa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
