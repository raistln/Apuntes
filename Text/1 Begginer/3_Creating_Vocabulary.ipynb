{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ts7N905pq4O"
   },
   "source": [
    "# **Creating Vocabulary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaJ4cb0Appgm"
   },
   "source": [
    "The vocabulary helps in pre-processing of corpus text which acts as a classification and also a storage location for the processed corpus text. Once a text has been processed, any relevant metadata can be collected and stored.\n",
    "\n",
    "In this practice session, we will discuss the implementation of vocabulary builder in python for storing processed text data that can be used in future for NLP tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaW0_TIcpxwJ"
   },
   "source": [
    "## **About the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPt1YCt-qxTC"
   },
   "source": [
    "The dataset is downloaded from the following link. It contains the tweets of youtube users. The tweets are used for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q\n",
    "!python -m pip install pandas numpy --user -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQtgQKRUnjG3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(\"youtube.csv\")\n",
    "data.shape\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llSsQs5I69ZX"
   },
   "source": [
    "## Normalize the text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPKroBKZ667h"
   },
   "source": [
    "\n",
    "The next step is to perform the normalization of our text data. We need to convert the text data to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKIMX5Nj7CWm"
   },
   "outputs": [],
   "source": [
    "data[\"comment_text\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7vzITwl7Cs7"
   },
   "source": [
    "## **Making a dictionary for expanding the English language**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMQYTMRj7Jtd"
   },
   "source": [
    "For data pre-processing, expand the data using a contraction function. For example, words like ‘can’t’ are expanded to ‘cannot’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKNpRGrN7N3C"
   },
   "outputs": [],
   "source": [
    "contractions_dict = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITlbfzZD7PlU"
   },
   "source": [
    "## **Contraction Function for expanding english language**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_xTac3D7TBO"
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "     def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "     return contractions_re.sub(replace, s)\n",
    "data[\"comment_text\"] = [expand_contractions(i) for i in data[\"comment_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVWV3FzM7Uo4"
   },
   "source": [
    "## **Remove patterns using regex(Keep a-zA-Z0-9)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e4JYeeB7aZq"
   },
   "source": [
    "We have to keep the letters and numbers in text data and remove all the punctuation data. This can be achieved by using the regex function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jl1vWMvd7cL1"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "data[\"comment_text\"].replace( { r'[^a-zA-Z0-9, ]' : '' }, inplace= True, regex = True)\n",
    "data[\"comment_text\"]\n",
    "#Import Libraries\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1zy93Zc7dn4"
   },
   "source": [
    "## **Tokenize words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozJAlqy77SDl"
   },
   "source": [
    "The main implementation of the code starts by tokenizing the words present in the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmOfaFx27h7k"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_sents = [word_tokenize(i) for i in data[\"comment_text\"]]\n",
    "print(tokenized_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbsfMDLn7ibg"
   },
   "source": [
    "## **Add words to the list**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3JTq_N57mJt"
   },
   "source": [
    "Let’s create a list and add all the tokenized words to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGBjLLof7k_W"
   },
   "outputs": [],
   "source": [
    "flattened = []\n",
    "for sublist in tokenized_sents:\n",
    "    for val in sublist:\n",
    "        flattened.append(val)\n",
    "print(flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djxzMGbL7pDM"
   },
   "source": [
    "## **Add words which are not in the list**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BEq2d_37ttu"
   },
   "source": [
    "The result obtained includes a lot of duplicate words. So, another list is used to store the words that are not present in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XTaOyaK7rjJ"
   },
   "outputs": [],
   "source": [
    "Vocab=[]\n",
    "for item in flattened:\n",
    "    if not item in Vocab:\n",
    "        Vocab.append(item)\n",
    "print(Vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prpNOPgzWgiU"
   },
   "source": [
    "# **Read more articles on:**\n",
    "\n",
    "> * [Vocabulary Builder](https://analyticsindiamag.com/how-to-create-a-vocabulary-builder-for-nlp-tasks/)\n",
    "\n",
    "> * [Spacy Basics](https://analyticsindiamag.com/nlp-deep-learning-nlp-framework-nlp-model/)\n",
    "\n",
    "> * [StanfordCore NLP](https://analyticsindiamag.com/how-to-use-stanza-by-stanford-nlp-group-with-python-code/)\n",
    "\n",
    "> * [Tokenization in NLP](https://analyticsindiamag.com/hands-on-guide-to-different-tokenization-methods-in-nlp/)\n",
    "\n",
    "> * [Keras Tokenizer](https://analyticsindiamag.com/tutorial-on-keras-tokenizer-for-text-classification-in-nlp/)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOKoD8LmCjGT4RDsHmCelQ5",
   "collapsed_sections": [],
   "name": "Creating_Vocabulary.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
