{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"Continuous_bag_of_words.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNOhkLd0BQ1vN6RBPNF1YaI"},"kernelspec":{"name":"python3","display_name":"Python 3.8.8 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"f60a20abaabf5a658075b37fac599269792a9493ddacd7c14d8505185d5625aa"}},"cells":[{"cell_type":"markdown","source":["\n","# **What is the CBOW Model?**"],"metadata":{"id":"QkcfFGTBTKbd"}},{"cell_type":"markdown","source":["The CBOW model tries to understand the context of the words and takes this as input. It then tries to predict words that are contextually accurate. Let us consider an example for understanding this. Consider the sentence: ‘It is a pleasant day’ and the word ‘pleasant’ goes as input to the neural network. We are trying to predict the word ‘day’ here. We will use the one-hot encoding for the input words and measure the error rates with the one-hot encoded target word. Doing this will help us predict the output based on the word with least error. "],"metadata":{"id":"msStymJPTO8J"}},{"cell_type":"markdown","source":["## **Implementation of the CBOW Model**"],"metadata":{"id":"JOVq1v1aTxBB"}},{"cell_type":"markdown","source":["For the implementation of this model, we will use a sample text data about coronavirus. You can use any text data of your choice. But to use the data sample I have used [click here](https://github.com/bhoomikamadhukar/NLP) to download the data."],"metadata":{"id":"5qbDIk5iT0Sx"}},{"cell_type":"markdown","source":["Now that you have the data ready, let us import the libraries and read our dataset. "],"metadata":{"id":"3vpvD7YLUM4U"}},{"cell_type":"code","execution_count":2,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim --user -q --no-warn-script-location\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'status': 'ok', 'restart': True}"]},"metadata":{},"execution_count":3}],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","import keras.backend as K\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, Lambda\n","from keras.utils import np_utils\n","from keras.preprocessing import sequence\n","from keras.preprocessing.text import Tokenizer\n","import gensim\n","data=open('corona.txt','r')\n","corona_data = [text for text in data if text.count(' ') >= 2]\n","vectorize = Tokenizer()\n","vectorize.fit_on_texts(corona_data)\n","corona_data = vectorize.texts_to_sequences(corona_data)\n","total_vocab = sum(len(s) for s in corona_data)\n","word_count = len(vectorize.word_index) + 1\n","window_size = 2"],"outputs":[{"output_type":"error","ename":"Error","evalue":"Session cannot generate requests","traceback":["Error: Session cannot generate requests","at w.executeCodeCell (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:301310)","at w.execute (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:300703)","at w.start (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:296367)","at runMicrotasks (<anonymous>)","at processTicksAndRejections (internal/process/task_queues.js:93:5)","at async t.CellExecutionQueue.executeQueuedCells (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:311160)","at async t.CellExecutionQueue.start (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:310700)"]}],"metadata":{"id":"_qkLfcppS4Xr","executionInfo":{"status":"ok","timestamp":1622011900119,"user_tz":-330,"elapsed":2333,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["In the above code, I have also used the built-in method to tokenize every word in the dataset and fit our data to the tokenizer. Once that is done, we need to calculate the total number of words and the total number of sentences as well for further use. As mentioned in the model architecture, we need to assign the window size and I have assigned it to 2."],"metadata":{"id":"2bpgKUmBUVj4"}},{"cell_type":"markdown","source":["The next step is to write a function that generates pairs of the context words and the target words. The function below does exactly that. Here we have generated a function that takes in window sizes separately for target and the context and creates the pairs of contextual words and target words. "],"metadata":{"id":"EHr68EifUa5Y"}},{"cell_type":"code","execution_count":null,"source":["def cbow_model(data, window_size, total_vocab):\n","  total_length = window_size*2\n","  for text in data:\n","      text_len = len(text)\n","      for idx, word in enumerate(text):\n","          context_word = []\n","          target   = []            \n","          begin = idx - window_size\n","          end = idx + window_size + 1\n","          context_word.append([text[i] for i in range(begin, end) if 0 <= i < text_len and i != idx])\n","          target.append(word)\n","          contextual = sequence.pad_sequences(context_word, total_length=total_length)\n","          final_target = np_utils.to_categorical(target, total_vocab)\n","          yield(contextual, final_target) "],"outputs":[{"output_type":"error","ename":"Error","evalue":"Session cannot generate requests","traceback":["Error: Session cannot generate requests","at w.executeCodeCell (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:301310)","at w.execute (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:300703)","at w.start (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:296367)","at runMicrotasks (<anonymous>)","at processTicksAndRejections (internal/process/task_queues.js:93:5)","at async t.CellExecutionQueue.executeQueuedCells (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:311160)","at async t.CellExecutionQueue.start (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:310700)"]}],"metadata":{"id":"T4dy7T3bUdZV","executionInfo":{"status":"ok","timestamp":1622011900120,"user_tz":-330,"elapsed":11,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["Finally, it is time to build the neural network model that will train the CBOW on our sample data."],"metadata":{"id":"naNP2_l0Ug2U"}},{"cell_type":"code","execution_count":null,"source":["model = Sequential()\n","model.add(Embedding(input_dim=total_vocab, output_dim=100, input_length=window_size*2))\n","model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(100,)))\n","model.add(Dense(total_vocab, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","for i in range(10):\n","    cost = 0\n","    for x, y in cbow_model(data, window_size, total_vocab):\n","        cost += model.train_on_batch(contextual, final_target)\n","    print(i, cost)"],"outputs":[{"output_type":"error","ename":"Error","evalue":"Session cannot generate requests","traceback":["Error: Session cannot generate requests","at w.executeCodeCell (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:301310)","at w.execute (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:300703)","at w.start (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:296367)","at runMicrotasks (<anonymous>)","at processTicksAndRejections (internal/process/task_queues.js:93:5)","at async t.CellExecutionQueue.executeQueuedCells (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:311160)","at async t.CellExecutionQueue.start (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:310700)"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Umo_xNmiUiw6","executionInfo":{"status":"ok","timestamp":1622011900122,"user_tz":-330,"elapsed":11,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"b01e6fd4-ea2f-4166-a9b4-72eb5419ca0c"}},{"cell_type":"markdown","source":["Once we have completed the training its time to see how the model has performed and test it on some words. But to do this, we need to create a file that contains all the vectors. Later we can access these vectors using the gensim library. "],"metadata":{"id":"9OvaCb0RUlRC"}},{"cell_type":"code","execution_count":null,"source":["dimensions=100\n","vect_file = open('vectors.txt' ,'w')\n","vect_file.write('{} {}\\n'.format(total_vocab,dimensions))"],"outputs":[{"output_type":"error","ename":"Error","evalue":"Session cannot generate requests","traceback":["Error: Session cannot generate requests","at w.executeCodeCell (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:301310)","at w.execute (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:300703)","at w.start (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:296367)","at runMicrotasks (<anonymous>)","at processTicksAndRejections (internal/process/task_queues.js:93:5)","at async t.CellExecutionQueue.executeQueuedCells (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:311160)","at async t.CellExecutionQueue.start (/home/aishwarya/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:310700)"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-jVlTntmWyKF","executionInfo":{"status":"ok","timestamp":1622011900123,"user_tz":-330,"elapsed":11,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"7d338e9a-a98c-462d-cb35-8673b82bb56a"}},{"cell_type":"markdown","source":["Next, we will access the weights of the trained model and write it to the above created file."],"metadata":{"id":"gJ_ddBagXUYd"}},{"cell_type":"code","execution_count":null,"source":["weights = model.get_weights()[0]\n","for text, i in vectorize.word_index.items():\n","    final_vec = ' '.join(map(str, list(weights[i, :])))\n","    vect_file.write('{} {}\\n'.format(text, final_vec))\n","vect_file.close()"],"outputs":[],"metadata":{"id":"SgdUVj7mXWlR","executionInfo":{"status":"ok","timestamp":1622011903943,"user_tz":-330,"elapsed":838,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["Now we will use the vectors that were created and use them in the gensim model. The word I have chosen in ‘virus’."],"metadata":{"id":"k-0LhJ4bXssi"}},{"cell_type":"code","execution_count":1,"source":["cbow_output = gensim.models.KeyedVectors.load_word2vec_format('vectors.txt')\n","cbow_output.most_similar(positive=['virus'])"],"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'gensim' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6d4dc00d5cd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcbow_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vectors.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcbow_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'virus'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'gensim' is not defined"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"M0lfRddzXvCm","executionInfo":{"status":"error","timestamp":1622013303992,"user_tz":-330,"elapsed":473,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"5a29ec7f-6d5e-4584-e27d-a1feb9941a73"}},{"cell_type":"markdown","source":["# **Read more articles on:**\n","\n","> * [Continuous Bag of Words](https://analyticsindiamag.com/the-continuous-bag-of-words-cbow-model-in-nlp-hands-on-implementation-with-codes/)\n","\n","> * [NLP Case Study of Documents Similarity](https://analyticsindiamag.com/nlp-case-study-identify/)\n","\n","> * [Review Classification](https://analyticsindiamag.com/step-by-step-guide-to-reviews-classification-using-svc-naive-bayes-random-forest/)\n","\n","> * [Multi Class Text Classification](https://analyticsindiamag.com/multi-class-text-classification-in-pytorch-using-torchtext/)\n","\n","> * [Text Classification](https://analyticsindiamag.com/how-to-solve-your-first-ever-nlp-classification-challenge/)\n","\n","> * [Word Frequency](https://analyticsindiamag.com/using-natural-language-processing-to-check-word-frequency-in-the-adventure-of-sherlock-holmes/)\n","\n"],"metadata":{"id":"prpNOPgzWgiU"}}]}