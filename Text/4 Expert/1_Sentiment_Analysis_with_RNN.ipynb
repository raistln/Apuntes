{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"1_Sentiment_Analysis_with_RNN.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"m6w3ETtszQ2V"},"source":["## **In this practice session, we will learn how to implement Recurrent Neural Networks for Sentiment Analysis**\n","## **We will use imdb reviews dataset that is available in the Keras library for the implementation**"]},{"cell_type":"markdown","metadata":{"id":"mXQdoQtl2W8s"},"source":["### **Data processing**\n","  *   Import the required libraries from Keras\n","  *   Load the IMDB reviews dataset from Keras library\n","  *   Load one instance of the review and sentiment\n","  *   Pad the input data to make all input information into the same length\n","\n","### **Build an RNN model**\n","  *   Construct a simple LSTM model \n","  *   Compile the model and fit the data into the model\n","  *   Evaluate the model on unseen test data\n","  *   Make model predictions on test data\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GyyD8zH-4aF0"},"source":["## **Importing the libraries**"]},{"cell_type":"code","metadata":{"id":"iv_Ig056m44W"},"source":["#import the required libraries for the implementation\n","import numpy as np\n","import pandas as pd\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n","from keras.preprocessing import sequence\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5p3Mjnh64d0B"},"source":["## **Load the data from Keras into train and test variables**"]},{"cell_type":"code","metadata":{"id":"Da3mdAicnvht"},"source":["#load the imdb dataset into train and test set\n","from keras.datasets import imdb\n","vocabulary_size = 5000\n","\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words = vocabulary_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXhvH7LD4m6Q"},"source":["## **Print one instance of the review and corresponding sentiment**"]},{"cell_type":"code","metadata":{"id":"Y_GILIwBqINi"},"source":["#understanding how the dataset looks like\n","words = imdb.get_word_index()\n","\n","#the words are already vectorized in the dataset, hence we reverse the process to see the word distribution\n","vects = {i: word for word, i in words.items()}\n","print('review')\n","print([vects.get(i, ' ') for i in X_train[6]])\n","\n","#the sentiment is 1 if the review is positive and 0 if the review is negative\n","print('sentiment')\n","print(y_train[6])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fUfJwZ4j4shL"},"source":["## **Pad the data sequence to make the inputs into same length**"]},{"cell_type":"code","metadata":{"id":"Od_HvaisqL_v"},"source":["#for the RNN to work all our input dependencies must have same length \n","total_words = 500\n","X_train = sequence.pad_sequences(X_train, maxlen=total_words)\n","X_test = sequence.pad_sequences(X_test, maxlen=total_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aPAN7UzT4yEU"},"source":["## **Build a basic LSTM model**"]},{"cell_type":"code","metadata":{"id":"0JeymMMLqQlp"},"source":["#we will build a simple LSTM model with one embedding layer, one LSTM and one output layer\n","embedding_size=32\n","max_words = 500\n","model=Sequential()\n","model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n","model.add(LSTM(100))\n","model.add(Dense(1, activation='sigmoid'))\n","print(model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iJOZxFUh412i"},"source":["## **Compile the model**"]},{"cell_type":"code","metadata":{"id":"zjFLKJRsqSJO"},"source":["#compile the model by passing the optimizer and loss function and the evaluation metric\n","model.compile(loss='binary_crossentropy', \n","             optimizer='adam', \n","             metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hBMcurgh44iV"},"source":["## **Pass required parameters and fit the model**"]},{"cell_type":"code","metadata":{"id":"kshIa4whqUSp"},"source":["#fit the data to the model and begin training\n","batch_size = 128\n","num_epochs = 5\n","x_val, y_val = X_train[:batch_size], y_train[:batch_size]\n","xtrain, ytrain = X_train[batch_size:], y_train[batch_size:]\n","model.fit(xtrain, ytrain, validation_data=(x_val, y_val), batch_size=batch_size, nb_epoch=num_epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xTeBZOeV5m9Z"},"source":["## **Evaluate the model on test set**"]},{"cell_type":"code","metadata":{"id":"rfuN-xZ9qWs6"},"source":["#evaluate the model accuracy on unseen test data\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print('Test accuracy:', scores[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Ze29pbz5vGs"},"source":["## **Make model predictions**"]},{"cell_type":"code","metadata":{"id":"CkdQIC9lqZ1F"},"source":["#make model predictions on test data\n","print(\"Prediction: \",model.predict_classes(X_test[1:10]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xjuQQqINxtqf"},"source":["#compare the model prediction with actual data\n","print(\"Actual: \",y_test[1:10])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-97TiyaOTWCr"},"source":[""],"execution_count":null,"outputs":[]}]}