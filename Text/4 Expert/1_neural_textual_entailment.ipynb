{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"neural_textual_entailment.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"KvjLtpNuOE5c"},"source":["**Loading Datasets using Pytorch**"]},{"cell_type":"code","metadata":{"id":"V6e-QVI7_Cib","trusted":false},"source":["from torchtext import data\n","class ShiftReduceField(data.Field):\n","    def __init__(self):\n","\n","        super(ShiftReduceField, self).__init__(preprocessing=lambda parse: [\n","            'reduce' if t == ')' else 'shift' for t in parse if t != '('])\n","\n","        self.build_vocab([['reduce'], ['shift']])\n","\n","class ParsedTextField(data.Field):\n","    def __init__(self, eos_token='<pad>', lower=False, reverse=False):\n","        if reverse:\n","            super(ParsedTextField, self).__init__(\n","                eos_token=eos_token, lower=lower,\n","                preprocessing=lambda parse: [t for t in parse if t not in ('(', ')')],\n","                postprocessing=lambda parse, _: [list(reversed(p)) for p in parse],\n","                include_lengths=True)\n","        else:\n","            super(ParsedTextField, self).__init__(\n","                eos_token=eos_token, lower=lower,\n","                preprocessing=lambda parse: [t for t in parse if t not in ('(', ')')],\n","                include_lengths=True)\n","            \n","class NLIDataset(data.TabularDataset):\n","\n","    urls = []\n","    directoryname = ''\n","    namenli = 'nli'\n","\n","    @staticmethod\n","    def sort_key(ex):\n","        return data.interleave_keys(\n","            len(ex.premise), len(ex.hypothesis))\n","\n","    @classmethod\n","    def splits(cls, text_field, label_field, parse_field=None,\n","               extra_fields={}, root='.data', train='train.jsonl',\n","               validation='val.jsonl', test='test.jsonl'):\n","        path = cls.download(root)\n","\n","        if parse_field is None:\n","            fields = {'sentence1': ('premise', text_field),\n","                      'sentence2': ('hypothesis', text_field),\n","                      'gold_label': ('label', label_field)}\n","        else:\n","            fields = {'sentence1_binary_parse': [('premise', text_field),\n","                                                 ('premise_transitions', parse_field)],\n","                      'sentence2_binary_parse': [('hypothesis', text_field),\n","                                                 ('hypothesis_transitions', parse_field)],\n","                      'gold_label': ('label', label_field)}\n","\n","        for key in extra_fields:\n","            if key not in fields.keys():\n","                fields[key] = extra_fields[key]\n","\n","        return super(NLIDataset, cls).splits(\n","            path, root, train, validation, test,\n","            format='json', fields=fields,\n","            filter_pred=lambda ex: ex.label != '-')\n","\n","    @classmethod\n","    def iters(cls, batch_size=32, device=0, root='.data',\n","              vectors=None, trees=False, **kwargs):\n","        if trees:\n","            text = ParsedTextField()\n","            transitions = ShiftReduceField()\n","        else:\n","            text = data.Field(tokenize='spacy')\n","            transitions = None\n","        label = data.Field(sequential=False)\n","\n","        train, val, test = cls.splits(\n","            text, label, transitions, root=root, **kwargs)\n","\n","        Text.build_vocab(train, vectors=vectors)\n","        label.build_vocab(train)\n","\n","        return data.BucketIterator.splits(\n","            (train, val, test), batch_size=batch_size, device=device)\n","\n","\n","class SNLI(NLIDataset):\n","    urls = ['http://nlp.stanford.edu/projects/snli/snli_1.0.zip']\n","    directoryname = 'snli_1.0'\n","    name1 = 'snli'\n","\n","    @classmethod\n","    def splits(cls, text_field, label_field, parse_field=None, root='.data',\n","               train='snli_1.0_train.jsonl', validation='snli_1.0_dev.jsonl',\n","               test='snli_1.0_test.jsonl'):\n","        return super(SNLI, cls).splits(text_field, label_field, parse_field=parse_field,\n","                                       root=root, train=train, validation=validation,\n","                                       test=test)\n","\n","\n","\n","class MultiNLI(NLIDataset):\n","    urls = ['http://www.nyu.edu/projects/bowman/multinli/multinli_1.0.zip']\n","    directoryname = 'multinli_1.0'\n","    name2 = 'multinli'\n","\n","    @classmethod\n","    def splits(cls, text_field, label_field, parse_field=None, genre_field=None,\n","               root='.data',\n","               train='multinli_1.0_train.jsonl',\n","               validation='multinli_1.0_dev_matched.jsonl',\n","               test='multinli_1.0_dev_mismatched.jsonl'):\n","        extra_fields = {}\n","        if genre_field is not None:\n","            extra_fields[\"genre\"] = (\"genre\", genre_field)\n","\n","        return super(MultiNLI, cls).splits(text_field, label_field,\n","                                           parse_field=parse_field,\n","                                           extra_fields=extra_fields,\n","                                           root=root, train=train,\n","                                           validation=validation, test=test)\n","\n","\n","\n","class XNLI(NLIDataset):\n","    urls = ['http://www.nyu.edu/projects/bowman/xnli/XNLI-1.0.zip']\n","    directoryname = 'XNLI-1.0'\n","    name3 = 'xnli'\n","\n","    @classmethod\n","    def splits(cls, text_field, label_field, genre_field=None, language_field=None,\n","               root='.data',\n","               validation='xnli.dev.jsonl',\n","               test='xnli.test.jsonl'):\n","        extra_fields = {}\n","        if genre_field is not None:\n","            extra_fields[\"genre\"] = (\"genre\", genre_field)\n","        if language_field is not None:\n","            extra_fields[\"language\"] = (\"language\", language_field)\n","\n","        return super(XNLI, cls).splits(text_field, label_field,\n","                                       extra_fields=extra_fields,\n","                                       root=root, train=None,\n","                                       validation=validation, test=test)\n","\n","    @classmethod\n","    def iters(cls, *args, **kwargs):\n","        raise NotImplementedError('XNLI dataset does not support iters')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uFyafPq6TZu7"},"source":["**Loading Datasets using Tensorflow**"]},{"cell_type":"code","metadata":{"id":"Hp06sXwYTb65","trusted":false},"source":["import csv\n","import os\n","\n","import tensorflow.compat.v2 as tf\n","import tensorflow_datasets.public_api as tfds\n","\n","url = 'https://nlp.stanford.edu/projects/snli/snli_1.0.zip'\n","\n","\n","class Snli(tfds.core.GeneratorBasedBuilder):\n","\n","  VERSION = tfds.core.Version('1.1.0')\n","\n","  def _info(self):\n","    return tfds.core.DatasetInfo(\n","        builder=self,\n","        features=tfds.features.FeaturesDict({\n","            'premise':\n","                tfds.features.Text(),\n","            'hypothesis':\n","                tfds.features.Text(),\n","            'label':\n","                tfds.features.ClassLabel(\n","                    names=['entailment', 'neutral', 'contradiction']),\n","        }),\n","        supervised_keys=None,\n","        homepage='https://nlp.stanford.edu/projects/snli/',\n","    )\n","\n","  def _split_generators(self, dl_manager):\n","    dl_directory = dl_manager.download_and_extract(url)\n","    data_directory = os.path.join(dl_directory, 'snli_1.0')\n","    return [\n","        tfds.core.SplitGenerator(\n","            name=tfds.Split.TRAIN,\n","            gen_kwargs={\n","                'filepath': os.path.join(data_directory, 'snli_1.0_train.txt')\n","            }),\n","    ]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U3SmPwXhTrsb","trusted":false},"source":["import os\n","import tensorflow.compat.v2 as tf\n","import tensorflow_datasets.public_api as tfds\n","\n","class MultiNLI(tfds.core.GeneratorBasedBuilder)\n","\n","  VERSION = tfds.core.Version(\"1.1.0\")\n","\n","  def _info(self):\n","    return tfds.core.DatasetInfo(\n","        builder=self,\n","        features=tfds.features.FeaturesDict({\n","            \"premise\":\n","                tfds.features.Text(),\n","            \"hypothesis\":\n","                tfds.features.Text(),\n","            \"label\":\n","                tfds.features.ClassLabel(\n","                    names=[\"entailment\", \"neutral\", \"contradiction\"]),\n","        }),\n","        \n","        supervised_keys=None,\n","        homepage=\"https://www.nyu.edu/projects/bowman/multinli/\",\n","        \n","    )\n","\n","  def _split_generators(self, dl_manager):\n","\n","    downloaded_dir = dl_manager.download_and_extract(\n","        \"https://cims.nyu.edu/~sbowman/multinli/multinli_1.0.zip\")\n","    multinli_path = os.path.join(downloaded_dir, \"multinli_1.0\")\n","    train_path = os.path.join(multinli_path, \"multinli_1.0_train.txt\")\n","    matched_validation_path = os.path.join(multinli_path,\n","                                           \"multinli_1.0_dev_matched.txt\")\n","    mismatched_validation_path = os.path.join(\n","        multinli_path, \"multinli_1.0_dev_mismatched.txt\")\n","\n","    return [\n","        tfds.core.SplitGenerator(\n","            name=tfds.Split.TRAIN,\n","            gen_kwargs={\"filepath\": train_path}),\n","    ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R4BMpjs1UEgX","trusted":false},"source":["import collections\n","import csv\n","import os\n","import six\n","\n","import tensorflow.compat.v2 as tf\n","import tensorflow_datasets.public_api as tfds\n","\n","url = 'https://cims.nyu.edu/~sbowman/xnli/XNLI-1.0.zip'\n","\n","languages = ('ar', 'bg', 'de', 'el', 'en', 'es', 'fr', 'hi', 'ru', 'sw', 'th',\n","              'tr', 'ur', 'vi', 'zh')\n","\n","\n","class Xnli(tfds.core.GeneratorBasedBuilder):\n","\n","  VERSION = tfds.core.Version('1.1.0')\n","\n","  def _info(self):\n","    return tfds.core.DatasetInfo(\n","        builder=self,\n","        features=tfds.features.FeaturesDict({\n","            'premise':\n","                tfds.features.Translation(\n","                    languages=languages,),\n","            'hypothesis':\n","                tfds.features.TranslationVariableLanguages(\n","                    languages=languages,),\n","            'label':\n","                tfds.features.ClassLabel(\n","                    names=['entailment', 'neutral', 'contradiction']),\n","        }),\n","        supervised_keys=None,\n","        homepage='https://www.nyu.edu/projects/bowman/xnli/',\n","    )\n","\n","  def _split_generators(self, dl_manager):\n","    dl_directory = dl_manager.download_and_extract(url)\n","    data_directory = os.path.join(dl_directory, 'XNLI-1.0')\n","    return [\n","        tfds.core.SplitGenerator(\n","            name=tfds.Split.TEST,\n","            gen_kwargs={'filepath': os.path.join(data_directory, 'xnli.test.tsv')}),\n","    ]\n"],"execution_count":null,"outputs":[]}]}