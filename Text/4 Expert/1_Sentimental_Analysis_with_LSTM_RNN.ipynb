{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"Sentimental_Analysis_with_LSTM_RNN.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOj/EVEF9Pnfnc4YE5drwbP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **What are Recurrent Neural Networks and Long Short Term Memory?**"],"metadata":{"id":"C1RhJ2lZLuV0"}},{"cell_type":"markdown","source":["In feed-forward networks, inputs are multiplied by a weight and then bias is added to that and so on and finally we get output from the last layer. But the problem with these types of networks is they do not store memory and cannot be used in sequential data. Even the input and output of this type of network is fixed. We cannot use these types of networks for problems like Stock Price prediction and similar problems. \n","\n","This is the reason Recurrent Neural Networks (RNN) was introduced. RNN was designed in a way such that they can catch the sequential / time series data. In RNN, we multiply with the weight associated with the input of the previous state (w1) and weight associated with output for the previous state. And then we pass them to the Tanh function to get the new state. Now to get the output vector we multiply the new state with an output of Tanh function. Deep networks are not preferred in RNN. \n","\n","But RNN suffers from a vanishing gradient problem that is very significant changes in the weights that do not help the model learn. To overcome this LSTM was introduced. "],"metadata":{"id":"QalY1U55L2GG"}},{"cell_type":"markdown","source":["## **Sentiment Analysis using LSTM**"],"metadata":{"id":"8SNvCeeJL8c5"}},{"cell_type":"markdown","source":["Let us first import the required libraries and data. You can import the data directly from [Kaggle](https://www.kaggle.com/seunowo/sentiment-analysis-twitter-dataset) and use it. There are also many publicly available datasets for sentiment analysis of tweets and reviews. We will use the Twitter Sentiment Data for this experiment. Use the below code to the same. "],"metadata":{"id":"Z27-OEeIMAm7"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn keras tensorflow nltk gensim --user -q --no-warn-script-location\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","import pandas as pd\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils.np_utils import to_categorical\n","import re\n","\n","df = pd.read_csv(\"Sentiment.csv\")"],"outputs":[],"metadata":{"id":"_Ryjnj2cJZOR","executionInfo":{"status":"ok","timestamp":1622177318836,"user_tz":-330,"elapsed":2694,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["We will now explore the data we just imported. We will first see what all is present in the data. We have checked the different columns for that."],"metadata":{"id":"j7CO8L0kM50Z"}},{"cell_type":"code","execution_count":null,"source":["print(df.columns)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FY74DJbvM7c-","executionInfo":{"status":"ok","timestamp":1622177318837,"user_tz":-330,"elapsed":17,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"2ec11a40-2163-4987-c2ca-323f10b8d808"}},{"cell_type":"markdown","source":["We will only use the tweets and their corresponding sentiments in this experiment. So we will create a new data frame that will only hold these two columns. We will also check the different sentiments present. Use the below code to the same."],"metadata":{"id":"y23JmJT0M71H"}},{"cell_type":"code","execution_count":null,"source":["new_df = df[['text','sentiment']]\n","print(new_df.sentiment)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"41JyhZCmNCYl","executionInfo":{"status":"ok","timestamp":1622177318839,"user_tz":-330,"elapsed":14,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"69fd7137-ee86-4e5c-e763-b304d203924c"}},{"cell_type":"markdown","source":["### **Preprocessing Of Tweets**"],"metadata":{"id":"-jyYFPjsNGxX"}},{"cell_type":"markdown","source":["We will now preprocess the tweets by excluding unnecessary things from text and convert them to lowercase. Use the below code to perform this."],"metadata":{"id":"LRArV5ivNOnG"}},{"cell_type":"code","execution_count":null,"source":["new_df = new_df[new_df.sentiment != \"Neutral\"]\n","new_df['text'] = new_df['text'].str.lower()\n","new_df['text'] = new_df['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))"],"outputs":[],"metadata":{"id":"ZY9ImdUwNRKg","executionInfo":{"status":"ok","timestamp":1622177318840,"user_tz":-330,"elapsed":10,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["After this, we will define the vocabulary size that is to be used and use tokenizer to convert them into vectors. We have stored that into the X variable. Use the below code to do so. "],"metadata":{"id":"pDRf7cJFNhmZ"}},{"cell_type":"code","execution_count":null,"source":["max_fatures = 2000\n","tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n","tokenizer.fit_on_texts(new_df['text'].values)\n","X = tokenizer.texts_to_sequences(new_df['text'].values)\n","X = pad_sequences(X)"],"outputs":[],"metadata":{"id":"b3xPuOWpNiIZ","executionInfo":{"status":"ok","timestamp":1622177328184,"user_tz":-330,"elapsed":996,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["We then define the LSTM model architecture. Use the below code to define it. The network is similar to Convents networks. The only difference is we have defined two hyperparameters that are embed_dim and lstm_out.  We have then compiled the model using adam optimizer and binary cross-entropy loss."],"metadata":{"id":"FDeaGkA4NmSL"}},{"cell_type":"code","execution_count":null,"source":["embed_dim = 128\n","lstm_out = 196\n","\n","model = Sequential()\n","model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n","model.add(SpatialDropout1D(0.4))\n","model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(2,activation='softmax'))\n","model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n","print(model.summary())"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o-w2o554Noh0","executionInfo":{"status":"ok","timestamp":1622177330183,"user_tz":-330,"elapsed":485,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"ba21dd56-3e0e-4171-9f8e-52b4f533aff4"}},{"cell_type":"markdown","source":["After this, we encode the sentiments using Label encoder. Use the below code to do that. We have stored the tweets into X and corresponding sentiments into Y."],"metadata":{"id":"wfUHfh7mNsce"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.preprocessing import LabelEncoder\n","Le = LabelEncoder()\n","y = Le.fit_transform(new_df['sentiment'])"],"outputs":[],"metadata":{"id":"MxRk9CjENwDv","executionInfo":{"status":"ok","timestamp":1622177332622,"user_tz":-330,"elapsed":347,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}}}},{"cell_type":"markdown","source":["Then we divide the data set into training and testing sets. Use the below code to do so. After which we passed the training data and validation data to the model. "],"metadata":{"id":"Un-hUkjWNyVq"}},{"cell_type":"code","execution_count":null,"source":["Y = pd.get_dummies(new_df['sentiment']).values\n","X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n","print(X_train.shape,Y_train.shape)\n","print(X_test.shape,Y_test.shape)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Qp8-Pg6N0dt","executionInfo":{"status":"ok","timestamp":1622177342796,"user_tz":-330,"elapsed":370,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"1fd5c83a-4f00-455f-cd1c-832cdf50dbf3"}},{"cell_type":"code","execution_count":null,"source":["batch_size = 32\n","model.fit(X_train, Y_train, epochs = 7, batch_size=batch_size, verbose = 2)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6EQHWgbyTMzr","executionInfo":{"status":"ok","timestamp":1622177604222,"user_tz":-330,"elapsed":259453,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"6f4cd6c9-6dcd-444e-e177-372c36f4bde9"}},{"cell_type":"markdown","source":["Now we will evaluate the model performance. Use the below code to evaluate the model. "],"metadata":{"id":"bpxsYAnkN2o4"}},{"cell_type":"code","execution_count":null,"source":["model.evaluate(X_test,Y_test)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eayYVNyAVtcu","executionInfo":{"status":"ok","timestamp":1622177967568,"user_tz":-330,"elapsed":4063,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"}},"outputId":"999d5f92-2c67-452a-fd55-bea20ba49d45"}},{"cell_type":"markdown","source":["We got 83% accuracy and loss of 0.43."],"metadata":{"id":"BFuH6hkQN42f"}},{"cell_type":"markdown","source":["# **Related Articles:**\n","\n","> * [Sentiment Analysis using LSTM](https://analyticsindiamag.com/how-to-implement-lstm-rnn-network-for-sentiment-analysis/)\n","\n","> * [VADER Sentiment Analysis](https://analyticsindiamag.com/sentiment-analysis-made-easy-using-vader/)\n","\n","> * [Polyglot](https://analyticsindiamag.com/hands-on-tutorial-on-polyglot-python-toolkit-for-multilingual-nlp-applications/)\n","\n","> * [Textblob](https://analyticsindiamag.com/lets-learn-textblob-quickstart-a-python-library-for-processing-textual-data/)\n","\n","> * [TextHero Guide](https://analyticsindiamag.com/texthero-guide-a-python-toolkit-for-text-processing/)\n","\n","> * [Guide to Pattern](https://analyticsindiamag.com/hands-on-guide-to-pattern-a-python-tool-for-effective-text-processing-and-data-mining/)\n","\n"],"metadata":{"id":"prpNOPgzWgiU"}}]}