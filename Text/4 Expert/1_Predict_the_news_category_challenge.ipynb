{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"Predict_the_news_category_challenge.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyObgg5KTlixQgntr8xmTKDM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Predict The News Category Hackathon**"],"metadata":{"id":"8Ogx3MKx0hqF"}},{"cell_type":"markdown","source":["The hackathon is about predicting the category or section of news from its content.The dataset consists of news pieces collected from a number of different sources along with the category or section of the news piece in which it was featured."],"metadata":{"id":"a6LFLI541BU9"}},{"cell_type":"markdown","source":["Given below is the description of the dataset.\n","\n","Size of training set: 7,628 records\n","\n","Size of test set: 2,748 records"],"metadata":{"id":"qJPjLHWr1D9x"}},{"cell_type":"markdown","source":["FEATURES:\n","\n","STORY: A part of the main content of the article to be published as a piece of news.\n","\n","SECTION: The genre/category the STORY falls in.\n","\n","There are four distinct sections where each story may fall in to. The Sections are labelled as follows :\n","\n","Politics: 0\n","\n","Technology: 1\n","\n","Entertainment: 2\n","\n","Business: 3"],"metadata":{"id":"APSAJ0H21HmK"}},{"cell_type":"markdown","source":["## **Getting The Datasets**"],"metadata":{"id":"mTTBzyL51Mpg"}},{"cell_type":"markdown","source":["Go to MachineHack, Sign Up as a user and click on the [Predict The News Category Hackathon](https://machinehack.com/hackathons/predict_the_news_category_hackathon/data). Start the hackathon and find the dataset in the Attachment section.\n","\n","Click here to register for the hackathon \n","\n","Without further ado, let’s crack the Hackathon!"],"metadata":{"id":"QORxWCe41Slo"}},{"cell_type":"markdown","source":["## **Solving The Hackathon**"],"metadata":{"id":"BVFOE8s01fYs"}},{"cell_type":"markdown","source":["Let’s break the solution into 6 parts as given below for better understanding.\n","\n","1. Exploratory Data Analysis: A Simple analysis of Data \n","2. Data cleaning \n","3. Data preprocessing: Count Vectors and TF-IDF Vectors\n","4. Training the classifier\n","5. Predicting for the test set\n","6. Submitting your solution at MachineHack\n"],"metadata":{"id":"GJ5WxK3y1icH"}},{"cell_type":"markdown","source":["## **Exploratory Data Analysis: A Simple analysis of Data**"],"metadata":{"id":"fKVDPXJ91qDp"}},{"cell_type":"markdown","source":["Let’s start off with the usual drill and import all the necessary modules for our project."],"metadata":{"id":"geHZZPww1sKZ"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim --user -q --no-warn-script-location\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["#Importing the libraries\n","import pandas as pd\n","import nltk\n","from nltk.corpus import stopwords\n","import string\n","#Download the following modules once\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"outputs":[],"metadata":{"id":"Jj9aTMUv0XYw"}},{"cell_type":"markdown","source":["Let’s do a simple analysis of the data in hand."],"metadata":{"id":"W-kbfBOF1wdv"}},{"cell_type":"code","execution_count":null,"source":["#Importing the training set\n","train_data = pd.read_excel(\"Data_Train.xlsx\")\n","\n","#Printing the top 5 rows\n","print(train_data.head(5))"],"outputs":[],"metadata":{"id":"HkIiev4H1zL-"}},{"cell_type":"code","execution_count":null,"source":["#Printing the dataset info\n","print(train_data.info())"],"outputs":[],"metadata":{"id":"zK4rSr6S2zyR"}},{"cell_type":"code","execution_count":null,"source":["#Printing the shape of the dataset\n","print(train_data.shape)"],"outputs":[],"metadata":{"id":"sSO8w91o22jE"}},{"cell_type":"code","execution_count":null,"source":["#Printing the group by description of each category\n","train_data.groupby(\"SECTION\").describe()"],"outputs":[],"metadata":{"id":"MoxzJhwU28Hn"}},{"cell_type":"markdown","source":["## **Data Cleaning**"],"metadata":{"id":"sOV05f9d2-IX"}},{"cell_type":"code","execution_count":null,"source":["#Removing duplicates to avoid overfitting\n","train_data.drop_duplicates(inplace = True)\n","\n","#A punctuations string for reference (added other valid characters from the dataset)\n","all_punctuations = string.punctuation + '‘’,:”][],' \n","\n","#Method to remove punctuation marks from the data\n","def punc_remover(raw_text):\n","  no_punct = \"\".join([i for i in raw_text if i not in all_punctuations])\n","  return no_punct\n","\n","#Method to remove stopwords from the data\n","def stopword_remover(no_punc_text):\n","  words = no_punc_text.split()\n","  no_stp_words = \" \".join([i for i in words if i not in stopwords.words('english')])\n","  return no_stp_words\n","\n","#Method to lemmatize the words in the data\n","lemmer = nltk.stem.WordNetLemmatizer()\n","def lem(words):\n","  return \" \".join([lemmer.lemmatize(word,'v') for word in words.split()])\n","\n","#Method to perform a complete cleaning\n","def text_cleaner(raw):\n","  cleaned_text = stopword_remover(punc_remover(raw))\n","  return lem(cleaned_text)\n","\n","#Testing the cleaner method\n","text_cleaner(\"Hi!, this is a sample text to test the text cleaner method. Removes *@!#special characters%$^* and stopwords. And lemmatizes, go, going - run, ran, running\")\n","\n","# Out: 'Hi sample text test text cleaner method Removes special character stopwords And lemmatizes go go run run run'\n","\n","#Applying the cleaner method to the entire data\n","train_data['CLEAN_STORY'] = train_data['STORY'].apply(text_cleaner)\n","\n","#Checking the new dataset\n","print(train_data.values) "],"outputs":[],"metadata":{"id":"ttZikd5A3C8H"}},{"cell_type":"markdown","source":["## **Data Preprocessing: Count Vectors and TF-IDF Vectors**"],"metadata":{"id":"bEQcalhb3vJW"}},{"cell_type":"markdown","source":["### **Creating Count vectors**"],"metadata":{"id":"A6cHAAJl3xgG"}},{"cell_type":"code","execution_count":null,"source":["#Importing sklearn’s Countvectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","#Creating a bag-of-words dictionary of words from the data\n","bow_dictionary = CountVectorizer().fit(train_data['CLEAN_STORY'])\n","\n","#Total number of words in the bow_dictionary\n","len(bow_dictionary.vocabulary_)"],"outputs":[],"metadata":{"id":"vM0IlxXi30Xu"}},{"cell_type":"code","execution_count":null,"source":["#Using the bow_dictionary to create count vectors for the cleaned data.\n","bow = bow_dictionary.transform(train_data['CLEAN_STORY'])\n","\n","#Printing the shape of the bag of words model\n","print(bow.shape)"],"outputs":[],"metadata":{"id":"lU_sZT-E323H"}},{"cell_type":"markdown","source":["### **Creating TF-IDF Vectors**"],"metadata":{"id":"HrnA2j1i34vf"}},{"cell_type":"code","execution_count":null,"source":["#Importing TfidfTransformer from sklearn\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","#Fitting the bag of words data to the TF-IDF transformer\n","tfidf_transformer = TfidfTransformer().fit(bow)\n","\n","#Transforming the bag of words model to TF-IDF vectors\n","storytfidf = tfidf_transformer.transform(bow)"],"outputs":[],"metadata":{"id":"DdDOs7kH37za"}},{"cell_type":"markdown","source":["## **Training The Classifier**"],"metadata":{"id":"iDbGmnJO4DJA"}},{"cell_type":"code","execution_count":null,"source":["#Creating a Multinomial Naive Bayes Classifier\n","from sklearn.naive_bayes import MultinomialNB\n","\n","#Fitting the training data to the classifier\n","classifier = MultinomialNB().fit(storytfidf, train_data['SECTION'])"],"outputs":[],"metadata":{"id":"HiOjUCYN4HOg"}},{"cell_type":"markdown","source":["## **Predicting For The Test Set**"],"metadata":{"id":"0aEuaQIw4I62"}},{"cell_type":"code","execution_count":null,"source":["#Importing and cleaning the test data\n","test_data = pd.read_excel(\"Data_Test.xlsx\")\n","test_data['CLEAN_STORY'] = test_data['STORY'].apply(text_cleaner)\n","\n","#Printing the cleaned data\n","print(test_data.values)"],"outputs":[],"metadata":{"id":"fpjdcNZC4MRF"}},{"cell_type":"markdown","source":["## **Creating A Pipeline To Pre-Process The Data & Initialise The Classifier**"],"metadata":{"id":"hilcttBB4Qzz"}},{"cell_type":"code","execution_count":null,"source":["#Importing the Pipeline module from sklearn\n","from sklearn.pipeline import Pipeline\n","\n","#Initializing the pipeline with necessary transformations and the required classifier\n","pipe = Pipeline([\n","('bow', CountVectorizer()),\n","('tfidf', TfidfTransformer()),\n","('classifier', MultinomialNB())])\n","\n","#Fitting the training data to the pipeline\n","pipe.fit(train_data['CLEAN_STORY'], train_data['SECTION'])\n","\n","#Predicting the SECTION\n","test_preds_mnb = pipe.predict(test_data['CLEAN_STORY'])\n","\n","#Writing the predictions to an excel sheet\n","pd.DataFrame(test_preds_mnb, columns = ['SECTION']).to_excel(\"predictions.xlsx\")"],"outputs":[],"metadata":{"id":"OlFECT4t4WLV"}},{"cell_type":"markdown","source":["# **Related Articles:**\n","\n","> * [Predict the News Category](https://analyticsindiamag.com/guide-to-cracking-machinehacks-predict-the-news-category-hackathon/)\n","\n","> * [Guide to Sense2vec](https://analyticsindiamag.com/guide-to-sense2vec-contextually-keyed-word-vectors-for-nlp/)\n","\n","> * [Download Twitter Data and Analyze](https://analyticsindiamag.com/hands-on-guide-to-download-analyze-and-visualize-twitter-data/)\n","\n","> * [Sentiment Analysis using LSTM](https://analyticsindiamag.com/how-to-implement-lstm-rnn-network-for-sentiment-analysis/)\n","\n","> * [VADER Sentiment Analysis](https://analyticsindiamag.com/sentiment-analysis-made-easy-using-vader/)\n","\n","> * [Polyglot](https://analyticsindiamag.com/hands-on-tutorial-on-polyglot-python-toolkit-for-multilingual-nlp-applications/)"],"metadata":{"id":"prpNOPgzWgiU"}}]}