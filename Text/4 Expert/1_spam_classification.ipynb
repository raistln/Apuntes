{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"1_spam_classification.ipynb","provenance":[],"collapsed_sections":["wripxz0xT1eF","N5Lz8iQAT1eY","5FMXzJoeT1eb","TkDiOQ7TT1er","cR-AVicuT1fS","lN_sj7EhT1fb","5vnnkK2fT1fh","ryfz7ed_T1f2","gSExV8qIT1gC"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"l1oboG2WIgm5"},"source":["## **In this practice we will use natural language processing to build a spam classifier**\n","\n","## **We will make use of the sms spam classification data for the implementation**"]},{"cell_type":"markdown","metadata":{"id":"QT0e91prLONA"},"source":["**Data processing**\n","*   Import the required packages\n","*   Load the data into train and test variables\n","*   Remove the unwanted data columns\n","*   Build wordcloud to see which message is spam and which is not.\n","*   Remove the stop words and punctuations\n","*   Convert the text data into vectors\n","\n","**Building a classification model**\n","*   Split the data into train and test sets\n","*   Use Sklearn built in classifiers to build the models\n","*   Train the data on the model\n","*   Make predictions on new data\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8ag7hrorT1dS"},"source":["## **Import the required packages that is to be used**"]},{"cell_type":"code","metadata":{"id":"lWibLvVTT1dS"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import csv\n","import sklearn\n","import pickle\n","from wordcloud import WordCloud\n","import pandas as pd\n","import numpy as np\n","import nltk\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.tree import DecisionTreeClassifier \n","from sklearn.model_selection import GridSearchCV,train_test_split,StratifiedKFold,cross_val_score,learning_curve"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vS4d4NtJT1dW"},"source":["## **Preprocessing and Exploring the Dataset**"]},{"cell_type":"code","metadata":{"id":"KcTtyR6vT1dY"},"source":["data = pd.read_csv('spam.csv', encoding='latin-1')\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7OHtOc2jT1dc"},"source":["## **Removing unwanted columns**"]},{"cell_type":"code","metadata":{"id":"sclvYeoVT1dd"},"source":["data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n","data = data.rename(columns={\"v2\" : \"text\", \"v1\":\"label\"})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LPCJVr9zT1di"},"source":["data[1990:2000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zNWfIdYvT1dm","scrolled":true},"source":["data['label'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BUaVXJY9T1d1"},"source":["# Import nltk packages and Punkt Tokenizer Models\n","import nltk\n","nltk.download(\"punkt\")\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J2Fi_co9T1d4"},"source":["### **WordClouds- to see which words are common in SPAM and NOT SPAM mesaages**"]},{"cell_type":"code","metadata":{"id":"6cbnNrgnT1d5"},"source":["ham_words = ''\n","spam_words = ''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2xtSTyueT1d8"},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o13b2oOET1eB"},"source":["# Creating a corpus of spam messages\n","for val in data[data['label'] == 'spam'].text:\n","    text = val.lower()\n","    tokens = nltk.word_tokenize(text)\n","    for words in tokens:\n","        spam_words = spam_words + words + ' '\n","# Creating a corpus of ham messages        \n","for val in data[data['label'] == 'ham'].text:\n","    text = val.lower()\n","    tokens = nltk.word_tokenize(text)\n","    for words in tokens:\n","        ham_words = ham_words + words + ' '"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wripxz0xT1eF"},"source":["## **Creating  Spam wordcloud and ham wordcloud**"]},{"cell_type":"code","metadata":{"id":"Zv7Fryv5T1eG"},"source":["spam_wordcloud = WordCloud(width=500, height=300).generate(spam_words)\n","ham_wordcloud = WordCloud(width=500, height=300).generate(ham_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RIHFH-BJT1eJ"},"source":["#Spam Word cloud\n","plt.figure( figsize=(10,8), facecolor='w')\n","plt.imshow(spam_wordcloud)\n","plt.axis(\"off\")\n","plt.tight_layout(pad=0)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yHgEfutRT1eO"},"source":["#Creating Ham wordcloud\n","plt.figure( figsize=(10,8), facecolor='g')\n","plt.imshow(ham_wordcloud)\n","plt.axis(\"off\")\n","plt.tight_layout(pad=0)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"opaPXshjT1eR"},"source":["data = data.replace(['ham','spam'],[0, 1]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nRveTZ9GT1eV"},"source":["data.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N5Lz8iQAT1eY"},"source":["## **Removing Stopwords from the messages**"]},{"cell_type":"code","metadata":{"id":"GO4u5sUqT1eY"},"source":["import nltk\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5FMXzJoeT1eb"},"source":["## **Remove punctuation  and stopwords**"]},{"cell_type":"code","metadata":{"id":"edDpGjhST1ec"},"source":["#remove the punctuations and stopwords\n","import string\n","def text_process(text):\n","    \n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n","    \n","    return \" \".join(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nUqNFnNPT1ef"},"source":["data['text'] = data['text'].apply(text_process)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MEsCTryOT1ek"},"source":["data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k0ZAwZiAT1en"},"source":["text = pd.DataFrame(data['text'])\n","label = pd.DataFrame(data['label'])\n","label"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TkDiOQ7TT1er"},"source":["## **Converting words to vectors**\n","\n"]},{"cell_type":"code","metadata":{"id":"FLk0XaOCT1es"},"source":["## Counting how many times a word appears in the dataset\n","\n","from collections import Counter\n","\n","total_counts = Counter()\n","for i in range(len(text)):\n","    for word in text.values[i][0].split(\" \"):\n","        total_counts[word] += 1\n","\n","print(\"Total words in data set: \", len(total_counts))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AGioZnysT1e7"},"source":["# Sorting in decreasing order (Word with highest frequency appears first)\n","vocab = sorted(total_counts, key=total_counts.get, reverse=True)\n","print(vocab[:60])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pme-lXRfT1e_"},"source":["# Mapping from words to index\n","\n","vocab_size = len(vocab)\n","word2idx = {}\n","#print vocab_size\n","for i, word in enumerate(vocab):\n","    word2idx[word] = i"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wvKOwVGBT1fF"},"source":["# Text to Vector\n","def text_to_vector(text):\n","    word_vector = np.zeros(vocab_size)\n","    for word in text.split(\" \"):\n","        if word2idx.get(word) is None:\n","            continue\n","        else:\n","            word_vector[word2idx.get(word)] += 1\n","    return np.array(word_vector)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HlQa0HhQT1fJ"},"source":["# Convert all titles to vectors\n","word_vectors = np.zeros((len(text), len(vocab)), dtype=np.int_)\n","for i, (_, text_) in enumerate(text.iterrows()):\n","    word_vectors[i] = text_to_vector(text_[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cR-AVicuT1fS"},"source":["## **Converting words to vectors using TFIDF Vectorizer**"]},{"cell_type":"code","metadata":{"id":"1p_1l3LrVsaB"},"source":["#convert the text data into vectors\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer()\n","vectors = vectorizer.fit_transform(data['text'])\n","vectors.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C9fGfx5sT1fX"},"source":["#features = word_vectors\n","features = vectors"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lN_sj7EhT1fb"},"source":["## **Splitting into training and test set**"]},{"cell_type":"code","metadata":{"id":"KKy_fPvST1fc"},"source":["#split the dataset into train and test set\n","X_train, X_test, y_train, y_test = train_test_split(features, data['label'], test_size=0.15, random_state=111)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5vnnkK2fT1fh"},"source":["## **Classifying using sklearn pre built classifiers**"]},{"cell_type":"code","metadata":{"id":"KOPj1HQhT1fi"},"source":["#import sklearn packages for building classifiers\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y1CE9S30T1fl"},"source":["#initialize multiple classification models \n","svc = SVC(kernel='sigmoid', gamma=1.0)\n","knc = KNeighborsClassifier(n_neighbors=49)\n","mnb = MultinomialNB(alpha=0.2)\n","dtc = DecisionTreeClassifier(min_samples_split=7, random_state=111)\n","lrc = LogisticRegression(solver='liblinear', penalty='l1')\n","rfc = RandomForestClassifier(n_estimators=31, random_state=111)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fP-v8pdvT1fo"},"source":["#create a dictionary of variables and models\n","clfs = {'SVC' : svc,'KN' : knc, 'NB': mnb, 'DT': dtc, 'LR': lrc, 'RF': rfc}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WimrKW_XT1fv"},"source":["#fit the data onto the models\n","def train(clf, features, targets):    \n","    clf.fit(features, targets)\n","\n","def predict(clf, features):\n","    return (clf.predict(features))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1GKN_IUWT1fy"},"source":["pred_scores_word_vectors = []\n","for k,v in clfs.items():\n","    train(v, X_train, y_train)\n","    pred = predict(v, X_test)\n","    pred_scores_word_vectors.append((k, [accuracy_score(y_test , pred)]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ryfz7ed_T1f2"},"source":["## **Predictions using TFIDF Vectorizer algorithm**"]},{"cell_type":"code","metadata":{"id":"qIRrQXPOT1f2","scrolled":true},"source":["pred_scores_word_vectors"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gSExV8qIT1gC"},"source":["##**Model predictions**\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"yz_DheJ1T1gN"},"source":["#write functions to detect if the message is spam or not\n","def find(x):\n","    if x == 'spam':\n","        print (\"Message is SPAM\")\n","    else:\n","        print (\"Message is NOT Spam\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b5ytyDshT1gR"},"source":["newtext = [\"win FREE mobiles and play games\"]\n","integers = vectorizer.transform(newtext)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"imQD1ahzT1gT"},"source":["x = mnb.predict(integers)\n","find(x)        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQ62GGOlFT0o"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7cgsAyabS25d"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"63vmuFLpS25d"},"source":[""],"execution_count":null,"outputs":[]}]}