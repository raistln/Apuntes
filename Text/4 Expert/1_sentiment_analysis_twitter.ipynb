{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"1_sentiment_analysis_twitter.ipynb","provenance":[],"collapsed_sections":["9te2Fg0wTA_s"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"4bNXPuFNTA_X"},"source":["## In this practice session, we will understand sentiment analysis using logistic regression and Artificial Neural Nets"]},{"cell_type":"code","metadata":{"id":"Se2wsC2oTA_e"},"source":["import re\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import seaborn as sns\n","\n","import nltk\n","from nltk.stem.porter import *\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","import tensorflow as tf\n","from tensorflow.python import keras\n","from keras.models import Sequential\n","from keras import layers\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"whVhOiNcTA_h"},"source":["train_data = pd.read_csv('train_tweets.csv') \n","test_data = pd.read_csv('test_tweets.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BprT_qN1TA_j"},"source":["y_train = train_data['label']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ahPR4brxTA_k"},"source":["train_data_len = len(train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYhEjIDFTA_k"},"source":["train_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"apwVIDBjTA_l"},"source":["test_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PIwNAu3nTA_m"},"source":["# Removing Twitter Handles"]},{"cell_type":"code","metadata":{"id":"R0XjrzxxTA_m"},"source":["# concatenating the training data set and the test data set to get a the complete data set\n","\n","comp_data = train_data.append(test_data, ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"97TWtgFwTA_n"},"source":["# a function is defined to remove patterns in text. Here the pattern that we are removing are the twitter handles as they\n","# don't hold much significance\n","\n","def remove_pattern(input_txt, pattern):\n","    r = re.findall(pattern, input_txt)\n","    for i in r:\n","        input_txt = re.sub(i, '', input_txt)\n","    \n","    return input_txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K_Is68tXTA_n"},"source":["# the formatted data is then stored to a new feature called the cleaned_tweets where the formatted tweets for each example\n","# are stored to create a cleaned vocabulary\n","\n","comp_data['cleaned_tweets'] = np.vectorize(remove_pattern)(comp_data['tweet'], \"@[\\w]*\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NWbzjGNYTA_p"},"source":["comp_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UeKKldoCTA_p"},"source":["# replacing numbers and other special characters with a space in the cleaned_tweets elements\n","\n","comp_data['cleaned_tweets'] = comp_data['cleaned_tweets'].str.replace(\"[^a-zA-Z#]\", \" \")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_z2GFZ2TA_q"},"source":["comp_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FBd7nxhhTA_q"},"source":["## Removing words of length less than 3"]},{"cell_type":"code","metadata":{"id":"PMD-t-mITA_s"},"source":["# There are going to be many words like 'we', 'i' etc., which don't provide us with a lot of information. Hence we are \n","# going to remove such words from our vocabulary and have words which gives us the most valued output.\n","\n","comp_data['cleaned_tweets'] = comp_data['cleaned_tweets'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"797JG8hITA_s"},"source":["comp_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9te2Fg0wTA_s"},"source":["## Tokenizing tweets"]},{"cell_type":"code","metadata":{"id":"CJJNl0XaTA_s"},"source":["# The tweets in the cleaned_tweets feature are then tokenized. Tokenization means to split all the words in a given \n","# sentence into individual words. These individual words are stored as a list in their respective row indexes\n","\n","tokenized_tweet = comp_data['cleaned_tweets'].apply(lambda x: x.split())\n","tokenized_tweet.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YNHAodLITA_s"},"source":["# Stemmer is a process of removing morphological affixes from the words. \n","\n","stemmer = PorterStemmer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uvMJDK1QTA_u"},"source":["# all the words in the tokenized tweet are stemmed to give their root form. This process is implemented because\n","# there might be different forms of the same word in the tweets. For example, let's take the word 'drag'. The different\n","# forms of drag are drags, dragging, dragged etc., These different forms might be present in different tweets. \n","# \n","# Considering the different forms of a word is going to increase the length of our corpus, there by increasing the length\n","# of our vocabulary. These different forms of the word would mean almost the same in the sentences. Hence when stemming\n","# is implemented, we get the core form of the word and helps in shrinking the total number of elements in our corpus.\n","\n","# The alternative for stemmer is lemmatizer\n","\n","tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n","tokenized_tweet.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ahU-nYClTA_u"},"source":["# joining all the words of the tweets in the tokenized_tweet and assigning it back to the cleaned_tweets feature for further\n","# implementation\n","\n","tokenized_tweet = [' '.join(i) for i in tokenized_tweet]\n","\n","comp_data['cleaned_tweets'] = tokenized_tweet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XnxMSSMPTA_u"},"source":["comp_data['cleaned_tweets'].head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hEDDccBcTA_u"},"source":["# wordcloud gives you a visual form of the most occuring words in the corpus\n","\n","all_words = ' '.join([text for text in comp_data['cleaned_tweets']])\n","from wordcloud import WordCloud\n","wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n","\n","plt.figure(figsize=(10, 7))\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a6ZVRbKHTA_v"},"source":["# plotting the wordcloud for the words that appear the most in the positive tweets\n","\n","positive_words =' '.join([text for text in comp_data['cleaned_tweets'][comp_data['label'] == 0]])\n","\n","wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(positive_words)\n","plt.figure(figsize=(10, 7))\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CXwx7FxHTA_w"},"source":["# plotting the wordcloud for the words that appear the most in the negative tweets\n","\n","negative_words = ' '.join([text for text in comp_data['cleaned_tweets'][comp_data['label'] == 1]])\n","wordcloud = WordCloud(width=800, height=500,\n","random_state=21, max_font_size=110).generate(negative_words)\n","plt.figure(figsize=(10, 7))\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KRcxHh2WTA_w"},"source":["# defining a function to extract all the hashtags from the tweets\n","\n","def hashtag_extract(x):\n","    hashtags = []\n","    # Loop over the words in the tweet\n","    for i in x:\n","        ht = re.findall(r\"#(\\w+)\", i)\n","        hashtags.append(ht)\n","\n","    return hashtags"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OqIq5rYUTA_w"},"source":["# extracting hashtags from positive tweets\n","HT_positive = hashtag_extract(comp_data['cleaned_tweets'][comp_data['label'] == 0])\n","\n","# extracting hashtags from negative tweets\n","HT_negative = hashtag_extract(comp_data['cleaned_tweets'][comp_data['label'] == 1])\n","\n","HT_positive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1VEdTzmCTA_x"},"source":["# unnesting list\n","HT_positive = sum(HT_positive,[])\n","HT_negative = sum(HT_negative,[])\n","\n","HT_positive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7tjDw1C_TA_x"},"source":["# FreqDist is the sortform of Frequency Distribution. Here we are getting the frequency distribution of the number of \n","# occurances of each hashtag in a positive tweet and then storing it as a dataframe to plot a bar graph\n","\n","ht_count_pos = nltk.FreqDist(HT_positive)\n","ht_df_pos = pd.DataFrame({'Hashtag': list(ht_count_pos.keys()),\n","                  'Count': list(ht_count_pos.values())})\n","# selecting top 10 most frequent hashtags     \n","ht_df_pos = ht_df_pos.nlargest(columns=\"Count\", n = 10) \n","plt.figure(figsize=(16,5))\n","ax = sns.barplot(data=ht_df_pos, x= \"Hashtag\", y = \"Count\")\n","ax.set(ylabel = 'Count')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"enYR3fcGTA_x"},"source":["ht_count_neg = nltk.FreqDist(HT_negative)\n","ht_df_neg = pd.DataFrame({'Hashtag': list(ht_count_neg.keys()), 'Count': list(ht_count_neg.values())})\n","# selecting top 10 most frequent hashtags\n","ht_df_neg = ht_df_neg.nlargest(columns=\"Count\", n = 10)   \n","plt.figure(figsize=(16,5))\n","ax = sns.barplot(data=ht_df_neg, x= \"Hashtag\", y = \"Count\")\n","ax.set(ylabel = 'Count')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZaTHTjTBTA_x"},"source":["# Count Vectorizer creates a bag of words of total count of 1000 elements. The process that is followed by count vectorizer \n","# is, it creates a matrix where the number of occurances of each word are checked and then a dictionary is created. Then,\n","# it sorts this dictionary to get the descending order of the count. The max_feature attribute takes the value which gives\n","# the top 1000 words which have occured the most in our vocabulary.\n","\n","bow_vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n","# bag-of-words feature matrix\n","bow_fit = bow_vectorizer.fit(comp_data['cleaned_tweets'])\n","bow =  bow_vectorizer.transform(comp_data['cleaned_tweets']).toarray()\n","bow"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OPBX159jTA_x"},"source":["train_bow = bow[:train_data_len,:]\n","test_bow = bow[train_data_len:,:]\n","\n","# splitting data into training and validation set\n","xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, y_train, test_size=0.3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6-yipLrcTA_y"},"source":["# Logistic Regression Implementation"]},{"cell_type":"code","metadata":{"id":"gSxMRDy6TA_y"},"source":["lreg = LogisticRegression()\n","lreg.fit(xtrain_bow, ytrain) # training the model\n","\n","prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\n","prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n","prediction_int = prediction_int.astype(np.int)\n","\n","f1_score(yvalid, prediction_int) # calculating f1 score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R-J6Bp5QTA_y"},"source":["# ANN Implementation"]},{"cell_type":"code","metadata":{"id":"nV9L7ExhTA_z"},"source":["input_dim = xtrain_bow.shape[1]\n","\n","model = Sequential()\n","model.add(layers.Dense(10, input_dim=input_dim, activation='sigmoid'))\n","model.add(layers.Dense(1, activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QVnyUco4TA_z"},"source":["model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"Uo2X_29GTA_z"},"source":["history = model.fit(xtrain_bow, ytrain, nb_epoch=5, validation_data=(xvalid_bow, yvalid), batch_size=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bu_0GknjTA_0"},"source":["loss, accuracy = model.evaluate(xtrain_bow, ytrain)\n","print(\"Training Accuracy: {:.4f}\".format(accuracy))\n","loss, accuracy = model.evaluate(xvalid_bow, yvalid)\n","print(\"Validation Accuracy: {:.4f}\".format(accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FEJ6PidGTA_0"},"source":["def plot_history(history):\n","    acc = history.history['acc']\n","    val_acc = history.history['val_acc']\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","    x = range(1, len(acc) + 1)\n","\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(x, acc, 'b', label='Training acc')\n","    plt.plot(x, val_acc, 'r', label='Validation acc')\n","    plt.title('Training and validation accuracy')\n","    plt.legend()\n","    plt.subplot(1, 2, 2)\n","    plt.plot(x, loss, 'b', label='Training loss')\n","    plt.plot(x, val_loss, 'r', label='Validation loss')\n","    plt.title('Training and validation loss')\n","    plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hXuz6hFJTA_0"},"source":["plot_history(history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gn2CopSETA_1"},"source":["result = model.predict(test_bow)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bnScZ6EaTA_1"},"source":["final_result = []\n","\n","for i in result:\n","    if i > 0.03:\n","        final_result.append(0)\n","    else:\n","        final_result.append(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xD2l50CjTA_1"},"source":["test_data['label'] = final_result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"93GudzluTA_1"},"source":["test_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aGPz0ZoWTA_1"},"source":[""],"execution_count":null,"outputs":[]}]}