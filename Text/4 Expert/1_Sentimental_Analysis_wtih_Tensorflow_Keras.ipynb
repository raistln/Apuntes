{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"1_Sentimental_Analysis_wtih_Tensorflow_Keras.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyP3gEoEmeUkPqFDNRNpVzV8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Get Started With Sentiment Analysis Using TensorFlow Keras\n","\n","Sentiment Analysis is among the text classification applications in which a given text is classified into a positive class or a negative class (sometimes, a neutral class, too) based on the context. This session discusses sentiment analysis using TensorFlow Keras with the IMDB movie reviews dataset, one of the famous Sentiment Analysis datasets.TensorFlow’s Keras API offers the complete functionality required to build and execute a deep learning model. This session assumes that the reader is familiar with the basics of deep learning and Recurrent Neural Networks (RNNs)\n","\n","References:\n","\n","https://www.tensorflow.org/datasets/catalog/imdb_reviews\n","\n","https://www.tensorflow.org/text/tutorials/text_classification_rnn\n"],"metadata":{"id":"w57lJp1sRFUX"}},{"cell_type":"markdown","source":["To read about it more please refer [this](https://analyticsindiamag.com/getting-started-sentiment-analysis-tensorflow-keras/) article."],"metadata":{"id":"oG-r2vTCRbAJ"}},{"cell_type":"markdown","source":["# Code Implementation"],"metadata":{"id":"dhsgX_isRhPX"}},{"cell_type":"markdown","source":["## Create the Environment\n","\n","Create the necessary Python environment by importing the frameworks and libraries."],"metadata":{"id":"4GuEcp4dRmXQ"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn tensorflow nltk gensim --user -q --no-warn-script-location\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from tensorflow import keras\n","from tensorflow.keras.layers import Dense, Dropout, Bidirectional, LSTM\n","import matplotlib.pyplot as plt"],"outputs":[],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-26T00:39:59.956795Z","iopub.execute_input":"2021-05-26T00:39:59.957348Z","iopub.status.idle":"2021-05-26T00:40:06.792029Z","shell.execute_reply.started":"2021-05-26T00:39:59.957236Z","shell.execute_reply":"2021-05-26T00:40:06.790876Z"},"trusted":true,"id":"FendORqRRFUb"}},{"cell_type":"markdown","source":["## Download the IMDB dataset \n","\n","IMDB reviews dataset is available with TensorFlow Datasets in different variants: \n","\n","1. Plain text reviews, \n","2. Byte-encoded texts, \n","3. Integer-encoded texts with around 8k vocabulary\n","4. Integer-encoded texts with around 32k vocabulary\n","\n","Here, we use the dataset that has integer-encoded texts with around 8k vocabulary words."],"metadata":{"id":"Jw3plcKlRFUd"}},{"cell_type":"code","execution_count":null,"source":["data, meta = tfds.load('imdb_reviews/subwords8k',\n","                      with_info = True,\n","                      as_supervised = True)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-26T00:40:06.793718Z","iopub.execute_input":"2021-05-26T00:40:06.794053Z","iopub.status.idle":"2021-05-26T00:43:13.009835Z","shell.execute_reply.started":"2021-05-26T00:40:06.794018Z","shell.execute_reply":"2021-05-26T00:43:13.008526Z"},"trusted":true,"id":"7YCtDRzqRFUf"}},{"cell_type":"code","execution_count":null,"source":["data.keys()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-26T00:43:13.011936Z","iopub.execute_input":"2021-05-26T00:43:13.012262Z","iopub.status.idle":"2021-05-26T00:43:13.020526Z","shell.execute_reply.started":"2021-05-26T00:43:13.012228Z","shell.execute_reply":"2021-05-26T00:43:13.019454Z"},"trusted":true,"id":"5PXoj_Q0RFUh"}},{"cell_type":"markdown","source":["We do not require unsupervised data. Hence, we can obtain two datasets for train and test sets."],"metadata":{"id":"47TRjsfqR0MS"}},{"cell_type":"code","execution_count":null,"source":["train = data['train']\n","test = data['test']\n","train, test"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-26T00:43:13.022848Z","iopub.execute_input":"2021-05-26T00:43:13.023247Z","iopub.status.idle":"2021-05-26T00:43:13.039196Z","shell.execute_reply.started":"2021-05-26T00:43:13.023211Z","shell.execute_reply":"2021-05-26T00:43:13.038243Z"},"trusted":true,"id":"w_l1BuGpRFUi"}},{"cell_type":"markdown","source":["## Prepare an Encoder"],"metadata":{"id":"aj1AnuXQR2xq"}},{"cell_type":"markdown","source":["We have discussed that the dataset comes with texts being encoded into integers. Encoding into integers is mandatory since machines can read only numbers. However, humans can not read those integer texts. Hence, we need a decoder that can reverse the encoding action, by which we can convert the numbers into text and read in English. We need an encoder that can convert an example text (from outside of the dataset) into integers. \n","\n","Metadata that comes with the dataset contains the encoder originally used while preparing the dataset. It can perform encoding and decoding operations."],"metadata":{"id":"_xXBED0lR5JK"}},{"cell_type":"code","execution_count":null,"source":["# explore the features in metadata\n","meta.features"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-26T00:43:13.040832Z","iopub.execute_input":"2021-05-26T00:43:13.041318Z","iopub.status.idle":"2021-05-26T00:43:13.055356Z","shell.execute_reply.started":"2021-05-26T00:43:13.041266Z","shell.execute_reply":"2021-05-26T00:43:13.054548Z"},"trusted":true,"id":"O2DlZOXeRFUj"}},{"cell_type":"markdown","source":["It can be observed that metadata contains the encoder under the key ‘text’."],"metadata":{"id":"EY9Lc-mjSMJs"}},{"cell_type":"code","execution_count":null,"source":["# extract the encoder\n","encoder = meta.features['text'].encoder"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-26T00:43:13.056661Z","iopub.execute_input":"2021-05-26T00:43:13.057142Z","iopub.status.idle":"2021-05-26T00:43:13.069121Z","shell.execute_reply.started":"2021-05-26T00:43:13.057078Z","shell.execute_reply":"2021-05-26T00:43:13.067744Z"},"trusted":true,"id":"GE0HvF5MRFUj"}},{"cell_type":"markdown","source":["The encoded integers will be numbered from 1 to vocabulary size. How many vocabulary words are there in the encoder?"],"metadata":{"id":"Lacxvn7xSP9p"}},{"cell_type":"code","execution_count":null,"source":["encoder.vocab_size"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-26T00:43:13.070530Z","iopub.execute_input":"2021-05-26T00:43:13.070957Z","iopub.status.idle":"2021-05-26T00:43:13.084264Z","shell.execute_reply.started":"2021-05-26T00:43:13.070922Z","shell.execute_reply":"2021-05-26T00:43:13.083412Z"},"trusted":true,"id":"qMCT_wo0RFUk"}},{"cell_type":"markdown","source":["What are the original text words?"],"metadata":{"id":"zWL6qs1DSSu9"}},{"cell_type":"code","execution_count":null,"source":["print(encoder.subwords[:100])"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-26T00:43:13.086970Z","iopub.execute_input":"2021-05-26T00:43:13.087558Z","iopub.status.idle":"2021-05-26T00:43:13.099990Z","shell.execute_reply.started":"2021-05-26T00:43:13.087512Z","shell.execute_reply":"2021-05-26T00:43:13.098818Z"},"trusted":true,"id":"f5ywkaaXRFUm"}},{"cell_type":"markdown","source":["Test the encoder by sampling a sentence, encoding it into integers, and decoding back into text."],"metadata":{"id":"vdgscYpUSU8z"}},{"cell_type":"code","execution_count":null,"source":["example = 'Analytics India Magazine !'\n","enc = encoder.encode(example)\n","enc"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-26T00:43:13.101751Z","iopub.execute_input":"2021-05-26T00:43:13.102323Z","iopub.status.idle":"2021-05-26T00:43:13.115257Z","shell.execute_reply.started":"2021-05-26T00:43:13.102288Z","shell.execute_reply":"2021-05-26T00:43:13.114399Z"},"trusted":true,"id":"YFZilLg_RFUn"}},{"cell_type":"markdown","source":["We have provided a sentence with three words and one exclamation mark, but it is encoded into an eleven-element integer list. The split words are technically called tokens. Let’s explore the numbers and corresponding tokens by using the decode method."],"metadata":{"id":"EGewOg1GSYKa"}},{"cell_type":"code","execution_count":null,"source":["for integer in enc:\n","    text = encoder.decode([integer])\n","    print('%4d : %s'%(integer, text))"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-26T00:43:13.116660Z","iopub.execute_input":"2021-05-26T00:43:13.117246Z","iopub.status.idle":"2021-05-26T00:43:13.131676Z","shell.execute_reply.started":"2021-05-26T00:43:13.117212Z","shell.execute_reply":"2021-05-26T00:43:13.130363Z"},"trusted":true,"id":"6IYLvsXvRFUo"}},{"cell_type":"code","execution_count":null,"source":["it = iter(train)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-26T00:43:13.133447Z","iopub.execute_input":"2021-05-26T00:43:13.134133Z","iopub.status.idle":"2021-05-26T00:43:13.246380Z","shell.execute_reply.started":"2021-05-26T00:43:13.134090Z","shell.execute_reply":"2021-05-26T00:43:13.245500Z"},"trusted":true,"id":"CaLv4f0hRFUp"}},{"cell_type":"code","execution_count":null,"source":["next(it)[0].numpy().shape, next(it)[1].numpy().shape"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-26T00:43:13.247752Z","iopub.execute_input":"2021-05-26T00:43:13.248263Z","iopub.status.idle":"2021-05-26T00:43:13.280044Z","shell.execute_reply.started":"2021-05-26T00:43:13.248215Z","shell.execute_reply":"2021-05-26T00:43:13.278949Z"},"trusted":true,"id":"LailfewVRFUq"}},{"cell_type":"markdown","source":["## Preprocess the Dataset\n","\n","The input texts are of variable lengths. But a deep learning model can not accept inputs of different sizes. We have to fix the length of each input token. If there are fewer tokens than fixed length, the vector will be made up by padding with zeros. It is accomplished by using the padded_batch method. It pads the sequences in a batch to have an equal number of sequence lengths. Since the large vocabulary size will make the manipulations complicated; it should be embedded into a small-sized vector representation. We perform this process with an Embedding layer"],"metadata":{"id":"YrE7BPBxSa5j"}},{"cell_type":"code","execution_count":null,"source":["BUFFER_SIZE = 10000\n","BATCH_SIZE = 64\n","AUTOTUNE = tf.data.AUTOTUNE\n","\n","train_data = train.shuffle(BUFFER_SIZE)\n","train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([None],[]))\n","train_data = train_data.prefetch(AUTOTUNE)\n","\n","test_data = test.padded_batch(BATCH_SIZE, padded_shapes=([None],[]))"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-26T00:43:13.281483Z","iopub.execute_input":"2021-05-26T00:43:13.281796Z","iopub.status.idle":"2021-05-26T00:43:13.294522Z","shell.execute_reply.started":"2021-05-26T00:43:13.281764Z","shell.execute_reply":"2021-05-26T00:43:13.293448Z"},"trusted":true,"id":"Lce_nt-URFUq"}},{"cell_type":"code","execution_count":null,"source":["embed_layer = keras.layers.Embedding(encoder.vocab_size, 64)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-26T00:43:13.296343Z","iopub.execute_input":"2021-05-26T00:43:13.296791Z","iopub.status.idle":"2021-05-26T00:43:13.309714Z","shell.execute_reply.started":"2021-05-26T00:43:13.296745Z","shell.execute_reply":"2021-05-26T00:43:13.308528Z"},"trusted":true,"id":"TXjpSkbuRFUq"}},{"cell_type":"markdown","source":["## Build Model\n","\n","Unlike images and structured data, texts have a sequential order of tokens that contribute to the context. Hence, the deep learning model should be able to remember past tokens in order when processing a specific token. This is achieved by implementing either Recurrent Neural Networks or Transformers. Here, we prefer Recurrent Neural Networks with LSTM units to model our problem. LSTM (Long-Short Term Memory) units capture the temporal relationship of the past portion of the embedded sequence in memory and models the sequential relationships among texts. LSTM units can be modeled with bi-directional layers so that the model can understand the context of a sentence in both directions, namely, left-to-right and right-to-left. "],"metadata":{"id":"CFs8450XRFUr"}},{"cell_type":"code","execution_count":null,"source":["model = keras.Sequential([\n","    # embedding layer\n","    embed_layer,\n","    # bidirectional LSTM layers\n","    Bidirectional(LSTM(64, \n","                       dropout=0.5, \n","                       recurrent_dropout=0.5, \n","                       return_sequences=True)),\n","    Bidirectional(LSTM(32, \n","                       dropout=0.5, \n","                       recurrent_dropout=0.5, \n","                       return_sequences=True)),\n","    Bidirectional(LSTM(16, \n","                       dropout=0.5, \n","                       recurrent_dropout=0.5)),\n","    # Classification head\n","    Dense(64, activation='relu', kernel_regularizer='l2'),\n","    Dropout(0.5),\n","    Dense(1, activation='sigmoid')    \n","])"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-26T00:43:13.311212Z","iopub.execute_input":"2021-05-26T00:43:13.312012Z","iopub.status.idle":"2021-05-26T00:43:14.643839Z","shell.execute_reply.started":"2021-05-26T00:43:13.311977Z","shell.execute_reply":"2021-05-26T00:43:14.642775Z"},"trusted":true,"id":"FXLb_g1ZRFUr"}},{"cell_type":"markdown","source":["We have used dropout layers and kernel regularizer to contain the overfitting of the model. In the LSTM layer, dropout is executed in two stages, one for the input data and another for the recurrent temporal data.\n","\n","How many parameters does the model have?"],"metadata":{"id":"P60Ye59GSmef"}},{"cell_type":"code","execution_count":null,"source":["model.summary()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-25T15:44:48.751754Z","iopub.execute_input":"2021-05-25T15:44:48.752013Z","iopub.status.idle":"2021-05-25T15:44:48.764825Z","shell.execute_reply.started":"2021-05-25T15:44:48.751987Z","shell.execute_reply":"2021-05-25T15:44:48.763875Z"},"trusted":true,"id":"pOVxRhboRFUs"}},{"cell_type":"markdown","source":["Plotting the model gives a better understanding of data flow through layers."],"metadata":{"id":"wvY3XjQgSrPY"}},{"cell_type":"code","execution_count":null,"source":["keras.utils.plot_model(model, show_shapes=True, dpi=48)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-25T22:55:30.000172Z","iopub.execute_input":"2021-05-25T22:55:30.000469Z","iopub.status.idle":"2021-05-25T22:55:30.172989Z","shell.execute_reply.started":"2021-05-25T22:55:30.000439Z","shell.execute_reply":"2021-05-25T22:55:30.172060Z"},"trusted":true,"id":"_GZX-7ANRFUs"}},{"cell_type":"markdown","source":["## Train the Model\n","\n","Compile the built model with Adam optimizer, Accuracy metric and Binary Cross-entropy loss function."],"metadata":{"id":"wTh-1F3HRFUs"}},{"cell_type":"code","execution_count":null,"source":["model.compile(loss='binary_crossentropy',\n","             optimizer='adam',\n","             metrics=['accuracy'])"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-25T15:44:49.640494Z","iopub.execute_input":"2021-05-25T15:44:49.640767Z","iopub.status.idle":"2021-05-25T15:44:49.662547Z","shell.execute_reply.started":"2021-05-25T15:44:49.640736Z","shell.execute_reply":"2021-05-25T15:44:49.661715Z"},"trusted":true,"id":"5AXp94fsRFUt"}},{"cell_type":"markdown","source":["Train the model for 2 epochs. It should be noted that model training may take more time than multi-layer perceptrons (MLPs) and CNNs, because of handling temporal relationships in LSTM layers."],"metadata":{"id":"1s3x0c73SwLP"}},{"cell_type":"code","execution_count":null,"source":["history = model.fit(train_data, \n","                    validation_data=test_data, \n","                    epochs=2)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-25T15:44:49.665267Z","iopub.execute_input":"2021-05-25T15:44:49.665592Z","iopub.status.idle":"2021-05-25T22:55:29.996352Z","shell.execute_reply.started":"2021-05-25T15:44:49.665564Z","shell.execute_reply":"2021-05-25T22:55:29.995350Z"},"trusted":true,"id":"muot3vhTRFUt"}},{"cell_type":"markdown","source":["Model Performance Evaluation\n","\n","The model has been trained and is ready to make inferences. Plot the training losses to have a better understanding of its performance."],"metadata":{"id":"IXRs43ntS2ea"}},{"cell_type":"code","execution_count":null,"source":["hist = history.history\n","\n","plt.plot(hist['loss'])\n","plt.plot(hist['val_loss'])\n","plt.legend(labels=['Training', 'Validation'])\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.show()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-25T22:55:30.175010Z","iopub.execute_input":"2021-05-25T22:55:30.175420Z","iopub.status.idle":"2021-05-25T22:55:30.355031Z","shell.execute_reply.started":"2021-05-25T22:55:30.175345Z","shell.execute_reply":"2021-05-25T22:55:30.354032Z"},"trusted":true,"id":"j-__sdRVRFUt"}},{"cell_type":"code","execution_count":null,"source":["# Sample prediction\n","\n","samples = ['The plot is fantastic', \n","           'The movie was cool and thrilling', \n","           'one of the worst films I have ever seen']\n","\n","# encode into integers\n","sample_encoded = [encoder.encode(sample) for sample in samples]\n","\n","# pad with zeros to have same length \n","sample_padded = []\n","for s in sample_encoded:\n","    pad_length = 128 - len(s)\n","    zeros = [0]*pad_length\n","    s.extend(zeros)\n","    s = tf.convert_to_tensor(s)\n","    sample_padded.append(s)\n","    \n","# convert into tensor before feeding the model\n","sample_padded = tf.convert_to_tensor(sample_padded)\n","#make predictions\n","predictions = model.predict(sample_padded)\n","predictions"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-26T00:57:18.330189Z","iopub.execute_input":"2021-05-26T00:57:18.330595Z","iopub.status.idle":"2021-05-26T00:57:18.337429Z","shell.execute_reply.started":"2021-05-26T00:57:18.330562Z","shell.execute_reply":"2021-05-26T00:57:18.336191Z"},"trusted":true,"id":"FLX0R2KDRFUu"}},{"cell_type":"code","execution_count":null,"source":["print('Predictions on sample test reviews... \\n')\n","for i in range(len(samples)):\n","    pred = predictions[i][0]\n","    sentiment = 'positive' if pred>0.5 else 'negative'\n","    print('%40s : %s'%(samples[i], sentiment))"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-05-26T01:11:08.274262Z","iopub.execute_input":"2021-05-26T01:11:08.274698Z","iopub.status.idle":"2021-05-26T01:11:08.281455Z","shell.execute_reply.started":"2021-05-26T01:11:08.274662Z","shell.execute_reply":"2021-05-26T01:11:08.280519Z"},"trusted":true,"id":"i9IBM6lJRFUv"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"lDFlZ79aGvsS"}}]}