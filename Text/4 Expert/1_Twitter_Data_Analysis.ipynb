{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"Twitter_Data_Analysis.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyP88ho5YQ3Z0VrbqgPfgYaL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Twitter Data Analysis**"],"metadata":{"id":"PS8JtBA9WqRI"}},{"cell_type":"markdown","source":["In this session, we will discuss a hands-on approach to download and analyze twitter data. We will import all the required libraries here. Make sure to install ‘tweepy’, ‘textblob‘ and ‘wordcloud‘ libraries using ‘pip install tweepy’, ‘pip install textblob‘ and ‘pip install wordcloud‘."],"metadata":{"id":"Ug7_Dsa0XOzo"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim tweepy textblob wordcloud --user -q --no-warn-script-location\n","\n","import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["#Importing Libraries\n","import tweepy\n","from textblob import TextBlob\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from wordcloud import WordCloud\n","import json\n","from collections import Counter"],"outputs":[],"metadata":{"id":"_XYDeZciXTIG"}},{"cell_type":"markdown","source":["## **Downloading the data from Twitter**"],"metadata":{"id":"b48VCiMFXccP"}},{"cell_type":"markdown","source":["**Note**: Please don't run the cell, will give error without consumer and access key tokens"],"metadata":{"id":"DEPehWJYmgXP"}},{"cell_type":"code","execution_count":null,"source":["#Authorization and Search tweets\n","#Replace X with your consumer keys and access keys\n","#Getting authorization\n","consumer_key = 'XXXXXXXXXXXXX'\n","consumer_key_secret = 'XXXXXXXXXXX'\n","access_token = 'XXXXXXXXXXXX'\n","access_token_secret = 'XXXXXXXXXXXX'\n","auth = tweepy.OAuthHandler(consumer_key, consumer_key_secret)\n","auth.set_access_token(access_token, access_token_secret)\n","api = tweepy.API(auth, wait_on_rate_limit=True)"],"outputs":[],"metadata":{"id":"XkttMHKnT9DZ"}},{"cell_type":"markdown","source":["You can pass the keyword of your interest here and the maximum number of tweets to be downloaded through the tweepy API."],"metadata":{"id":"QC26BGrTj1TC"}},{"cell_type":"code","execution_count":null,"source":["#Defining Search keyword and number of tweets and searching tweets\n","query = 'lockdown'\n","max_tweets = 2000\n","searched_tweets = [status for status in tweepy.Cursor(api.search, q=query).items(max_tweets)]"],"outputs":[],"metadata":{"id":"O9t_S1q6j4sb"}},{"cell_type":"markdown","source":["## **Creating the Data Frame and Saving into CSV File**"],"metadata":{"id":"MZbB_ssQmIas"}},{"cell_type":"code","execution_count":null,"source":["#Creating Dataframe of Tweets\n","#Cleaning searched tweets and converting into Dataframe\n","my_list_of_dicts = []\n","for each_json_tweet in searched_tweets:\n","    my_list_of_dicts.append(each_json_tweet._json)\n","\n","    \n","with open('tweet_json_Data.txt', 'w') as file:\n","        file.write(json.dumps(my_list_of_dicts, indent=4))\n","\n","        \n","my_demo_list = []\n","with open('tweet_json_Data.txt', encoding='utf-8') as json_file:  \n","    all_data = json.load(json_file)\n","    for each_dictionary in all_data:\n","        tweet_id = each_dictionary['id']\n","        text = each_dictionary['text']\n","        favorite_count = each_dictionary['favorite_count']\n","        retweet_count = each_dictionary['retweet_count']\n","        created_at = each_dictionary['created_at']\n","        my_demo_list.append({'tweet_id': str(tweet_id),\n","                             'text': str(text),\n","                             'favorite_count': int(favorite_count),\n","                             'retweet_count': int(retweet_count),\n","                             'created_at': created_at,\n","                            })\n","        \n","        tweet_dataset = pd.DataFrame(my_demo_list, columns = \n","                                  ['tweet_id', 'text', \n","                                   'favorite_count', 'retweet_count', \n","                                   'created_at'])\n","\n"," #Writing tweet dataset ti csv file for future reference\n","tweet_dataset.to_csv('tweet_data.csv')"],"outputs":[],"metadata":{"id":"qswqS3LSmRWk"}},{"cell_type":"markdown","source":["## **Read the data**"],"metadata":{"id":"OWk6UP_wmc52"}},{"cell_type":"code","execution_count":null,"source":["tweet_dataset"],"outputs":[],"metadata":{"id":"26FEEqKTqJRw"}},{"cell_type":"code","execution_count":null,"source":["#due to unavailability of consumer keys and access keys\n","import pandas as pd\n","tweet_dataset = pd.read_csv('tweet_data.csv')\n","searched_tweets = tweet_dataset[\"text\"].values"],"outputs":[],"metadata":{"id":"aOWEVqS6loHS"}},{"cell_type":"markdown","source":["## **Sentimental Analysis**"],"metadata":{"id":"X81x-FU4mOUK"}},{"cell_type":"markdown","source":["We will now analyze the sentiments of tweets that we have downloaded and then visualize them here."],"metadata":{"id":"QA3IFm3Ckhgl"}},{"cell_type":"code","execution_count":null,"source":["#Sentiment Analysis Report\n","#Finding sentiment analysis (+ve, -ve and neutral)\n","pos = 0\n","neg = 0\n","neu = 0\n","for tweet in searched_tweets:\n","    analysis = TextBlob(tweet)\n","    if analysis.sentiment[0]>0:\n","       pos = pos +1\n","    elif analysis.sentiment[0]<0:\n","       neg = neg + 1\n","    else:\n","       neu = neu + 1\n","print(\"Total Positive = \", pos)\n","print(\"Total Negative = \", neg)\n","print(\"Total Neutral = \", neu)\n","\n","#Plotting sentiments\n","labels = 'Positive', 'Negative', 'Neutral'\n","sizes = [257, 223, 520]\n","colors = ['gold', 'yellowgreen', 'lightcoral']\n","explode = (0.1, 0, 0)  # explode 1st slice\n","plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n","plt.axis('equal')\n","plt.show()"],"outputs":[],"metadata":{"id":"r47hJMj4k1Hx"}},{"cell_type":"markdown","source":["Here, we will create a data frame of all the tweet data that we have downloaded. Later all the processed data will be saved to a CSV file in the local system. Through this way, we can utilize this tweet data for other experimental purposes."],"metadata":{"id":"aBrZ-K7AmC-a"}},{"cell_type":"markdown","source":["## **Cleaning Tweet Texts using NLP Operations**"],"metadata":{"id":"d1oeqVIfm98i"}},{"cell_type":"code","execution_count":null,"source":["tweet_dataset.shape"],"outputs":[],"metadata":{"id":"Y2TSIF8emCar"}},{"cell_type":"code","execution_count":null,"source":["tweet_dataset.head()"],"outputs":[],"metadata":{"id":"mLDDpw-xkn1h"}},{"cell_type":"code","execution_count":null,"source":["#Cleaning Data\n","#Removing @ handle\n","def remove_pattern(input_txt, pattern):\n","    r = re.findall(pattern, input_txt)\n","    for i in r:\n","        input_txt = re.sub(i, '', input_txt)\n","     \n","    return input_txt \n","\n","tweet_dataset['text'] = np.vectorize(remove_pattern)(tweet_dataset['text'], \"@[\\w]*\")\n","\n","tweet_dataset.head()"],"outputs":[],"metadata":{"id":"_byXGkgGqqem"}},{"cell_type":"code","execution_count":null,"source":["tweet_dataset['text'].head()"],"outputs":[],"metadata":{"id":"vJZ--hpSqs0p"}},{"cell_type":"markdown","source":["Here, as we are ready with the clean tweet data, we will perform NLP operations on the tweet texts including taking only alphabets, converting all to lower cases, tokenization and stemming. As retweets, hypertexts etc. are present in the tweets, we need to remove all those unnecessary information."],"metadata":{"id":"hQrpiecHquai"}},{"cell_type":"code","execution_count":null,"source":["#Cleaning Tweets\n","corpus = []\n","for i in range(0, 1000):\n","    tweet = re.sub('[^a-zA-Z0-9]', ' ', tweet_dataset['text'][i])\n","    tweet = tweet.lower()\n","    tweet = re.sub('rt', '', tweet)\n","    tweet = re.sub('http', '', tweet)\n","    tweet = re.sub('https', '', tweet)\n","    tweet = tweet.split()\n","    ps = PorterStemmer()\n","    tweet = [ps.stem(word) for word in tweet if not word in set(stopwords.words('english'))]\n","    tweet = ' '.join(tweet)\n","    corpus.append(tweet)"],"outputs":[],"metadata":{"id":"nLwFExabqwow"}},{"cell_type":"markdown","source":["Now, after performing the NLP operations, we visualize the most frequent words in the tweets through a word cloud and using the term frequency."],"metadata":{"id":"oX-rZELXqzLt"}},{"cell_type":"markdown","source":["## **Visualizing Highest Occurring Words using Word Cloud**"],"metadata":{"id":"j6L8Iuljq0pH"}},{"cell_type":"code","execution_count":null,"source":["#Visualization\n","#Word Cloud\n","all_words = ' '.join([text for text in corpus])\n","wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n","plt.figure(figsize=(10, 7))\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis('off')\n","plt.show()"],"outputs":[],"metadata":{"id":"izA_IyI3qyoK"}},{"cell_type":"markdown","source":["## **Analyzing Highest Occurring Words Term Frequency**"],"metadata":{"id":"-OZplD9Hq41r"}},{"cell_type":"code","execution_count":null,"source":["#Term Freuency - TF-IDF\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n","tfidf = tfidf_vectorizer.fit_transform(tweet_dataset['text'])\n","#Count Most Frequent Words\n","Counter = Counter(corpus)\n","most_occur = Counter.most_common(10) \n","print(most_occur)"],"outputs":[],"metadata":{"id":"RO7eu4F2q7Ra"}},{"cell_type":"markdown","source":["# **Related Articles:**\n","\n","> * [Download Twitter Data and Analyze](https://analyticsindiamag.com/hands-on-guide-to-download-analyze-and-visualize-twitter-data/)\n","\n","> * [Sentiment Analysis using LSTM](https://analyticsindiamag.com/how-to-implement-lstm-rnn-network-for-sentiment-analysis/)\n","\n","> * [VADER Sentiment Analysis](https://analyticsindiamag.com/sentiment-analysis-made-easy-using-vader/)\n","\n","> * [Polyglot](https://analyticsindiamag.com/hands-on-tutorial-on-polyglot-python-toolkit-for-multilingual-nlp-applications/)\n","\n","> * [Textblob](https://analyticsindiamag.com/lets-learn-textblob-quickstart-a-python-library-for-processing-textual-data/)\n","\n","> * [TextHero Guide](https://analyticsindiamag.com/texthero-guide-a-python-toolkit-for-text-processing/)\n","\n"],"metadata":{"id":"prpNOPgzWgiU"}}]}