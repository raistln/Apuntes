{"cells":[{"cell_type":"markdown","source":["# PyKeen"],"metadata":{"id":"-xrppXS5vRRG"}},{"cell_type":"markdown","source":["Pykeen is a python package that generates knowledge graph embeddings while abstracting away the training loop and evaluation. The knowledge graph embeddings obtained using pykeen are reproducible, and they convey precise semantics in the knowledge graph."],"metadata":{"id":"8nNdK2pnvTEF"}},{"cell_type":"markdown","source":["To read about it more, please refer [this](https://analyticsindiamag.com/complete-guide-to-pykeen-python-knowledge-embeddings-for-knowledge-graphs/) article."],"metadata":{"id":"yilvU-vnvWPA"}},{"cell_type":"markdown","source":["# Code Implementation"],"metadata":{"id":"Jn3cgBvHvh67"}},{"cell_type":"markdown","source":["## PyKeen Installation"],"metadata":{"id":"x09Qicw9vaQC"}},{"cell_type":"markdown","source":["Installation of pykeen is quite simple. You can just do a pip install."],"metadata":{"id":"XbgNChMVvlbI"}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pip --upgrade --user -q --no-warn-script-location\n","!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim --user -q --no-warn-script-location\n","\n","\n"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["!python -m pip install pykeen==1.0.4 --user -q"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11792,"status":"ok","timestamp":1624013455007,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"},"user_tz":-330},"id":"aIqOXBhjQx41","outputId":"0e79319e-3b54-4d0e-a85f-3dc2173ebe63"}},{"cell_type":"code","execution_count":null,"source":["import IPython\n","IPython.Application.instance().kernel.do_shutdown(True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["from pykeen.pipeline import pipeline\n","pipeline_result = pipeline(\n","    dataset='Nations',\n","    model='TransE',\n",")\n","pipeline_result.save_to_directory('nations_transe')"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5654,"status":"ok","timestamp":1624013462258,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"},"user_tz":-330},"id":"8-rNZKYwXhhx","outputId":"0b3a4b6c-58fc-4ee0-850f-19bded2f23cd"}},{"cell_type":"markdown","source":["## Data\n","\n","Pykeen provides lots of Open Source datasets as classes for seamless integration with the rest of the module.Let’s check out the OpenBioLink Knowledge graph in this article."],"metadata":{"id":"Wat9hzNqvp6I"}},{"cell_type":"code","execution_count":null,"source":["from pykeen.datasets import OpenBioLink\n","dataset = OpenBioLink()\n","training_triples_factory = dataset.training"],"outputs":[],"metadata":{"executionInfo":{"elapsed":15284,"status":"ok","timestamp":1624013521860,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"},"user_tz":-330},"id":"qy8rumB1X6L4"}},{"cell_type":"code","execution_count":null,"source":["from pykeen.datasets import OpenBioLink\n","dataset = OpenBioLink()\n","dataset.training.triples"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14649,"status":"ok","timestamp":1624013536503,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"},"user_tz":-330},"id":"KfUBM6zjJIA4","outputId":"87a29e4a-ea03-4c90-d34a-6cae4eab979c"}},{"cell_type":"markdown","source":["## Model, Optimizer and Training Approach\n","\n","Next, we need to pick an embedding model to extract embeddings from the OpenBioLink Knowledge graph. Following is the code to load TransE model in pykeen:"],"metadata":{"id":"nyzY4fpAvyi9"}},{"cell_type":"code","execution_count":null,"source":["# Pick a model\n","from pykeen.models import TransE\n","model = TransE(triples_factory=training_triples_factory)"],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1624013536504,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"},"user_tz":-330},"id":"ifQ_gIryaNzj","outputId":"4965de35-5c65-406a-b32d-305325044f7b"}},{"cell_type":"markdown","source":["We can choose optimizers from torch to train the model."],"metadata":{"id":"-Vr83T28v2FZ"}},{"cell_type":"code","execution_count":null,"source":["# Pick an optimizer from Torch\n","from torch.optim import Adam\n","optimizer = Adam(params=model.get_grad_params())"],"outputs":[],"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1624013536505,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"},"user_tz":-330},"id":"rryUnBjFaOnS"}},{"cell_type":"code","execution_count":null,"source":["# Pick a training approach (sLCWA or LCWA)\n","from pykeen.training import SLCWATrainingLoop\n","training_loop = SLCWATrainingLoop(model=model, optimizer=optimizer)"],"outputs":[],"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1624013536506,"user":{"displayName":"Aishwarya Verma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64","userId":"06108390091304498033"},"user_tz":-330},"id":"q6v_rdw3Iy_A"}},{"cell_type":"markdown","source":["We need to select a training approach to use to train the model "],"metadata":{"id":"vhpxcoCGv8XQ"}},{"cell_type":"markdown","source":["## Training and Evaluation"],"metadata":{"id":"pD8NASQov-2O"}},{"cell_type":"markdown","source":["We are all set to train the model now. Following command trains the model."],"metadata":{"id":"OAVIBaoEwCon"}},{"cell_type":"code","execution_count":null,"source":["training_loop.train(num_epochs=5, batch_size=256)"],"outputs":[],"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"DYxtOPL_NjvC"}},{"cell_type":"markdown","source":["Following is the code  to evaluate the trained model using a test set."],"metadata":{"id":"mTcfNzh_wEu2"}},{"cell_type":"code","execution_count":null,"source":["# Pick an evaluator\n","from pykeen.evaluation import RankBasedEvaluator\n","evaluator = RankBasedEvaluator()\n","\n","# Get triples to test\n","mapped_triples = dataset.testing.mapped_triples\n","\n","# Evaluate\n","results = evaluator.evaluate(model, mapped_triples, batch_size=128)\n","print(results)"],"outputs":[],"metadata":{"id":"iptZBHbWR2cB"}},{"cell_type":"code","execution_count":null,"source":["results.to_df()"],"outputs":[],"metadata":{"id":"Qwwu2BvNm56O"}},{"cell_type":"code","execution_count":null,"source":["import pprint\n","pp = pprint.PrettyPrinter(indent=4)\n","pp.pprint(results)"],"outputs":[],"metadata":{"id":"gYmrO79nl_Mw"}},{"cell_type":"markdown","source":["## Pipeline\n","\n","PyKeen provides a high-level entry point to access the models. It is called a pipeline. We should provide all the information about the model to the pipeline, and the pipeline takes care of everything required for training."],"metadata":{"id":"NhsmcjD1wKlf"}},{"cell_type":"code","execution_count":null,"source":["from pykeen.pipeline import pipeline\n","pipeline_result = pipeline(\n","    dataset='Nations',\n","    model='TransE',\n","    evaluator='RankBasedEvaluator',\n","    training_loop='sLCWA',\n","    negative_sampler='basic',\n","    model_kwargs=dict(\n","        scoring_fct_norm=2,\n","    ),\n",")\n","pipeline_result.save_to_directory('nations_transe')"],"outputs":[],"metadata":{"id":"2QndhCJwV9-s"}},{"cell_type":"markdown","source":["## Hyper Parameter Optimization\n","\n","PyKeen provides a hyper parameter optimization pipeline function pykeen.hpo.hpo_pipeline().It uses optuna in the backend and does optimization.Following is a code snippet that shows how to optimize the hyperparameters."],"metadata":{"id":"NlAMAVG6wOoh"}},{"cell_type":"code","execution_count":null,"source":["from pykeen.hpo import hpo_pipeline\n","hpo_pipeline_result = hpo_pipeline(\n","   n_trials=30,\n","   dataset='Nations',\n","   model='TransE',\n","   loss='MarginRankingLoss',\n","   model_kwargs_ranges=dict(\n","        embedding_dim=dict(type=int, low=100, high=500, q=100),\n","    ),\n","   loss_kwargs_ranges=dict(\n","       margin=dict(type=float, low=1.0, high=2.0),\n","   ),\n",")"],"outputs":[],"metadata":{"id":"DfhMQ-ovZoS4"}},{"cell_type":"markdown","source":["## Saving and Restoring Model\n","\n","PyKeen Models are torch models with utility functions on the top. We can use the torch’s functionality to save and reload a model."],"metadata":{"id":"cNTOWwjDwTkR"}},{"cell_type":"code","execution_count":null,"source":["import torch\n","torch.save(model,'trained_model.pkl')\n","my_pykeen_model = torch.load('trained_model.pkl')"],"outputs":[],"metadata":{"id":"O3HLIK5TcTVF"}},{"cell_type":"markdown","source":["We can also save the model checkpoints during training to restore the training process if training fails due to a crash.This functionality can be added using the training_kwargs argument"],"metadata":{"id":"zkZXlfSWwWhv"}},{"cell_type":"code","execution_count":null,"source":["training_kwargs=dict(\n","        num_epochs=2000,\n","        checkpoint_name='my_checkpoint.pt',\n","        checkpoint_directory='doctests/checkpoint_dir',\n","        checkpoint_frequency=5,\n","    ),"],"outputs":[],"metadata":{"id":"Rxr_3iYXdwvY"}},{"cell_type":"code","execution_count":null,"source":["dir(model)"],"outputs":[],"metadata":{"id":"Chw75D5Co4y5"}},{"cell_type":"markdown","source":["## Results\n","\n","We have taken a knowledge graph and converted all the entities and relations into embeddings. Let’s see some of the interesting information we can extract from these embeddings.\n","\n","What are the possible phenotypes observed due to the presence of the gene NCBIGENE:534? "],"metadata":{"id":"s7xJzqvLwZSA"}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","np.array([['NCBIGENE:534', 'GENE_PHENOTYPE']])[:,0]"],"outputs":[],"metadata":{"id":"ROugCzjOqq39"}},{"cell_type":"code","execution_count":null,"source":["#Predict all tails\n","predicted_tails_df = model.predict_tails('NCBIGENE:534', 'GENE_PHENOTYPE')\n","predicted_tails_df#.head(10)"],"outputs":[],"metadata":{"id":"C8f6SuiVnfuY"}},{"cell_type":"code","execution_count":null,"source":["# Predict relations\n","predicted_relations_df = model.get_relation_prediction_df('brazil', 'uk')\n","# Predict heads\n","predicted_heads_df = model.get_head_prediction_df('conferences', 'brazil')\n","\n","# Score top K triples\n","top_k_predictions_df = model.get_all_prediction_df(k=150)"],"outputs":[],"metadata":{"id":"kU-l-KUuote6"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}}],"metadata":{"colab":{"authorship_tag":"ABX9TyN07Jl2tnE7n4Zz1GfA/Nmu","collapsed_sections":[],"name":"1_PyKeen.ipynb","toc_visible":true,"version":""},"kernelspec":{"name":"python3","display_name":"Python 3.8.8 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"f60a20abaabf5a658075b37fac599269792a9493ddacd7c14d8505185d5625aa"}},"nbformat":4,"nbformat_minor":2}