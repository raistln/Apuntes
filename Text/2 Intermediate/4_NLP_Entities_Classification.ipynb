{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqjjkpHBzulI"
   },
   "source": [
    "# Entity Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk4xsud532nH"
   },
   "source": [
    "Generally, when we read a text, we recognize entities straightway like people, values, locations and more. For example, in the sentence “ Alexander the Great, was a king of the ancient Greek kingdom of Macedonia.”, we can identify three types of entities as follows:\n",
    "\n",
    "\n",
    "> * Person: Alexander\n",
    "> * Culture: Greek\n",
    "> * Kingdom: Macedonia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "832R1sge3-Ip"
   },
   "source": [
    "We are getting an enormous amount of text data; with the help of the modern machine, we can process this text to perform tasks like Sentiment Analysis, search specific content, Named Entity Recognition, part of speech tagging, information retrieval and the list goes on.  \n",
    "\n",
    "In this practice session, with the help of the Naive Bayes classifier, we will classify the text into different entities or into what category it belongs. To perform this task, we are going to use a famous 20 newsgroup dataset. The 20 newsgroups dataset comprises around 19000 newsgroups posts on 20 different topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSJFccM54Avf"
   },
   "source": [
    "# Code Implementation to identify entities\n",
    "\n",
    "\n",
    "\n",
    "Create the Environment:\n",
    "\n",
    "Create the necessary Python environment by importing the frameworks and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q --no-warn-script-location\n",
    "!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim --user -q --no-warn-script-location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DN1rXwZ80Dlb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTBz0MZj4Fur"
   },
   "source": [
    "Let’s quickly walk through the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkSMt8Qq0PS6"
   },
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(subset='all',shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7MMEiX74Ln3"
   },
   "source": [
    "We can not directly feed raw data like this to the machine before. It should be converted into a vector of numerical values representing each sentence of the document. Utilities like CountVectorizer and TfidfTransformer provided by Sklearn are used to represent raw text into meaningful vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbml0S0W0RnS"
   },
   "outputs": [],
   "source": [
    "count_vecto = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trWhRYor4PhD"
   },
   "source": [
    "CountVectorizer is used to tokenize a given collection of text documents and build a vocabulary of known words. When you call fit_transform on a given document, the result is an encoded vector with the length of the full vocabulary and an integer count for how many times each word appeared in the document, as shown in the above picture. The vectors returned are mostly sparse. To understand what the function has done, you can convert it into a NumPy array by calling toarray() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9R6Ydxd0Vlm"
   },
   "outputs": [],
   "source": [
    "x_train_count = count_vecto.fit_transform(dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BwkTwN84SLC"
   },
   "source": [
    "This text representation will not help as it does consider the words like ‘the’,’ an’, ’a’, and so on, which appear many times throughout the document and their large counts are not meaningful in the encoded vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXFp3ET65spa"
   },
   "source": [
    "###Limit subset to the train if want see vector representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0PQQF_a4WIP"
   },
   "source": [
    "TfidfTransformer is an alternative method to perform tokenization and encoding for a given text document. TF-IDF are word frequency scores that try to highlight words that have more relevance to the context.\n",
    "\n",
    "The frequency of occurrence of terms in a document is measured by Term Frequency. Inverse Document Frequency assigns the rank to the words based on their relevance in the document; in other words, it downscale the words that appear more frequently a’,’ an’,’ the’. The use case of TF-IDF is similar to that of the CountVectorizer.\n",
    "\n",
    "Here we have already performed the first step of TF-IDF. We can directly use our countvectorizer data to calculate inverse document frequencies, i.e. to downscale the data.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AReMiVC1uKm"
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame(x_train_count.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6K-KCTd0Xrp"
   },
   "outputs": [],
   "source": [
    "tfid_vecto = TfidfTransformer()\n",
    "train_tfid = tfid_vecto.fit_transform(x_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HihAKEHq0lrV"
   },
   "outputs": [],
   "source": [
    "train_tfid.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtw6g4aK4ZJE"
   },
   "source": [
    "Use of MultiNomialNaiveBayes Classifier in identifying entities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyfG35FM4cfL"
   },
   "source": [
    "As our training data is represented in the term frequency, the MultinomialNB classifier is most suitable for discrete features such as word counts. It uses term frequency to compute maximum likelihood estimates based on training data to estimate conditional probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWX2joPg6B2a"
   },
   "outputs": [],
   "source": [
    "len(dataset.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2idi0b-0e7i"
   },
   "outputs": [],
   "source": [
    "model= MultinomialNB().fit(train_tfid,dataset.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcEXFHAt4hEb"
   },
   "source": [
    "Let’s try to predict random text which includes few targets of our trained data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nae7tjDD8RB9"
   },
   "outputs": [],
   "source": [
    "new=['i have a motorbike which made by honda.','i have GPU based system.','The Bible is simply the written core of that tradition.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHAzMVDZ8REl"
   },
   "outputs": [],
   "source": [
    "test = count_vecto.transform(new)\n",
    "test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Wg_wheO6QDr"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwU_k5zH6o56"
   },
   "outputs": [],
   "source": [
    "len(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5ujQbI6_9g9"
   },
   "outputs": [],
   "source": [
    "pred=model.predict(tfid_vecto.transform(test))\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGR_sPiV4jDN"
   },
   "source": [
    "Cross-check the outcomes with the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpNjUC2bADQF"
   },
   "outputs": [],
   "source": [
    "for a,b in zip(new,pred):\n",
    "    print(a,'---> is predictes as --->',dataset.target_names[b])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMzun0/K3oBzLhEyqyZ9kvB",
   "collapsed_sections": [],
   "name": "4_NLP_Entities_Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
