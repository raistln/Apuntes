{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEzx_et-iFvL"
   },
   "source": [
    "# **What is TF-IDF?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iM7jbHA9iFm2"
   },
   "source": [
    "TF-IDF is a method which gives us a numerical weightage of words which reflects how important the particular word is to a document in a corpus. A corpus is a collection of documents. Tf is Term frequency, and IDF is Inverse document frequency. This method is often used for information retrieval and text mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h44RloDSiyQS"
   },
   "source": [
    "Tf(Term Frequency): Term frequency can be thought of as how often does a word ‘w’ occur in a document ‘d’. More importance is given to words frequently occurring in a document. The formula of Term frequency is:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx4WNnTpi0gh"
   },
   "source": [
    "DF(inverse document frequency): Sometimes, words like ‘the’ occur a lot and do not give us vital information regarding the document. To minimize the weight of terms occurring very frequently by incorporating the weight of words rarely occurring in the document. In other words, idf is used to calculate rare words’ weight across all documents in corpus. Words rarely occurring in the corpus will have higher IDF values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6TQSexLi2IQ"
   },
   "source": [
    "Combining these two we come up with the TF-IDF score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvYjSGvIi5Ya"
   },
   "source": [
    "##**Building TF-IDF from Scratch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PztOmvlvkDKk"
   },
   "source": [
    "First and foremost is to import all the libraries needed for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q --no-warn-script-location\n",
    "!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim tqdm --user -q --no-warn-script-location\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkF4e8nxh4kv"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32q0izMbkIzx"
   },
   "source": [
    "Basic libraries imported. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0s0Jx13BkKoT"
   },
   "source": [
    "‘from sklearn.preprocessing import normalize’:- As the documentation says, normalization here means making our data have a unit length, so specifying which length (i.e. which norm) is also required. Here Sklearn applies L2-normalization on its output matrix, i.e. Euclidean length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdxnWvKqkMUr"
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b5Ej8AkkSdi"
   },
   "source": [
    "For simplicity, we are taking four reviews or documents as our data corpus and storing them in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QepsX_fkUV3"
   },
   "outputs": [],
   "source": [
    "def IDF(corpus, unique_words):\n",
    "  idf_dict={}\n",
    "  N=len(corpus)\n",
    "  for i in unique_words:\n",
    "    count=0\n",
    "    for sen in corpus:\n",
    "      if i in sen.split():\n",
    "        count=count+1\n",
    "      idf_dict[i]=(math.log((1+N)/(count+1)))+1\n",
    "  return idf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uDIag7UkZOr"
   },
   "source": [
    "We will be defining a function IDF whose parameter will be the corpus and the unique words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwRsPQ0CkcFj"
   },
   "source": [
    "The reason why we are adding ‘1’ to numerator and denominator and also to the whole equation of ‘idf_dict[i]’ is to maintain numerical stability. There might be situations where there are no values, which will generate an error(avoiding division of zeros). So to avoid that error, we are creating numerical stability.\n",
    "\n",
    "This code snippet will generate the idf values of all the unique words when ‘fit’ function is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vB2PDJ7wkbYV"
   },
   "outputs": [],
   "source": [
    "def fit(whole_data):\n",
    "  unique_words = set()\n",
    "  if isinstance(whole_data, (list,)):\n",
    "    for x in whole_data:\n",
    "      for y in x.split():\n",
    "        if len(y)<2:\n",
    "          continue\n",
    "        unique_words.add(y)\n",
    "    unique_words = sorted(list(unique_words))\n",
    "    vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "    Idf_values_of_all_unique_words=IDF(whole_data,unique_words)\n",
    "  return vocab, Idf_values_of_all_unique_words\n",
    "Vocabulary, idf_of_vocabulary=fit(corpus) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "193Wy06Dkick"
   },
   "source": [
    "Here we initialised ‘unique_words’ as a set to get all the unique values.(Set has a property where it does not print out duplicate values).\n",
    "\n",
    "Checking if the ‘whole_data’ is a list or not. In our case, Corpus is a list. We are splitting the list and iterating over the list to find unique words and appending them in the set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiYruHE5kqZC"
   },
   "source": [
    "All words having a length of less than two are discarded.\n",
    "\n",
    "We are calling IDF function inside the fit function which will give us the idf values of all the unique words generated and will store them in ‘idf_values_of_all_unique_words’ \n",
    "\n",
    "The fit function will return the words and their idf values respectively.\n",
    "\n",
    "We will assign the values to ‘Vocabulary’ and ‘idf_of_vocabulary’ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 551,
     "status": "ok",
     "timestamp": 1622014098548,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "nKateOBBksy2",
    "outputId": "83e99b36-0eb9-4ff6-fa2f-c2f467cf9bc2"
   },
   "outputs": [],
   "source": [
    "print(list(Vocabulary.keys())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 389,
     "status": "ok",
     "timestamp": 1622014108228,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "Ox5s1HxYkxeK",
    "outputId": "1fec2e39-e212-4594-fa14-f3e5c109775f"
   },
   "outputs": [],
   "source": [
    "print(list(idf_of_vocabulary.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ws7IODgUk0il"
   },
   "source": [
    "This is the output we will get when we perform the fit function.\n",
    "\n",
    "We are coding the fit and transform the function of TFIDFVectorizer.\n",
    "\n",
    "Now jumping towards the transform function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1622014130526,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "NJFzNjZQkRkF",
    "outputId": "343749bd-9b3b-4fe4-ba49-d891dde98dc0"
   },
   "outputs": [],
   "source": [
    "def transform(dataset,vocabulary,idf_values):\n",
    "    sparse_matrix= csr_matrix( (len(dataset), len(vocabulary)), dtype=np.float64)\n",
    "    for row  in range(0,len(dataset)):\n",
    "      number_of_words_in_sentence=Counter(dataset[row].split())\n",
    "      for word in dataset[row].split():\n",
    "          if word in  list(vocabulary.keys()):\n",
    "              tf_idf_value=(number_of_words_in_sentence[word]/len(dataset[row].split()))*(idf_values[word])\n",
    "              sparse_matrix[row,vocabulary[word]]=tf_idf_value\n",
    "    print(\"NORM FORM\\n\",normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False))\n",
    "    output =normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False)\n",
    "    return output\n",
    "final_output=transform(corpus,Vocabulary,idf_of_vocabulary)\n",
    "print(final_output.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCAV5ilulINp"
   },
   "source": [
    "Here we are using the transform function to get a sparse matrix representation output of the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30hax3TplLQW"
   },
   "source": [
    "We used the TF-IDF formula to calculate the values of all the unique words in the set.\n",
    "\n",
    "As we talked earlier about the l2 norm, here sklearn implements l2 so with the help of ‘normalize’  we initialize l2 norm to get perfect output. \n",
    "\n",
    "We want the sparse matrix representation so initialised ‘sparse_matrix’ in ‘normalize’ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgLWH7oDlswR"
   },
   "source": [
    "Sparse matrix is a type of matrix with very few non zero values and more zero values. We use sparse matrix only when the matrix has several zero values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9HHuKghlv2l"
   },
   "source": [
    "We can convert the sparse matrix representation to a dense representation or dense matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1622014372775,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "sJ_fjDpolx15",
    "outputId": "f1be310b-75c2-4d9a-f698-f84675933a46"
   },
   "outputs": [],
   "source": [
    "print(final_output[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prpNOPgzWgiU"
   },
   "source": [
    "# **Related Articles:**\n",
    "\n",
    "> * [TF-IDF from Scratch in Python](https://analyticsindiamag.com/hands-on-implementation-of-tf-idf-from-scratch-in-python/)\n",
    "\n",
    "> * [Continuous Bag of Words](https://analyticsindiamag.com/the-continuous-bag-of-words-cbow-model-in-nlp-hands-on-implementation-with-codes/)\n",
    "\n",
    "> * [NLP Case Study of Documents Similarity](https://analyticsindiamag.com/nlp-case-study-identify/)\n",
    "\n",
    "> * [Review Classification](https://analyticsindiamag.com/step-by-step-guide-to-reviews-classification-using-svc-naive-bayes-random-forest/)\n",
    "\n",
    "> * [Multi Class Text Classification](https://analyticsindiamag.com/multi-class-text-classification-in-pytorch-using-torchtext/)\n",
    "\n",
    "> * [Text Classification](https://analyticsindiamag.com/how-to-solve-your-first-ever-nlp-classification-challenge/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOS276mBPYT25+wkFP1N2G9",
   "collapsed_sections": [],
   "name": "TF_IDF_from_Scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
