{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zojLxgmW9JMt"
   },
   "source": [
    "# **NLP Classification Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqY7uQKY9NMs"
   },
   "source": [
    "NLP should be one of the most updated skill sets in a Data Scientist’s Tool kit. In this article, we will learn to implement Natural Language Processing in Machine Learning in the simplest way possible to solve MachineHack’s – [Whose Line Is It Anyway: Identify The Author Hackathon](https://machinehack.com/hackathons/whose_line_is_it_anyway_identify_the_author_hackathon/scripts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWoidpC5C9Ah"
   },
   "source": [
    "## **About The Data Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUxrLsnYDA2a"
   },
   "source": [
    "The dataset we are going to use consists of sentences from thousands of books of 10 authors. The idea is to train our machine to predict which author has written a specific sentence. This is an NLP classification problem where the objective is to classify each sentence based on who wrote it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRX2-kbzDDD8"
   },
   "source": [
    "## **Natural Language Processing With Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYAqV28ADFTE"
   },
   "source": [
    "We will implement NLP in 8 simple steps as explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUAsDfMWDIFr"
   },
   "source": [
    "### **Importing necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q --no-warn-script-location\n",
    "!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn nltk gensim --user -q --no-warn-script-location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QCr077W8Jk2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rf5uzCfWD9VL"
   },
   "source": [
    "The above code block consists of the necessary libraries that we need to implement our NLP classifier. We will look into each of them as we come across various methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3ltjTWMD-5B"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o325PGO4EE2k"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('https://gitlab.com/AnalyticsIndiaMagazine/practicedatasets/-/raw/main/NLP_classification/TRAIN.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0ecsb0yEKdN"
   },
   "source": [
    "The above code block reads the data from the csv file and loads it into a pandas data-frame using the read_csv method of the pandas library that we imported earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjDniR1zELNC"
   },
   "source": [
    "Let’s have a peek at the dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1621939205849,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "-pcvEb-HERQV",
    "outputId": "f9b1c6a0-df99-43cc-be02-9313587a6815"
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tprNPq_6EUM6"
   },
   "source": [
    "### **Cleaning and preprocessing the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfXygAhZEXeb"
   },
   "source": [
    "Cleaning the data is one of the most essential tasks in not just Natural Language Processing but in the entire Data Science spectrum. In Natural Language Processing, there are various stages of cleaning. Some of the basic stages are listed below :\n",
    "\n",
    "> * Cleaning the test for unnecessary data (noises such as symbols, emojis, special characters, etc.)\n",
    "> * Stemming or lemmatization for reducing the words to its root form.\n",
    "> * Removing stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1042439,
     "status": "ok",
     "timestamp": 1621940248657,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "0OdY6wWgE6qK",
    "outputId": "54e9a4b2-3160-402d-ef88-7edf1fbe895a"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords') #downloading the stopwords from nltk\n",
    "corpus = [] # List for storing cleaned data\n",
    "ps = PorterStemmer() #Initializing object for stemming\n",
    "for i in range(len(dataset)): # for each obervation in the dataset\n",
    "   #Removing special characters\n",
    "   text = re.sub('[^a-zA-Z]', ' ', dataset['text'][i]).lower().split()\n",
    "   #Stemming and removing stop words\n",
    "   text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "   #Joining all the cleaned words to form a sentence\n",
    "   text = ' '.join(text)\n",
    "   #Adding the cleaned sentence to a list\n",
    "   corpus.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvtwIRgLE8Xd"
   },
   "source": [
    "The NLTK library comes with a collection of stopwords which we can use to clean the dataset. The PorterStemmer method of nltk.stem.porter library is used to perform stemming. In the above code block, we traverse through each observation in the dataset, removing special characters, performing stemming and removing stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbXp36fqFA1t"
   },
   "source": [
    "Let’s see the cleaned data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1621940248661,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "Ur8rgegaFBY3",
    "outputId": "861cddee-afdf-4848-b9d0-e97907b61bb1"
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAq0heXVFL1V"
   },
   "source": [
    "### **Generating Count Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUOqEC7UFQ-J"
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features = 120)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AL_7eYyXFSte"
   },
   "source": [
    "With the above code block, we will create a Bag-of-Words model. The CountVectorizer method imported from sklearn.feature_extraction.text creates a matrix of vectors consisting of the counts of each word in a sentence. The parameter max_features = 120 selects a maximum of 120 unique words. We transform the cleaned data in corpus into CountVector X which is the independent variable set for the test classifier that we will build in the coming steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yU2XT3qFFUVe"
   },
   "source": [
    "Here is what X  looks like :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1621940251424,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "n1Wn0dk2FU6k",
    "outputId": "ba59bb2e-01a9-4450-da04-380a67f3cdb5"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff4PEgUSFVkg"
   },
   "source": [
    "Each row represents a row in the actual observation and each column represents a word of the 120 selected words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QY_quy4TFYep"
   },
   "source": [
    "### **Splitting the dataset into the Training set and Validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbRYvlggFxC-"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQ17QIdeFy6t"
   },
   "source": [
    "In the above code block, we split the dataset into training and validation sets. The parameter test_size = 0.2 specifies that the test set (X_val & y_val ) should consist of 20 % of the overall data in X and y. The random_state parameter allows us to set a seed value to reproduce the exact same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KP9N6Be1F0i_"
   },
   "source": [
    "## **Building a classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45028,
     "status": "ok",
     "timestamp": 1621940296444,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "yEp0OrY2HBsK",
    "outputId": "8c59c394-32ef-4271-97d4-e75bfb67cf65"
   },
   "outputs": [],
   "source": [
    "classifier = SVC()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIyv7CDzHIGx"
   },
   "source": [
    "Since we are ready with the training data we can now use it to train a classifier. The above code block initializes a Support Vector Classifier and fits the training data for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezKG11Y1HJ0R"
   },
   "source": [
    "### **Predicting the author**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAkzJXXSHM7E"
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjSqi6JxHQ3t"
   },
   "source": [
    "After training the classifier with X_train and y_train, we can now make the classifier to predict the authors for the texts in the validation set X_val."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61HsqiFcHS2y"
   },
   "source": [
    "## **Evaluating the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7h5EVXAHVZ_"
   },
   "source": [
    "After predicting for the validation set, we need to check how many of the predictions are actually right.To do this, we will make use of the confusion matrix.Using the confusion matrix we will compare the predicted values in y_pred and the actual values in y_val.The accuracy from a confusion matrix can be calculated by summing up the diagonal elements and diving it by the total sum of elements in the matrix. We define a method as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1621940310343,
     "user": {
      "displayName": "Aishwarya Verma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiG6BREocxcd5R6rzlQGApoCsYso7BQAh63eXNz6Q=s64",
      "userId": "06108390091304498033"
     },
     "user_tz": -330
    },
    "id": "NQ-KE8x1HX75",
    "outputId": "f9bb8487-a8bc-49ae-ff02-2cd52501dcbb"
   },
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrix):\n",
    "   diagonal_sum = confusion_matrix.trace()\n",
    "   sum_of_all_elements = confusion_matrix.sum()\n",
    "   return diagonal_sum / sum_of_all_elements\n",
    "\n",
    "#Creating the confusion matrix with y_val and y_pred\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "print(\"Accuracy : \", accuracy(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnmReyIcHbC7"
   },
   "source": [
    "This means that 67 % of the overall predictions were actually true when compared to the real observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prpNOPgzWgiU"
   },
   "source": [
    "# **Read more articles on:**\n",
    "\n",
    "\n",
    "> * [Text Classification](https://analyticsindiamag.com/how-to-solve-your-first-ever-nlp-classification-challenge/)\n",
    "\n",
    "> * [Word Frequency](https://analyticsindiamag.com/using-natural-language-processing-to-check-word-frequency-in-the-adventure-of-sherlock-holmes/)\n",
    "\n",
    "> * [Vocabulary Builder](https://analyticsindiamag.com/how-to-create-a-vocabulary-builder-for-nlp-tasks/)\n",
    "\n",
    "> * [Spacy Basics](https://analyticsindiamag.com/nlp-deep-learning-nlp-framework-nlp-model/)\n",
    "\n",
    "> * [StanfordCore NLP](https://analyticsindiamag.com/how-to-use-stanza-by-stanford-nlp-group-with-python-code/)\n",
    "\n",
    "> * [Tokenization in NLP](https://analyticsindiamag.com/hands-on-guide-to-different-tokenization-methods-in-nlp/)\n",
    "\n",
    "> * [Keras Tokenizer](https://analyticsindiamag.com/tutorial-on-keras-tokenizer-for-text-classification-in-nlp/)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNHezcTtqp4scQtAX9yDce5",
   "collapsed_sections": [],
   "name": "1_NLP_Classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "bf1c1c88dcd4a5670b9a47bf2bba5adec14ec31820c4d35855d0349e55a0f49f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
