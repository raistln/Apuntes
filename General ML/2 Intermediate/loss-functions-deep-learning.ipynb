{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytkf7CBYy4U5"
   },
   "source": [
    "## In this practice session, we will learn to about the popular Regression and Classification Loss Functions used in Deep learning.\n",
    "\n",
    "## Regression Loss Functions\n",
    "\n",
    "## **1.Mean Squared Error (MSE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q\n",
    "!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn keras tensorflow opencv-python scikit-image --user -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "updbNCdFywxq",
    "outputId": "8b26fc01-363c-4c94-af47-310941794f37"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=5000, n_features=20, noise=0.1, random_state=1)\n",
    "\n",
    "# standardize dataset\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = StandardScaler().fit_transform(y.reshape(len(y),1))[:,0]\n",
    "\n",
    "# split into train and test\n",
    "train1 = 2500\n",
    "trainX, testX = X[:train1, :], X[train1:, :]\n",
    "trainy, testy = y[:train1], y[train1:]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "train_mse = model.evaluate(trainX, trainy, verbose=0)\n",
    "test_mse = model.evaluate(testX, testy, verbose=0)\n",
    "# print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "\n",
    "# plot loss during training\n",
    "pyplot.title('Mean Squared Error')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPEWh7G18Gir"
   },
   "source": [
    "## **2.Mean Squared Logarithmic Error Loss (MSLE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEbV-RG943GW",
    "outputId": "60749da0-308d-4558-e3d1-1aa3e3c1adfa"
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='mean_squared_logarithmic_error', optimizer=opt, metrics=['mse'])\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "train_mse = model.evaluate(trainX, trainy, verbose=0)\n",
    "test_mse = model.evaluate(testX, testy, verbose=0)\n",
    "\n",
    "# plot loss during training\n",
    "pyplot.title('Mean Squared Logarithmic Error Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ig9hUK958N30"
   },
   "source": [
    "## **3.Mean Absolute Error Loss (MAE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ahc_ZpXq6wVK",
    "outputId": "62fc6e71-f67e-4202-a1e5-463051c88395"
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mse'])\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "train_mse = model.evaluate(trainX, trainy, verbose=0)\n",
    "test_mse = model.evaluate(testX, testy, verbose=0)\n",
    "\n",
    "# plot loss during training\n",
    "pyplot.title('Mean Absolute Error Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xwNJ1ur74Fm"
   },
   "source": [
    "## **Binary Classification Loss Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nuao3Q1A8fyn"
   },
   "source": [
    "## **1.Cross Entropy Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAG353-a78ZP",
    "outputId": "0176f530-0f22-4188-9b75-7702a16f9e4d"
   },
   "outputs": [],
   "source": [
    "# Cross entropy loss\n",
    "from sklearn.datasets import make_circles\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_circles(n_samples=5000, noise=0.1, random_state=1)\n",
    "\n",
    "# split into train and test\n",
    "train1 = 2500\n",
    "trainX, testX = X[:train1, :], X[train1:, :]\n",
    "trainy, testy = y[:train1], y[train1:]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "\n",
    "# plot loss during training\n",
    "pyplot.title('Binary Cross Entropy Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akVwwOH484iB"
   },
   "source": [
    "## **2.Hinge Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EPYa_4XQ8-XL",
    "outputId": "5c549a75-a8a4-4b98-927b-3d3c42dcf6b8"
   },
   "outputs": [],
   "source": [
    "# Hinge Loss\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='hinge', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "\n",
    "# plot loss during training\n",
    "pyplot.title('Hinge Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSpyDjSE-1wG"
   },
   "source": [
    "## **Multi Class Classification Loss Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGv8stwf8-Cg"
   },
   "source": [
    "## **1.Categorical Cross Entropy Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfsvFKXH-0x7",
    "outputId": "eca245cb-c0c4-4d73-8f3f-9e9e651b04a9"
   },
   "outputs": [],
   "source": [
    "# Multi-class Cross-entropy loss\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorlfow.keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=5000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "\n",
    "# split into train and test\n",
    "train1 = 500\n",
    "trainX, testX = X[:train1, :], X[train1:, :]\n",
    "trainy, testy = y[:train1], y[train1:]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "\n",
    "# plot loss during training\n",
    "pyplot.title('Categorical Cross Entropy')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eP2FySv9EBq"
   },
   "source": [
    "## **2.KL Divergence Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdwEHTem_bTL",
    "outputId": "bd7f425b-56ce-4da7-a00b-bf0b72d14d0a"
   },
   "outputs": [],
   "source": [
    "# KL Divergence\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='kullback_leibler_divergence', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=50, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "\n",
    "# plot loss during training\n",
    "pyplot.title('KL Divergence Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
