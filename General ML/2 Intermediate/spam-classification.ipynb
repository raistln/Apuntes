{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1oboG2WIgm5"
   },
   "source": [
    "## **In this practice we will use natural language processing to build a spam classifier**\n",
    "\n",
    "## **We will make use of the sms spam classification data for the implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QT0e91prLONA"
   },
   "source": [
    "**Data processing**\n",
    "*   Import the required packages\n",
    "*   Load the data into train and test variables\n",
    "*   Remove the unwanted data columns\n",
    "*   Build wordcloud to see which message is spam and which is not.\n",
    "*   Remove the stop words and punctuations\n",
    "*   Convert the text data into vectors\n",
    "\n",
    "**Building a classification model**\n",
    "*   Split the data into train and test sets\n",
    "*   Use Sklearn built in classifiers to build the models\n",
    "*   Train the data on the model\n",
    "*   Make predictions on new data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ag7hrorT1dS"
   },
   "source": [
    "## **Import the required packages that is to be used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q\n",
    "!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn keras tensorflow opencv-python scikit-image nltk bs4 wordcloud --user -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWibLvVTT1dS"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import sklearn\n",
    "import pickle\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,StratifiedKFold,cross_val_score,learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vS4d4NtJT1dW"
   },
   "source": [
    "## **Preprocessing and Exploring the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcTtyR6vT1dY",
    "outputId": "e0f41928-e2d5-44c2-815a-78fa28b70e37"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OHtOc2jT1dc"
   },
   "source": [
    "## **Removing unwanted columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sclvYeoVT1dd"
   },
   "outputs": [],
   "source": [
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v2\" : \"text\", \"v1\":\"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPCJVr9zT1di",
    "outputId": "ed4a1a1e-6fd9-482f-8946-dcb69be21141"
   },
   "outputs": [],
   "source": [
    "data[1990:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNWfIdYvT1dm",
    "outputId": "ff1affde-1ba2-4586-8356-4be806be30b2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUaVXJY9T1d1"
   },
   "outputs": [],
   "source": [
    "# Import nltk packages and Punkt Tokenizer Models\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2Fi_co9T1d4"
   },
   "source": [
    "### **WordClouds- to see which words are common in SPAM and NOT SPAM mesaages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cbnNrgnT1d5"
   },
   "outputs": [],
   "source": [
    "ham_words = ''\n",
    "spam_words = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xtSTyueT1d8"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o13b2oOET1eB"
   },
   "outputs": [],
   "source": [
    "# Creating a corpus of spam messages\n",
    "for val in data[data['label'] == 'spam'].text:\n",
    "    text = val.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for words in tokens:\n",
    "        spam_words = spam_words + words + ' '\n",
    "# Creating a corpus of ham messages        \n",
    "for val in data[data['label'] == 'ham'].text:\n",
    "    text = val.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for words in tokens:\n",
    "        ham_words = ham_words + words + ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wripxz0xT1eF"
   },
   "source": [
    "## **Creating  Spam wordcloud and ham wordcloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zv7Fryv5T1eG"
   },
   "outputs": [],
   "source": [
    "spam_wordcloud = WordCloud(width=500, height=300).generate(spam_words)\n",
    "ham_wordcloud = WordCloud(width=500, height=300).generate(ham_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIHFH-BJT1eJ"
   },
   "outputs": [],
   "source": [
    "#Spam Word cloud\n",
    "plt.figure( figsize=(10,8), facecolor='w')\n",
    "plt.imshow(spam_wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHgEfutRT1eO"
   },
   "outputs": [],
   "source": [
    "#Creating Ham wordcloud\n",
    "plt.figure( figsize=(10,8), facecolor='g')\n",
    "plt.imshow(ham_wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opaPXshjT1eR"
   },
   "outputs": [],
   "source": [
    "data = data.replace(['ham','spam'],[0, 1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRveTZ9GT1eV"
   },
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5Lz8iQAT1eY"
   },
   "source": [
    "## **Removing Stopwords from the messages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GO4u5sUqT1eY"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FMXzJoeT1eb"
   },
   "source": [
    "## **Remove punctuation  and stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edDpGjhST1ec"
   },
   "outputs": [],
   "source": [
    "#remove the punctuations and stopwords\n",
    "import string\n",
    "def text_process(text):\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUqNFnNPT1ef"
   },
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEsCTryOT1ek"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0ZAwZiAT1en",
    "outputId": "b8d9c95c-7469-4619-e865-c411f5e1e970"
   },
   "outputs": [],
   "source": [
    "text = pd.DataFrame(data['text'])\n",
    "label = pd.DataFrame(data['label'])\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkDiOQ7TT1er"
   },
   "source": [
    "## **Converting words to vectors**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLk0XaOCT1es",
    "outputId": "5a53d429-d513-4077-89a2-6f8d7ed24fe6"
   },
   "outputs": [],
   "source": [
    "## Counting how many times a word appears in the dataset\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "total_counts = Counter()\n",
    "for i in range(len(text)):\n",
    "    for word in text.values[i][0].split(\" \"):\n",
    "        total_counts[word] += 1\n",
    "\n",
    "print(\"Total words in data set: \", len(total_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGioZnysT1e7",
    "outputId": "53dc4bfe-240e-427a-b80a-5ea668b9d9c6"
   },
   "outputs": [],
   "source": [
    "# Sorting in decreasing order (Word with highest frequency appears first)\n",
    "vocab = sorted(total_counts, key=total_counts.get, reverse=True)\n",
    "print(vocab[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pme-lXRfT1e_"
   },
   "outputs": [],
   "source": [
    "# Mapping from words to index\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "word2idx = {}\n",
    "#print vocab_size\n",
    "for i, word in enumerate(vocab):\n",
    "    word2idx[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvKOwVGBT1fF"
   },
   "outputs": [],
   "source": [
    "# Text to Vector\n",
    "def text_to_vector(text):\n",
    "    word_vector = np.zeros(vocab_size)\n",
    "    for word in text.split(\" \"):\n",
    "        if word2idx.get(word) is None:\n",
    "            continue\n",
    "        else:\n",
    "            word_vector[word2idx.get(word)] += 1\n",
    "    return np.array(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlQa0HhQT1fJ"
   },
   "outputs": [],
   "source": [
    "# Convert all titles to vectors\n",
    "word_vectors = np.zeros((len(text), len(vocab)), dtype=np.int_)\n",
    "for i, (_, text_) in enumerate(text.iterrows()):\n",
    "    word_vectors[i] = text_to_vector(text_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cR-AVicuT1fS"
   },
   "source": [
    "## **Converting words to vectors using TFIDF Vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1p_1l3LrVsaB",
    "outputId": "1e1232dc-60b4-4af5-b2e3-31fe5917e39b"
   },
   "outputs": [],
   "source": [
    "#convert the text data into vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(data['text'])\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9fGfx5sT1fX"
   },
   "outputs": [],
   "source": [
    "#features = word_vectors\n",
    "features = vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lN_sj7EhT1fb"
   },
   "source": [
    "## **Splitting into training and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKy_fPvST1fc"
   },
   "outputs": [],
   "source": [
    "#split the dataset into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data['label'], test_size=0.15, random_state=111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vnnkK2fT1fh"
   },
   "source": [
    "## **Classifying using sklearn pre built classifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KOPj1HQhT1fi"
   },
   "outputs": [],
   "source": [
    "#import sklearn packages for building classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1CE9S30T1fl"
   },
   "outputs": [],
   "source": [
    "#initialize multiple classification models \n",
    "svc = SVC(kernel='sigmoid', gamma=1.0)\n",
    "knc = KNeighborsClassifier(n_neighbors=49)\n",
    "mnb = MultinomialNB(alpha=0.2)\n",
    "dtc = DecisionTreeClassifier(min_samples_split=7, random_state=111)\n",
    "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "rfc = RandomForestClassifier(n_estimators=31, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fP-v8pdvT1fo"
   },
   "outputs": [],
   "source": [
    "#create a dictionary of variables and models\n",
    "clfs = {'SVC' : svc,'KN' : knc, 'NB': mnb, 'DT': dtc, 'LR': lrc, 'RF': rfc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WimrKW_XT1fv"
   },
   "outputs": [],
   "source": [
    "#fit the data onto the models\n",
    "def train(clf, features, targets):    \n",
    "    clf.fit(features, targets)\n",
    "\n",
    "def predict(clf, features):\n",
    "    return (clf.predict(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GKN_IUWT1fy"
   },
   "outputs": [],
   "source": [
    "pred_scores_word_vectors = []\n",
    "for k,v in clfs.items():\n",
    "    train(v, X_train, y_train)\n",
    "    pred = predict(v, X_test)\n",
    "    pred_scores_word_vectors.append((k, [accuracy_score(y_test , pred)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryfz7ed_T1f2"
   },
   "source": [
    "## **Predictions using TFIDF Vectorizer algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIRrQXPOT1f2",
    "outputId": "a5b34508-34d9-421a-963c-f20b5838607f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_scores_word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSExV8qIT1gC"
   },
   "source": [
    "##**Model predictions**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yz_DheJ1T1gN"
   },
   "outputs": [],
   "source": [
    "#write functions to detect if the message is spam or not\n",
    "def find(x):\n",
    "    if x == 'spam':\n",
    "        print (\"Message is SPAM\")\n",
    "    else:\n",
    "        print (\"Message is NOT Spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5ytyDshT1gR"
   },
   "outputs": [],
   "source": [
    "newtext = [\"win FREE mobiles and play games\"]\n",
    "integers = vectorizer.transform(newtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imQD1ahzT1gT",
    "outputId": "ed529783-c2ce-4e1b-c304-7589d502a52f"
   },
   "outputs": [],
   "source": [
    "x = mnb.predict(integers)\n",
    "find(x)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQ62GGOlFT0o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
