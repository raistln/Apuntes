{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adcLQt-yKwJS"
   },
   "source": [
    "# **Image Generation with Tensorflow Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaVkVWR2LSYK"
   },
   "source": [
    "Computer Vision is a wide, deep learning field with enormous applications. Image Generation is one of the most curious applications in Computer Vision. Again, Image Generation has a great collection of tasks; to mention, a few can outperform humans. Most image generation tasks are common for videos, too, since a video is a sequence of images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4kpdg_NLS_-"
   },
   "source": [
    "To read about all the tasks of image generation, please refer [here](https://analyticsindiamag.com/getting-started-image-generation-tensorflow-keras/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6J_BU-rLl52"
   },
   "source": [
    "This practice session discusses the concepts behind image generation and the code implementation of Variational Autoencoder with a practical example using TensorFlow Keras. TensorFlow is one of the top preferred frameworks for deep learning processes. Keras is a high-level API built on top of TensorFlow, which is meant exclusively for deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GSHnHQkGa3Q"
   },
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q --no-warn-script-location\n",
    "!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn tensorflow keras opencv-python pillow scikit-image --user -q --no-warn-script-location\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3Yoc4KLLQwe"
   },
   "source": [
    "## **Get Started With Image Generation**\n",
    "\n",
    "References:\n",
    "\n",
    "https://www.tensorflow.org/tutorials/generative/cvae\n",
    "\n",
    "https://keras.io/examples/generative/vae/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__O1p66CLQwh"
   },
   "source": [
    "### **Create the Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DShmMOO2L0bG"
   },
   "source": [
    "Create the necessary Python environment by importing the required frameworks, libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-22T07:27:04.877168Z",
     "iopub.status.busy": "2021-05-22T07:27:04.876855Z",
     "iopub.status.idle": "2021-05-22T07:27:10.008410Z",
     "shell.execute_reply": "2021-05-22T07:27:10.007593Z",
     "shell.execute_reply.started": "2021-05-22T07:27:04.877140Z"
    },
    "id": "m_w-4Z6kLQwi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24UgQyy7LQwk"
   },
   "source": [
    "### **Load data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fDsiniFL4ht"
   },
   "source": [
    "We use Fashion MNIST data available in-built with Keras Datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T07:27:10.009743Z",
     "iopub.status.busy": "2021-05-22T07:27:10.009513Z",
     "iopub.status.idle": "2021-05-22T07:27:11.360882Z",
     "shell.execute_reply": "2021-05-22T07:27:11.360174Z",
     "shell.execute_reply.started": "2021-05-22T07:27:10.009722Z"
    },
    "id": "ERDq-6gGLQwl"
   },
   "outputs": [],
   "source": [
    "fashion_data = keras.datasets.fashion_mnist.load_data()\n",
    "(x_train,y_train),(x_val,y_val)= fashion_data \n",
    "\n",
    "x_train.shape, x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDVeYPZBL9rF"
   },
   "source": [
    "There are 60000 images in the train set and 10000 images in the validation set. Each image is a grayscale (1 channel) image of shape 28 by 28. Image generation using VAE follows a self-supervised approach. Therefore, we may delete the y_train and y_val data to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T07:27:11.363037Z",
     "iopub.status.busy": "2021-05-22T07:27:11.362693Z",
     "iopub.status.idle": "2021-05-22T07:27:11.367896Z",
     "shell.execute_reply": "2021-05-22T07:27:11.367133Z",
     "shell.execute_reply.started": "2021-05-22T07:27:11.363001Z"
    },
    "id": "5rkqSFSaLQwm"
   },
   "outputs": [],
   "source": [
    "del y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OfUvEyiMBs0"
   },
   "source": [
    "Visualize an example from the downloaded image data to get a better insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T07:27:11.369411Z",
     "iopub.status.busy": "2021-05-22T07:27:11.369147Z",
     "iopub.status.idle": "2021-05-22T07:27:11.568995Z",
     "shell.execute_reply": "2021-05-22T07:27:11.568140Z",
     "shell.execute_reply.started": "2021-05-22T07:27:11.369387Z"
    },
    "id": "vvnpyBlmLQwn"
   },
   "outputs": [],
   "source": [
    "plt.imshow(x_train[10])\n",
    "plt.colorbar()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsMdXMxAMEg2"
   },
   "source": [
    "It can be observed that the pixel values range from 0 to 255. We need to scale the values. Further, convolutional layers expect three-dimensional inputs, whereas the available images are in two dimensions. Self-supervised models do not require separate datasets for training and validation. We can merge the available training and validation sets to get relatively large data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T07:27:11.570558Z",
     "iopub.status.busy": "2021-05-22T07:27:11.570260Z",
     "iopub.status.idle": "2021-05-22T07:27:11.810290Z",
     "shell.execute_reply": "2021-05-22T07:27:11.809543Z",
     "shell.execute_reply.started": "2021-05-22T07:27:11.570531Z"
    },
    "id": "bRRjxmNnLQwo"
   },
   "outputs": [],
   "source": [
    "data = tf.concat([x_train, x_val], axis=0)\n",
    "data = tf.expand_dims(data, -1)\n",
    "data = tf.cast(data, tf.float32)\n",
    "data = data / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNHVRu5PLQwq"
   },
   "source": [
    "## **Build the VAE Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T07:27:11.813575Z",
     "iopub.status.busy": "2021-05-22T07:27:11.813335Z",
     "iopub.status.idle": "2021-05-22T07:27:11.818257Z",
     "shell.execute_reply": "2021-05-22T07:27:11.817506Z",
     "shell.execute_reply.started": "2021-05-22T07:27:11.813552Z"
    },
    "id": "L1kVRxwRLQwr"
   },
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        mean, logvar = inputs\n",
    "        batch = tf.shape(mean)[0]\n",
    "        dim = tf.shape(mean)[1]\n",
    "        eps = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return mean + tf.exp(0.5 * logvar) * eps\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNgr94KYLQwr"
   },
   "source": [
    "### **Build the Encoder network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-w_3KZXMTy7"
   },
   "source": [
    "Build an encoder that takes an image as input and outputs sampling representation as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T07:27:11.819452Z",
     "iopub.status.busy": "2021-05-22T07:27:11.819131Z",
     "iopub.status.idle": "2021-05-22T07:27:11.969506Z",
     "shell.execute_reply": "2021-05-22T07:27:11.968928Z",
     "shell.execute_reply.started": "2021-05-22T07:27:11.819428Z"
    },
    "id": "efLmCNtgLQws"
   },
   "outputs": [],
   "source": [
    "# latent_dim = 2\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "mean = layers.Dense(2, name=\"mean\")(x)\n",
    "logvar = layers.Dense(2, name=\"logvar\")(x)\n",
    "z = Sampling()([mean, logvar])\n",
    "encoder = keras.Model(encoder_inputs, [mean, logvar, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6gWv8tUMWEB"
   },
   "source": [
    "Plotting the model is always a great way to ensure shapes and workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T07:27:11.971118Z",
     "iopub.status.busy": "2021-05-22T07:27:11.970790Z",
     "iopub.status.idle": "2021-05-22T07:27:12.369486Z",
     "shell.execute_reply": "2021-05-22T07:27:12.368856Z",
     "shell.execute_reply.started": "2021-05-22T07:27:11.971093Z"
    },
    "id": "Mnm8byZzLQws"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(encoder, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxlWKdQ7LQwu"
   },
   "source": [
    "### **Build the Decoder network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HHeLxcvMa7r"
   },
   "source": [
    "Build a decoder that takes the inputs from the encoder, performs transpose convolution, and develops a synthetic image of size 14 by 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T07:27:12.370930Z",
     "iopub.status.busy": "2021-05-22T07:27:12.370610Z",
     "iopub.status.idle": "2021-05-22T07:27:12.433516Z",
     "shell.execute_reply": "2021-05-22T07:27:12.432816Z",
     "shell.execute_reply.started": "2021-05-22T07:27:12.370905Z"
    },
    "id": "EPqRHEbnLQwu"
   },
   "outputs": [],
   "source": [
    "latent_inputs = keras.Input(shape=(2,))\n",
    "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "# form 7 by 7 feature map\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "# form 14 by 14 feature map\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# form 28 by 28 feature map\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# form the sigmoid output\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e23DGMlGMdKw"
   },
   "source": [
    "Plot the decoder to get a better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T07:27:12.434675Z",
     "iopub.status.busy": "2021-05-22T07:27:12.434451Z",
     "iopub.status.idle": "2021-05-22T07:27:12.524195Z",
     "shell.execute_reply": "2021-05-22T07:27:12.523484Z",
     "shell.execute_reply.started": "2021-05-22T07:27:12.434654Z"
    },
    "id": "wvk7ySrKLQwv"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(decoder, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bgBZqpOLQww"
   },
   "source": [
    "### **Define Training with metrics and lossesd text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zJY3mctMgxh"
   },
   "source": [
    "Let’s formulate the training methodology by customizing the losses and metrics as necessitated by the original research paper. The loss is the binary cross-entropy, calculated by comparing the original input image with the reconstructed synthetic (generated) image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T07:42:00.212717Z",
     "iopub.status.busy": "2021-05-22T07:42:00.211616Z",
     "iopub.status.idle": "2021-05-22T07:42:00.224282Z",
     "shell.execute_reply": "2021-05-22T07:42:00.223571Z",
     "shell.execute_reply.started": "2021-05-22T07:42:00.212674Z"
    },
    "id": "HXRIVuhzLQww"
   },
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            mean, logvar, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + logvar - tf.square(mean) - tf.exp(logvar))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUIs4SduLQwx"
   },
   "source": [
    "### **Train the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ3_PTamMoBV"
   },
   "source": [
    "We have built our model and defined the losses and metrics required to train it. We can compile the model with Adam optimizer and train it over 30 epochs with a batch size of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T07:42:11.302210Z",
     "iopub.status.busy": "2021-05-22T07:42:11.301827Z",
     "iopub.status.idle": "2021-05-22T08:04:47.760734Z",
     "shell.execute_reply": "2021-05-22T08:04:47.759683Z",
     "shell.execute_reply.started": "2021-05-22T07:42:11.302181Z"
    },
    "id": "Gxut_LCBLQwx"
   },
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "history = vae.fit(data, epochs=30, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1zZ75N-LQwy"
   },
   "source": [
    "### **Plot the Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T09:06:41.483449Z",
     "iopub.status.busy": "2021-05-22T09:06:41.483113Z",
     "iopub.status.idle": "2021-05-22T09:06:41.632834Z",
     "shell.execute_reply": "2021-05-22T09:06:41.631870Z",
     "shell.execute_reply.started": "2021-05-22T09:06:41.483421Z"
    },
    "id": "vD813n1RLQwz"
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "# plot loss from 4rd epoch onwards\n",
    "index = np.arange(3, 30)\n",
    "plt.plot(index, loss[3:], 'o-r')\n",
    "plt.xticks(np.arange(3, 30, 2))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xt_hk-SlLQwz"
   },
   "source": [
    "### **Sample Mean & Variance, and Generate some images**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yl8pUdrHMuhF"
   },
   "source": [
    "The model is trained with the input data. It is ready now to generate the images that look close to the original images. To generate the images, we need to sample some mean and variance with which the model can generate the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T08:44:00.579953Z",
     "iopub.status.busy": "2021-05-22T08:44:00.579636Z",
     "iopub.status.idle": "2021-05-22T08:44:08.258965Z",
     "shell.execute_reply": "2021-05-22T08:44:08.258349Z",
     "shell.execute_reply.started": "2021-05-22T08:44:00.579926Z"
    },
    "id": "u35BeQOgLQw0"
   },
   "outputs": [],
   "source": [
    "def plot_latent_space(vae, n=16, figsize=8):\n",
    "    # display a n*n 2D manifold of fashion data\n",
    "    digit_size = 28\n",
    "    scale = 1.0\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = vae.decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"mean: z[0]\")\n",
    "    plt.ylabel(\"log of variance: z[1]\")\n",
    "    plt.imshow(figure, cmap=\"jet\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_latent_space(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTZaiaYEM5G_"
   },
   "source": [
    "We can interpret the above generation as follows. With a constant variance sampled, we can generate different images by controlling the mean value. Likewise, by controlling the variance value against a fixed mean value, we can generate different images. Thus, image generation is greatly controlled by the sampling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqDB1l9I6Br-"
   },
   "source": [
    "#**Related Articles:**\n",
    "\n",
    "> * [Image Generation with Tensorflow Keras](https://analyticsindiamag.com/getting-started-image-generation-tensorflow-keras/)\n",
    "\n",
    "> * [Getting Started with Computer Vision using Tensorflow Keras](https://analyticsindiamag.com/computer-vision-using-tensorflow-keras/)\n",
    "\n",
    "> * [Feature Extraction of Images with Skimage](https://analyticsindiamag.com/image-feature-extraction-using-scikit-image-a-hands-on-guide/)\n",
    "\n",
    "> * [Bitwise Operations On Images Using OpenCV](https://analyticsindiamag.com/how-to-implement-bitwise-operations-on-images-using-opencv/)\n",
    "\n",
    "> * [Face Swaping with OpenCV](https://analyticsindiamag.com/a-fun-project-on-building-a-face-swapping-application-with-opencv/)\n",
    "\n",
    "> * [Create Watermark Images with OpenCV](https://analyticsindiamag.com/how-to-create-a-watermark-on-images-using-opencv/)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOOrHlm7aCrLuTohjxG7Xql",
   "collapsed_sections": [],
   "name": "2_Image_Generation_with_Tensorflow_Keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
