{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8kDeMU65BA7"
   },
   "source": [
    "# Transfer Learning Using TensorFlow Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNxWqyck5RdV"
   },
   "source": [
    "A good deep learning model has a carefully carved architecture. It needs enormous training data, effective hardware, skilled developers, and a vast amount of time to train and hyper-tune the model to achieve satisfactory performance. Therefore, building a deep learning model from scratch and training is practically impossible for every deep learning task. Here comes the power of Transfer Learning. Transfer Learning is the approach of making use of an already trained model for a related task. \n",
    "\n",
    "In this session, we discuss Transfer Learning with necessary examples to perform image classification using TensorFlow Keras. This article assumes that readers have good knowledge of the fundamentals of deep learning and computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf0U3NML5UK7"
   },
   "source": [
    "To read about it more, please refer [this](https://analyticsindiamag.com/transfer-learning-using-tensorflow-keras/) article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKWFRh-G5dAk"
   },
   "source": [
    "# Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6U2eY-Q27T9O"
   },
   "source": [
    "## Feature Extraction Using a Pre-trained Model\n",
    "\n",
    "Import the necessary frameworks and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q --no-warn-script-location\n",
    "!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels sklearn tensorflow tensorflow_datasets keras opencv-python pillow scikit-image --user -q --no-warn-script-location\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bu3Qdhju1srX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2yV-gQ07abZ"
   },
   "source": [
    "TensorFlow Datasets has a huge collection of pre-processed and vectorized datasets from different domains. Here, we discuss feature extraction using transfer learning with image classification problems. Load in-built Oxford_Flowers102 dataset that has images of flowers from 102 different classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdRKAWab7VHo"
   },
   "outputs": [],
   "source": [
    "data, meta = tfds.load('oxford_flowers102', \n",
    "                      as_supervised=True,\n",
    "                      with_info=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KDJmi3b7dr_"
   },
   "source": [
    "Obtain train, validation and test sets from the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_akAcUz7awY"
   },
   "outputs": [],
   "source": [
    "raw_train = data['train']\n",
    "raw_val = data['validation']\n",
    "raw_test = data['test'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtE7crrI7hYF"
   },
   "source": [
    "Sample an image and display it with its label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8tQ2IzP7fkc"
   },
   "outputs": [],
   "source": [
    "label_extractor = meta.features['label'].int2str\n",
    "for image,label in raw_train.take(1):\n",
    "  plt.imshow(image)\n",
    "  plt.title(label_extractor(label))\n",
    "  plt.colorbar()\n",
    "  plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJOqOIlv7kZl"
   },
   "source": [
    "Visual observation yields that the image is 3 channel colour image with pixel values ranging from 0 to 255. However, we can get exact bounding values using the following codes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dm9AlXrb7jAc"
   },
   "outputs": [],
   "source": [
    "print(image.shape)\n",
    "min(image.numpy().ravel()), max(image.numpy().ravel()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRDx-R3t7xAY"
   },
   "source": [
    "We get our problem (the image classification dataset). We need to choose a suitable pre-trained model. ImageNet is one of the most famous datasets used in image classification. It has more than a million images belonging to 1000 classes. Newly developed competing architectures are trained and tested with this dataset. NASNetLarge, developed purely by Google’s reinforcement learning environment (NAS- Neural Architecture Search) without human intervention, is one among the popular architectures. \n",
    "\n",
    "The convolution neural network part in the architecture is called the base, and the artificial neural network part (with Dense layers) is called the head. Base extracts features from the input images. Head makes classification using the extracted features. ImageNet has 1000 classes, but our dataset has only 102 classes. Hence, we can use only the base of the pre-trained NASNetLarge model as a feature extractor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6iTaFWs7mOx"
   },
   "outputs": [],
   "source": [
    "base = keras.applications.NASNetLarge(input_shape=(331,331,3),\n",
    "                                      include_top=False,\n",
    "                                      weights='imagenet') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYNqH6dn71Gs"
   },
   "source": [
    "NASNetLarge expects its input to be in the shape of (331,331,3). We need to resize our images to conform to the requirements. Define a helper function to scale and resize the input images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVJkkjdR7zbl"
   },
   "outputs": [],
   "source": [
    "def scale_image(img,label):\n",
    "  img = tf.cast(img,tf.float32)\n",
    "  img = img/255.0\n",
    "  img = tf.image.resize(img, (331,331))\n",
    "  return img,label  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rw246X9L740t"
   },
   "source": [
    "Scale and resize the train, validation and test images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNrzkQ8v72dx"
   },
   "outputs": [],
   "source": [
    "train = raw_train.map(scale_image)\n",
    "val = raw_val.map(scale_image)\n",
    "test = raw_test.map(scale_image) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0EqwWzK7-BI"
   },
   "source": [
    "Sample some 25 images and display them with their text labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGcQ_hDF76E1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "i=1\n",
    "for img, label in train.take(25):\n",
    "  plt.subplot(5,5,i)\n",
    "  plt.imshow(img)\n",
    "  plt.title(label_extractor(label))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  i += 1\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kz8qfuum8Bol"
   },
   "source": [
    "Prepare data in batches as the optimizer expects it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7qBvj4W7-Te"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_batches = train.batch(batch_size)\n",
    "val_batches = val.batch(batch_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxpSZnJH8Em7"
   },
   "source": [
    "We use the pre-trained model’s base and its weights as such. Hence we should not train them again. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Bh7wf-V8C6t"
   },
   "outputs": [],
   "source": [
    "base.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P32230eY8Imt"
   },
   "source": [
    "Develop a classification head to classify 102 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etXEzgLV8FoG"
   },
   "outputs": [],
   "source": [
    "head = keras.models.Sequential([\n",
    "                                keras.layers.GlobalAveragePooling2D(),\n",
    "                                keras.layers.Dense(512, activation='relu'),\n",
    "                                keras.layers.Dropout(0.5),\n",
    "                                keras.layers.Dense(4, activation='softmax')\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TYiDD5A8KaU"
   },
   "source": [
    "Build the final model with the non-trainable base (feature extractor) and the head.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lXP0Ps28JLK"
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([base,head])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E43zVNsP8Oqi"
   },
   "source": [
    "We can explore the number of parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sIifwtz8OfZ"
   },
   "outputs": [],
   "source": [
    "base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9RbvP918SFj"
   },
   "source": [
    "The base has around 85 million parameters, none of which are trainable (pre-trained model). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3q8cqhG8NAm"
   },
   "outputs": [],
   "source": [
    "head.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjQRX8Ys8VBA"
   },
   "source": [
    "The head has around 2 million parameters, everything being trainable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zr_nD0CX8S4h"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTw2197K8ZZW"
   },
   "source": [
    "We compile the model with a suitable loss function, an optimizer and an evaluation metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baxZNXIQ8WD3"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOIKIJ5T8dKD"
   },
   "source": [
    "Our job is to train the head of the model with the input data while the base remains as such. We can define an early stopping callback. It stops training when there is no remarkable improvement in the validation performance. Here, we provide 3 as the value to the argument patience. It is the number of epochs for which the training will continue even if there is no improvement in performance. For highly unstable performance (zig-zag performance curves), higher patience is preferred.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZ2pLIe-8ahl"
   },
   "outputs": [],
   "source": [
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss',patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_9bO14L8f9p"
   },
   "source": [
    "Train the model for 100 epochs. The early stopping callback will break training at some early epoch itself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWFPCIpJ8eON"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_batches, validation_data=val_batches, epochs=100, callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_nfPZtP8jAT"
   },
   "source": [
    "We observe that the training has stopped just after the 30th epoch due to a decline in validation loss. Visualize the losses and accuracies to get a better insight about training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUOspsOg8g66"
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "length = len(hist['loss'])\n",
    "epochs = np.arange(1,length+1)\n",
    "\n",
    "plt.plot(epochs,hist['loss'], label='Train Loss')\n",
    "plt.plot(epochs,hist['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.xticks(np.arange(1,length+1,2))\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJ6ZWzs-8ptK"
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs,hist['accuracy'], label='Train Accuracy')\n",
    "plt.plot(epochs,hist['val_accuracy'], label='Val Accuracy')\n",
    "plt.legend()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.xticks(np.arange(1,length+1,2))\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMeEM25w8vUn"
   },
   "source": [
    "Both training and validation performances get saturated at around 10th epoch. There is no remarkable improvement afterwards. This is exactly because the base was originally trained to extract features from ImageNet dataset. Our dataset may have some minor feature differences that the base can not extract. We handle this issue with another dataset in the sequel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uopXDAQ48xcE"
   },
   "source": [
    "## Fine-tuning a Pre-trained Model\n",
    "\n",
    "When the present image features differ from originally trained images, we lack performances, even though the pre-trained model is a well-acclaimed state-of-the-art. We look at another dataset here, the Cassava Leaf Disease dataset, available in-built with TensorFlow Datasets. This problem has leaves in each image but with minute differences according to their disease states.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAeIX-QB8sE8"
   },
   "outputs": [],
   "source": [
    "cassava, meta = tfds.load('cassava',\n",
    "                          with_info=True,\n",
    "                          as_supervised=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2pzcxOp81iQ"
   },
   "source": [
    "Obtain the split data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tr7Yd-d28zd0"
   },
   "outputs": [],
   "source": [
    "raw_train  = cassava['train']\n",
    "raw_val = cassava['validation']\n",
    "raw_test = cassava['test'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUJNjC5i85pU"
   },
   "source": [
    "Metadata gives the details about the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPwedmuq82qB"
   },
   "outputs": [],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KhEZ0yz89pe"
   },
   "source": [
    "The dataset has 5 classes: one healthy and four different disease classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtGoJdig87zW"
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "  print(f'{i}: {label_extractor(i)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHaFSpzR9CFD"
   },
   "source": [
    "Sample an image and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ox7f0FHP8_B3"
   },
   "outputs": [],
   "source": [
    "label_extractor = meta.features['label'].int2str\n",
    "for img, label in raw_train.skip(1).take(1):\n",
    "  plt.imshow(img)\n",
    "  plt.title(label_extractor(label))\n",
    "  plt.colorbar()\n",
    "  plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFHEzWDQ9EWM"
   },
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "min(img.numpy().ravel()), max(img.numpy().ravel()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjdA7h2t9J2n"
   },
   "source": [
    "The images are 3 channel colour images with pixel values ranging from 0 to 255 as before. Scale and resize the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uU_1SVdL9GvT"
   },
   "outputs": [],
   "source": [
    "train = raw_train.map(scale_image)\n",
    "val = raw_val.map(scale_image)\n",
    "test = raw_test.map(scale_image) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaGDwQZE9MDj"
   },
   "source": [
    "Sample some 25 resized images and visualize them along with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiQe4il49Kck"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "i=1\n",
    "for img, label in train.take(25):\n",
    "  plt.subplot(5,5,i)\n",
    "  plt.imshow(img)\n",
    "  plt.title(label_extractor(label))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Led4_RGt9RcT"
   },
   "source": [
    "Prepare batches of datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKrlm4rp9OrX"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_batches = train.batch(batch_size)\n",
    "val_batches = val.batch(batch_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx6Rf0l79UH6"
   },
   "source": [
    "Use the same feature extractor base. But, develop a new head to classify 5 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3hyFfKG9S3T"
   },
   "outputs": [],
   "source": [
    "base.trainable=False\n",
    "head = keras.models.Sequential([\n",
    "                                keras.layers.GlobalAveragePooling2D(),\n",
    "                                keras.layers.Dense(256, activation='relu'),\n",
    "                                keras.layers.Dropout(0.5),\n",
    "                                keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model = keras.models.Sequential([base,head]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpdXYFuQ9XHf"
   },
   "source": [
    "Compile the model and visualize the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fga-mKcX9V-t"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uosLTJJ9cD1"
   },
   "source": [
    "As before, the base parameters are non-trainable, and the head parameters are trainable. Train the model for 5 epochs only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jqr9vrWX9bgM"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_batches, validation_data=val_batches, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4RX4v2e9gxH"
   },
   "source": [
    "Visualize the performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-inwjCFU9eYg"
   },
   "outputs": [],
   "source": [
    "hist = history.history\n",
    "loss = hist['loss']\n",
    "val_loss = hist['val_loss']\n",
    "acc = hist['accuracy']\n",
    "val_acc = hist['val_accuracy']\n",
    "plt.plot(loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Val Loss')\n",
    "plt.legend()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fgswZUo9iN1"
   },
   "outputs": [],
   "source": [
    "plt.plot(acc, label='Train Accuracy')\n",
    "plt.plot(val_acc, label='Val Accuracy')\n",
    "plt.legend()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjHl1koF9mRp"
   },
   "source": [
    "After some initial training (here, 5 epochs), we train a few of the top layers in the base to extract the task-based features precisely. The bottom layers will remain untrained. This is called fine-tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyQxuOqP9k1N"
   },
   "outputs": [],
   "source": [
    "len(base.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UN6XlwD9pWK"
   },
   "source": [
    "There are 1039 layers in the base architecture. We freeze the bottom 700 layers and fine-tune the top layers.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYMhV2Oy9nuU"
   },
   "outputs": [],
   "source": [
    "base.trainable = True\n",
    "for layer in base.layers[:700]:\n",
    "  layer.trainable = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgdocXiX9ymc"
   },
   "source": [
    "Compile the model once again to have the changes into effect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qv_9eL2X9xo1"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcDeO8Wy91XD"
   },
   "source": [
    "Continue training from the 6th epoch. Train until 10th epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqiYJGxX9z72"
   },
   "outputs": [],
   "source": [
    "fine_tune = model.fit(train_batches, \n",
    "                      validation_data=val_batches,\n",
    "                      initial_epoch=5,\n",
    "                      epochs=10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3u7A71ra94bW"
   },
   "source": [
    "Visualize the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GRq-qNG93Ng"
   },
   "outputs": [],
   "source": [
    "fine_hist = fine_tune.history\n",
    "loss += fine_hist['loss']\n",
    "val_loss += fine_hist['val_loss']\n",
    "acc += fine_hist['accuracy']\n",
    "val_acc += fine_hist['val_accuracy']\n",
    "plt.plot(loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Val Loss')\n",
    "plt.plot([4,4],[0.5,1.0], '--', label='Fine Tuning Starts')\n",
    "plt.legend()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZ_E4q_797EY"
   },
   "outputs": [],
   "source": [
    "plt.plot(acc, label='Train Accuracy')\n",
    "plt.plot(val_acc, label='Val Accuracy')\n",
    "plt.plot([4,4],[0.6,1.0], '--', label='Fine Tuning Starts')\n",
    "plt.legend()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRDAm_-T_gSo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP9Z3OTtoBpuAF3m97Id/Kp",
   "collapsed_sections": [],
   "name": "4_Transfer_learning_with_Tensorflow_Keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
