{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_b0T285WK_m"
   },
   "source": [
    "# Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4weXk-vWMws"
   },
   "source": [
    "Numba is an open-source Just-In-Time compiler that does exactly that. It enables Python developers to translate a subset of Python and NumPy code directly into machine code by using the LLVM compiler in the backend. In addition to that, Numba offers a wide range of choices for parallelizing Python code for CPUs and GPUs with trivial code changes. There are a lot of ways to approach compiling Python; the approach Numba takes is to compile individual functions or a collection of functions just in time as you need them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWN-LT3lWSU2"
   },
   "source": [
    "To read about it more, please refer [this](https://analyticsindiamag.com/make-python-code-faster-with-numba/) article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HetXW5UzWPjo"
   },
   "source": [
    "# Using Numba to make Python & NumPy code faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv8SWOzFWRsb"
   },
   "source": [
    "Numba can be installed from PyPI as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhgL5aGDWBrK"
   },
   "outputs": [],
   "source": [
    "!python -m pip install numba numpy --user -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oH0egHh2dx5m"
   },
   "source": [
    "Numba uses decorators to convert Python functions into functions that compile themselves. The most common Numba decorator is @jit. Let’s create an example function and see @jit in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKFzPBg5dwcN"
   },
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import jit\n",
    "@jit(nopython=True)\n",
    "def example_function(n): \n",
    "    trace = 0.0\n",
    "    for i in range(n.shape[0]):  \n",
    "        trace += np.tanh(n[i, i]) \n",
    "    return n + trace   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqJK05v8d_MK"
   },
   "source": [
    "The nopython=True option tells Numba to fully compile the function to remove the Python interpreter calls completely. If it is not used, exceptions are raised, indicating places in the function that need to be refactored to achieve better-than-Python performance. Using nopython=True is strongly recommended. \n",
    "\n",
    "We’ll be using the %timeit magic function to measure execution time because it runs the function multiple times to get a more accurate estimate of short functions. Our function has not been compiled yet; to do that, we need to call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WM4CCq-gd0T9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = np.arange(10000).reshape(100, 100)\n",
    "%timeit example_function(n) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poqMwFSUeGE1"
   },
   "source": [
    "The function was compiled, executed and cached. Now when it is called again, the previously generated machine code is executed directly without any need for compilation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9o_rYZl8eBr_"
   },
   "outputs": [],
   "source": [
    "%timeit example_function(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4v3aE1-eKi-"
   },
   "source": [
    "When benchmarking Numba-compiled functions, it is important to time them without including the compilation step since the compilation of a given function only happens once. Let’s compare to the uncompiled function. Numba-compiled functions have a .py_func attribute that can be used to access the original uncompiled Python function.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZymcsbReIZt"
   },
   "outputs": [],
   "source": [
    "%timeit example_function.py_func(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkEWLw4SeOxz"
   },
   "source": [
    "The original Python function is more than 20 times slower than the Numba-compiled version. However, our example function used explicit loops, which are very fast in Numba and not so much in Python. Our function is really simple so we can try optimizing it by rewriting it using only NumPy expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l0igpiUReMrl"
   },
   "outputs": [],
   "source": [
    "def numpy_example(n):\n",
    "    return n + np.tanh(np.diagonal(n)).sum()\n",
    "%timeit numpy_example(n) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae6s65nkeW8W"
   },
   "source": [
    "The refactored NumPy version is roughly 10 times faster than the Python version but still slower than the Numba-compiled version. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OI2eYwvbeYZA"
   },
   "source": [
    "## Multithreading with Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze0TQhkReapU"
   },
   "source": [
    "Operations on NumPy array expressions are often broadcasted independently over the input elements and have a significant amount of implied parallelism. Numba’s ParallelAccelerator optimization identifies this parallelism and automatically distributes it over several threads. To enable the parallelization pass, all we need to do is use the parallel=True option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXUWUEnxeQrq"
   },
   "outputs": [],
   "source": [
    "SQRT_2PI = np.sqrt(2 * np.pi)\n",
    "@jit(nopython=True, parallel=True)\n",
    "def gaussians(x, means, widths):\n",
    "    n = means.shape[0]\n",
    "    result = np.exp( -0.5 * ((x - means) / widths)**2 ) / widths\n",
    "    return result / SQRT_2PI / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3AR3XD7ed49"
   },
   "source": [
    "Let’s call the function once to compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8iY4G2b7ecsO"
   },
   "outputs": [],
   "source": [
    "means = np.random.uniform(-1, 1, size=1000000)\n",
    "widths = np.random.uniform(0.1, 0.3, size=1000000)\n",
    "gaussians(0.4, means, widths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bnRX1p_eiRP"
   },
   "source": [
    "Now we can accurately compare the effect of threading and compiling with the normal Python version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCJ4wMz-egsj"
   },
   "outputs": [],
   "source": [
    "gaussians_nothread = jit(nopython=True)(gaussians.py_func)\n",
    "%timeit gaussians(0.4, means, widths)  # numba-compiled and threading\n",
    "%timeit gaussians_nothread(0.4, means, widths) # no threading\n",
    "%timeit gaussians.py_func(0.4, means, widths) # normal python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cq9oCtd4el69"
   },
   "source": [
    "There are situations suited for multithreading where there’s no array expression but rather a loop where each iteration is independent of the other. In these cases, we can use prange() in a for loop to indicate to ParallelAccelerator that this loop can be executed in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8JRIS3bekLa"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# Serial version\n",
    "@jit(nopython=True)\n",
    "def monte_carlo_pi_serial(nsamples):\n",
    "    acc = 0\n",
    "    for i in range(nsamples):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if (x**2 + y**2) < 1.0:\n",
    "            acc += 1\n",
    "    return 4.0 * acc / nsamples\n",
    "\n",
    "# Parallel version\n",
    "@jit(nopython=True, parallel=True)\n",
    "def monte_carlo_pi_parallel(nsamples):\n",
    "    acc = 0\n",
    "    # Only change is here\n",
    "    for i in numba.prange(nsamples):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if (x**2 + y**2) < 1.0:\n",
    "            acc += 1\n",
    "    return 4.0 * acc / nsamples\n",
    "\n",
    "%time monte_carlo_pi_serial(int(4e8))\n",
    "%time monte_carlo_pi_parallel(int(4e8)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZQMPMydevEV"
   },
   "source": [
    "One thing to note here is that prange() automatically handles the reduction variable acc in a thread-safe way.  Additionally, Numba automatically initializes the random number generator in each thread independently.\n",
    "\n",
    "Alternatively, you can also use modules like concurrent.futures or Dask to run functions in multiple threads. For these use-cases, ParallelAccelerator isn’t helpful; we only want to obtain the Numba-compiled function to run concurrently in different threads. For accomplishing this, we need the Numba function to release the Global Interpreter Lock (GIL) during execution. This can be done using the nogil=True option. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMG+ZhK7AnKWp9ZbJXh8JkS",
   "collapsed_sections": [],
   "name": "Numba.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
