{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxAgNz6VekmP"
   },
   "source": [
    "# urllib & Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPQEkkCzeokQ"
   },
   "source": [
    "## Urllib\n",
    "\n",
    "Urllib is a package that combines several modules to preprocess the URLs. In simple words, it is an HTTP client for python programming languages, the latest version of \n",
    "\n",
    "Urllib is urllib3 1.26.2 which supports thread-safe connection, connection pooling, client-side verification using SSL/TLS verification, multipart encoding, support for gzip, and brotli encoding. It brings many critical features that are missing from traditional python libraries.\n",
    "\n",
    "Urllib3 is one of the widely downloaded packages on PyPi, and it is the first to execute in any web scraping script, it is available under the MIT license."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DV2ybOgkesgW"
   },
   "source": [
    "By using urllib.request we can simply open and read URLs.\n",
    "\n",
    "urllib.error defines the exceptions and errors raised by the urllib.request command.\n",
    "\n",
    "urllib.parse is used for parsing URLs.\n",
    "\n",
    "urllib.robotparser is used for parsing [robots.txt](https://developers.google.com/search/docs/advanced/robots/intro) files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVmMztQcfNMf"
   },
   "source": [
    "## Requests\n",
    "\n",
    "Requests is an open-source python library that makes HTTP requests more human-friendly and simple to use. It is developed by Kenneth Reitz, Cory Benfield, Ian Stapleton Cordasco, Nate Prewitt with an initial release in February 2011.\n",
    "\n",
    "Requests module library is Apache2 licensed, which is written in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouV5Zva3fOgZ"
   },
   "source": [
    "To read about it more, please refer [this](https://analyticsindiamag.com/web-scraping-frameworks/) article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSMOnCvwezsK"
   },
   "source": [
    "# Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UW8Qrgme2D5"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q\n",
    "!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels scikit-image urllib3 --user -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7VNikjle6Vj"
   },
   "source": [
    "## Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCn3yLR0bx1q"
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "r = http.request('GET', 'http://httpbin.org/robots.txt')\n",
    "r.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TD31h02kfBIU"
   },
   "outputs": [],
   "source": [
    "r.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okuaoaJ3e-eH"
   },
   "source": [
    "Letâ€™s scrape a website using urllib and regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ohDmYZZe9Cv"
   },
   "outputs": [],
   "source": [
    "#1 libraries needed\n",
    "import urllib.request\n",
    "import urllib.parse \n",
    "import re \n",
    "\n",
    "#2 search   \n",
    "url = 'https://analyticsindiamag.com/'\n",
    "values = {'s':'Web Scraping', \n",
    "          'submit':'search'} \n",
    "#defining header\n",
    "header= {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) ' \n",
    "      'AppleWebKit/537.11 (KHTML, like Gecko) '\n",
    "      'Chrome/23.0.1271.64 Safari/537.11',\n",
    "      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "      'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "      'Accept-Encoding': 'none',\n",
    "      'Accept-Language': 'en-US,en;q=0.8',\n",
    "      'Connection': 'keep-alive'}\n",
    "\n",
    "#3 parse\n",
    "data = urllib.parse.urlencode(values) \n",
    "data = data.encode('utf-8') \n",
    "req = urllib.request.Request(url, data, headers=header) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_52wYFbjVo5"
   },
   "outputs": [],
   "source": [
    "resp = urllib.request.urlopen(req) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cw8b4oWafFED"
   },
   "outputs": [],
   "source": [
    "respData = resp.read() \n",
    "\n",
    "#4 extract using regular expressions\n",
    "document = re.findall(r'<p>(.*?)</p>',str(respData)) \n",
    "   \n",
    "for line in document: \n",
    "    print(line) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBOwSk9zkdpJ"
   },
   "source": [
    "## Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Agg6DN5yjrRl"
   },
   "outputs": [],
   "source": [
    "#1 importing modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#2 using .GET()\n",
    "res = requests.get('https://analyticsindiamag.com/')\n",
    "\n",
    "#3 beautiful for extracting only reliable data\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "article_block =soup.find_all('div',class_='post-title')\n",
    "for titles in article_block:\n",
    "\ttitle =titles.find('span').get_text()\n",
    "\tprint(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9VrK4kXkrW1"
   },
   "source": [
    "## Use- case of Request other than Web scraping\n",
    "\n",
    "We can use the requests module to request our web API to get answers like in this case we are using POST on web API: https://loan5.herokuapp.com/api\n",
    "\n",
    "This API is used to predict loan approval. It returns 1 or 0, i.e. approved or disapproved on passing some attributes like gender, credit history, married, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVhUzMJmknsF"
   },
   "outputs": [],
   "source": [
    "#1\n",
    "import json\n",
    "import requests\n",
    "url= 'https://loan5.herokuapp.com/api'\n",
    "\n",
    "#2 sample data\n",
    "data={'Gender':1, 'Married':1, 'Dependents':2, 'Education':0, 'Self_Employed':1,'Credit_History':0,'Property_Area':1, 'Income':1}\n",
    "data = json.dumps(data)\n",
    "\n",
    "#3 sending requesting with data to webapi and it will \n",
    "#Return the answer.\n",
    "send_req = requests.post(url, data)\n",
    "print(send_req.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPMijr3rwVKquywtEyfh11L",
   "collapsed_sections": [],
   "name": "3_Essential_Of_Web_scraping.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "f60a20abaabf5a658075b37fac599269792a9493ddacd7c14d8505185d5625aa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
