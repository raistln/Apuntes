{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_1b_mivA3or"
   },
   "source": [
    "# Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziINBm0DpYis"
   },
   "source": [
    "Maximum Likelihood Estimation, simply known as MLE, is a traditional probabilistic approach that can be applied to data belonging to any distribution, i.e., Normal, Poisson, Bernoulli, etc. With prior assumption or knowledge about the data distribution, Maximum Likelihood Estimation helps find the most likely-to-occur distribution parameters. For instance, let us say we have data that is assumed to be normally distributed, but we do not know its mean and standard deviation parameters. Maximum Likelihood Estimation iteratively searches the most likely mean and standard deviation that could have generated the distribution. Moreover, Maximum Likelihood Estimation can be applied to both regression and classification problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGkBxxxjpbFX"
   },
   "source": [
    "Therefore, Maximum Likelihood Estimation is simply an optimization algorithm that searches for the most suitable parameters. Since we know the data distribution a priori, the algorithm attempts iteratively to find its pattern. The approach is much generalized, so that it is important to devise a user-defined Python function that solves the particular machine learning problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzajekTXpdpI"
   },
   "source": [
    "## Regression on Normally Distributed Data\n",
    "\n",
    "Here, we perform simple linear regression on synthetic data. The data is ensured to be normally distributed by incorporating some random Gaussian noises. Data can be said to be normally distributed if its residual follows the normal distribution—Import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip --upgrade --user -q\n",
    "!python -m pip install numpy pandas seaborn matplotlib scipy statsmodels --user -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vn14Y8NbArtr"
   },
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels import api\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToGvKDWmpoRw"
   },
   "source": [
    "Generate some synthetic data based on the assumption of Normal Distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ot8tmdfnWMrE"
   },
   "outputs": [],
   "source": [
    "# create an independent variable \n",
    "x = np.linspace(-10, 30, 100)\n",
    "\n",
    "# create a normally distributed residual\n",
    "e = np.random.normal(10, 5, 100)\n",
    "\n",
    "# generate ground truth\n",
    "y = 10 + 4*x + e\n",
    "\n",
    "df = pd.DataFrame({'x':x, 'y':y})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFStlFCnpqJ3"
   },
   "source": [
    "Visualize the synthetic data on Seaborn’s regression plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DIoMu9MYP0S"
   },
   "outputs": [],
   "source": [
    "# visualize data distribution\n",
    "sns.regplot(x='x', y='y', data = df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ktLs6ihMO0B"
   },
   "source": [
    "## Solve by OLS approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-_y9cFtptxn"
   },
   "source": [
    "The data is normally distributed, and the output variable is a continuously varying number. Hence, we can use the Ordinary Least Squares (OLS) method to determine the model parameters and use them as a benchmark to evaluate the Maximum Likelihood Estimation approach. Apply the OLS algorithm to the synthetic data and find the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4YMSsmDYfy3"
   },
   "outputs": [],
   "source": [
    "features = api.add_constant(df.x)\n",
    "model = api.OLS(y, features).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eroC5d_xpxWD"
   },
   "source": [
    "We get the intercept and regression coefficient values of the simple linear regression model. Further, we can derive the standard deviation of the normal distribution with the following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9oWdmGAOaH1c"
   },
   "outputs": [],
   "source": [
    "# find the std dev\n",
    "res = model.resid\n",
    "standard_dev = np.std(res)\n",
    "standard_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6KHvfhuMFDm"
   },
   "source": [
    "## Solve by MLE approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9ozvZwSp4Z1"
   },
   "source": [
    "Define a user-defined Python function that can be iteratively called to determine the negative log-likelihood value. The key idea of formulating this function is that it must contain two elements: the first is the model building equation (here, the simple linear regression). The second is the logarithmic value of the probability density function (here, the log PDF of normal distribution). Since we need negative log-likelihood, it is obtained just by negating the log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OU7JErOaYCG"
   },
   "outputs": [],
   "source": [
    "# MLE\n",
    "\n",
    "def MLE_Norm(parameters):\n",
    "  const, beta, std_dev = parameters\n",
    "  pred = const + beta*x\n",
    "\n",
    "  LL = np.sum(stats.norm.logpdf(y, pred, std_dev))\n",
    "  neg_LL = -1*LL\n",
    "  return neg_LL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTe71db_p8B9"
   },
   "source": [
    "Minimize the negative log-likelihood of the generated data using the minimize method available with SciPy’s optimize module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gf_EjLCSgkLI"
   },
   "outputs": [],
   "source": [
    "mle_model = minimize(MLE_Norm, np.array([2,2,2]), method='L-BFGS-B')\n",
    "mle_model"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOajAla8XGDCimBBsc5Wyzo",
   "collapsed_sections": [],
   "name": "1_Maximum_Likelihood_Estimation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
